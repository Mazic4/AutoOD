{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn import metrics\n",
    "import scipy as sp\n",
    "import logging\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "    \n",
    "def run_lof(X, y, k=60):\n",
    "    clf = LocalOutlierFactor(n_neighbors=k)\n",
    "    clf.fit(X)\n",
    "    lof_scores = -clf.negative_outlier_factor_\n",
    "    return lof_scores\n",
    "\n",
    "def get_predictions(scores, num_outliers = 400, method_name = 'LOF'):\n",
    "    threshold = np.sort(scores)[::-1][num_outliers]\n",
    "    # threshold, max_f1 = get_best_f1_score(y, lof_scores)\n",
    "    predictions = np.array(scores > threshold)\n",
    "    predictions = np.array([int(i) for i in predictions])\n",
    "#     print('F1 for {} : {}'.format(method_name, metrics.f1_score(y, predictions)))\n",
    "    return predictions, scores, metrics.f1_score(y, predictions)\n",
    "\n",
    "def get_best_F1(scores):\n",
    "    best_f1 = 0\n",
    "    for i in range(np.shape(scores)[0]):\n",
    "        threshold = np.sort(scores)[::-1][i]\n",
    "        predictions = np.array(scores > threshold)\n",
    "        predictions = np.array([int(i) for i in predictions])\n",
    "        cur_f1 = metrics.f1_score(y, predictions)\n",
    "        best_f1 = max(cur_f1, best_f1)\n",
    "    return best_f1\n",
    "\n",
    "def run_knn(X, y, k=60):\n",
    "    neigh = NearestNeighbors(n_neighbors=k)\n",
    "    neigh.fit(X)\n",
    "    knn_dists = neigh.kneighbors(X)[0][:,-1]\n",
    "    return knn_dists\n",
    "\n",
    "def run_isolation_forest(X, y, max_features = 1.0):\n",
    "    # training the model\n",
    "    clf = IsolationForest(random_state=42,max_features=max_features)\n",
    "    clf.fit(X)\n",
    "    # predictions\n",
    "    sklearn_score_anomalies = clf.decision_function(X)\n",
    "    if_scores = [-1*s + 0.5 for s in sklearn_score_anomalies]\n",
    "    return if_scores\n",
    "\n",
    "def mahalanobis(x):\n",
    "    \"\"\"Compute the Mahalanobis Distance between each row of x and the data\n",
    "    \"\"\"\n",
    "    x_minus_mu = x - np.mean(x)\n",
    "    cov = np.cov(x.T)\n",
    "    det = np.linalg.det(cov)\n",
    "    if det != 0:\n",
    "        inv_covmat = sp.linalg.inv(cov)\n",
    "    else:\n",
    "        inv_covmat = sp.linalg.pinv(cov)\n",
    "    results = []\n",
    "    x_minus_mu = np.array(x_minus_mu)\n",
    "    for i in range(np.shape(x)[0]):\n",
    "        cur_data = x_minus_mu[i,:]\n",
    "        results.append(np.dot(np.dot(x_minus_mu[i,:], inv_covmat), x_minus_mu[i,:].T))\n",
    "    return np.array(results)\n",
    "#     left_term = np.dot(x_minus_mu, inv_covmat)\n",
    "#     mahal = np.dot(left_term, x_minus_mu.T)\n",
    "#     print(mahal.diagonal())\n",
    "#     return mahal.diagonal()\n",
    "\n",
    "def run_mahalanobis(X, y):\n",
    "    # training the model\n",
    "    dist = mahalanobis(x=X)\n",
    "    return dist\n",
    "\n",
    "def load_dataset(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        data, meta = arff.loadarff(f)\n",
    "    data = pd.DataFrame(data)\n",
    "    X = data.drop(columns=['id', 'outlier'])\n",
    "    # Map dataframe to encode values and put values into a numpy array\n",
    "    y = data[\"outlier\"].map(lambda x: 1 if x == b'yes' else 0).values\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load SpamBase dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4207, 57) (4207,)\n",
      "0.39909674352270025\n"
     ]
    }
   ],
   "source": [
    "filename = './SpamBase_withoutdupl_norm_40.arff'\n",
    "X, y = load_dataset(filename=filename)\n",
    "print(np.shape(X), np.shape(y))\n",
    "K = 9\n",
    "N = 1679\n",
    "class_balance = [1- N/4207.0, N/4207.0]\n",
    "lof_krange = list(range(10,110,10)) * 6\n",
    "knn_krange = list(range(10,110,10)) * 6\n",
    "if_range = [0.5, 0.6, 0.7, 0.8, 0.9] * 6\n",
    "# mahalanobis_N_range=[N]\n",
    "mahalanobis_N_range = [1400, 1500, 1600, 1700, 1800, 1900]\n",
    "if_N_range = np.sort(mahalanobis_N_range * 5)\n",
    "N_range = np.sort(mahalanobis_N_range * 10) \n",
    "print(N/len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pageblock dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5473, 10) (5473,)\n"
     ]
    }
   ],
   "source": [
    "filename = './PageBlocks/PageBlocks_norm_10.arff'\n",
    "X, y = load_dataset(filename=filename)\n",
    "print(np.shape(X), np.shape(y))\n",
    "K = 80\n",
    "N = 560\n",
    "# num_outliers = [N, N, N, N]\n",
    "class_balance = [0.9, 0.1]\n",
    "# lof_krange = [55, 60, 65, 70, 75] \n",
    "# knn_krange = [55, 60, 65, 70, 75] \n",
    "# if_range = [0.5, 0.6,0.7, 0.8,0.9]\n",
    "lof_krange = list(range(10,110,10)) * 6\n",
    "knn_krange = list(range(10,110,10)) * 6\n",
    "# lof_krange = range(70,90,4) \n",
    "# knn_krange = [60, 70, 80, 90, 100] \n",
    "if_range = [0.5, 0.6, 0.7, 0.8, 0.9] * 6\n",
    "# mahalanobis_N_range=[560]\n",
    "mahalanobis_N_range = [300, 400, 500, 600, 700, 800]\n",
    "# mahalanobis_N_range = [550, 560, 570, 580, 590, 600]\n",
    "N_size = 6\n",
    "if_N_range = np.sort(mahalanobis_N_range * 5)\n",
    "N_range = np.sort(mahalanobis_N_range * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pima dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 8) (768,)\n",
      "0.3489583333333333\n"
     ]
    }
   ],
   "source": [
    "filename = './Pima_withoutdupl_norm_35.arff'\n",
    "X, y = load_dataset(filename=filename)\n",
    "print(np.shape(X), np.shape(y))\n",
    "K = 100\n",
    "N = 268\n",
    "print(N/len(y))\n",
    "num_outliers = [N, N, N, N]\n",
    "class_balance = [1- N/768.0, N/768.0]\n",
    "lof_krange = list(range(10,210,10)) * 6\n",
    "knn_krange = list(range(10,210,10)) * 6\n",
    "if_range = [0.5, 0.6, 0.7, 0.8, 0.9] * 6\n",
    "mahalanobis_N_range=[N]\n",
    "mahalanobis_N_range = [220,230,240,250,260,270]\n",
    "\n",
    "if_N_range = np.sort(mahalanobis_N_range * 5)\n",
    "N_range = np.sort(mahalanobis_N_range * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALOI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49534, 27) (49534,)\n",
      "1508\n"
     ]
    }
   ],
   "source": [
    "filename = './ALOI_withoutdupl_norm.arff'\n",
    "X, y = load_dataset(filename=filename)\n",
    "print(np.shape(X), np.shape(y))\n",
    "print(sum(y))\n",
    "N = 1508\n",
    "num_outliers = [N, N, N, N]\n",
    "class_balance = [1- N/49534.0, N/49534.0]\n",
    "lof_krange = list(range(10,110,10)) * 6\n",
    "knn_krange = list(range(10,110,10)) * 6\n",
    "if_range = [0.5, 0.6, 0.7, 0.8, 0.9] * 6\n",
    "# mahalanobis_N_range=[N]\n",
    "mahalanobis_N_range=[1500, 2000, 2500, 3000, 3500, 4000]\n",
    "if_N_range = np.sort(mahalanobis_N_range * 5)\n",
    "N_range = np.sort(mahalanobis_N_range *10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load InternetAds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1966, 1555) (1966,)\n",
      "368\n"
     ]
    }
   ],
   "source": [
    "filename = './InternetAds_withoutdupl_norm_19.arff'\n",
    "X, y = load_dataset(filename=filename)\n",
    "print(np.shape(X), np.shape(y))\n",
    "print(sum(y))\n",
    "N = 368\n",
    "num_outliers = [N, N, N, N]\n",
    "class_balance = [1- N/1966.0, N/1966.0]\n",
    "lof_krange = list(range(5,55,5)) * 6\n",
    "knn_krange = list(range(5,55,5)) * 6\n",
    "if_range = [0.5, 0.6, 0.7, 0.8, 0.9] * 6\n",
    "# mahalanobis_N_range=[N]\n",
    "mahalanobis_N_range = [300, 350, 400, 450, 500, 550]\n",
    "if_N_range = np.sort(mahalanobis_N_range * 5)\n",
    "N_range = np.sort(mahalanobis_N_range *10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load KDDCup 99 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48113, 40) (48113,)\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "filename = './KDDCup99_withoutdupl_norm_catremoved.arff'\n",
    "X, y = load_dataset(filename=filename)\n",
    "print(np.shape(X), np.shape(y))\n",
    "print(sum(y))\n",
    "N = 200\n",
    "num_outliers = [N, N, N, N]\n",
    "class_balance = [1- N/48113.0, N/48113.0]\n",
    "lof_krange = list(range(10,110,10)) * 6\n",
    "knn_krange = list(range(10,110,10)) * 6\n",
    "if_range = [0.5, 0.6, 0.7, 0.8, 0.9] * 6\n",
    "mahalanobis_N_range=[500,1000,1500,2000,2500,3000]\n",
    "# mahalanobis_N_range = [20, 40, 60,80, 100,120]\n",
    "if_N_range = np.sort(mahalanobis_N_range * 5)\n",
    "N_range = np.sort(mahalanobis_N_range *10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(620098, 29)\n",
      "1052\n"
     ]
    }
   ],
   "source": [
    "filename='kdd99-unsupervised-ad.csv'\n",
    "import pandas as pd\n",
    "data = pd.read_csv(filename, header=None)\n",
    "X = data.drop(columns=[29])\n",
    "print(np.shape(np.array(X)))\n",
    "# Map dataframe to encode values and put values into a numpy array\n",
    "y = data[29].map(lambda x: 1 if x == 'o' else 0).values\n",
    "print(sum(y))\n",
    "\n",
    "lof_krange = list(range(10,110,10)) * 6\n",
    "knn_krange = list(range(10,110,10)) * 6\n",
    "if_range = [0.5, 0.6, 0.7, 0.8, 0.9] * 6\n",
    "mahalanobis_N_range=[1000,1500,2000,2500,3000,3500]\n",
    "if_N_range = np.sort(mahalanobis_N_range * 5)\n",
    "N_range = np.sort(mahalanobis_N_range *10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load shuttle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49097\n",
      "0.0715114976475141\n",
      "(49097, 9)\n"
     ]
    }
   ],
   "source": [
    "import hdf5storage\n",
    "mat = hdf5storage.loadmat('shuttle.mat')\n",
    "X = mat['X']\n",
    "y = mat['y']\n",
    "print(len(y))\n",
    "print(np.sum(y)/len(y))\n",
    "lof_krange = list(range(10,110,10)) * 6\n",
    "knn_krange = list(range(10,110,10)) * 6\n",
    "if_range = [0.5, 0.6, 0.7, 0.8, 0.9] * 6\n",
    "mahalanobis_N_range=[1000,1500,2000, 2500,3000, 3500]\n",
    "# mahalanobis_N_range = [20, 40, 60,80, 100,120]\n",
    "if_N_range = np.sort(mahalanobis_N_range * 5)\n",
    "N_range = np.sort(mahalanobis_N_range *10)\n",
    "print(np.shape(X))\n",
    "\n",
    "# normalize\n",
    "# from sklearn.preprocessing import Normalizer\n",
    "# transformer = Normalizer().fit(X) \n",
    "# X = transformer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load mulcross dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(262144, 4) (262144,)\n",
      "26214\n",
      "0.09999847412109375\n"
     ]
    }
   ],
   "source": [
    "filename = './mulcross.arff'\n",
    "\n",
    "with open(filename, 'r') as f:\n",
    "    data, meta = arff.loadarff(f)\n",
    "data = pd.DataFrame(data)\n",
    "X = data.drop(columns=['Target'])\n",
    "y = data[\"Target\"].map(lambda x: 1 if x == b'Anomaly' else 0).values\n",
    "# X, y = load_dataset(filename=filename)\n",
    "print(np.shape(X), np.shape(y))\n",
    "print(sum(y))\n",
    "print(sum(y)/len(y))\n",
    "lof_krange = list(range(10,110,10)) * 6\n",
    "knn_krange = list(range(10,110,10)) * 6\n",
    "if_range = [0.5, 0.6, 0.7, 0.8, 0.9] * 6\n",
    "mahalanobis_N_range=[20000, 22000, 24000, 26000, 28000, 30000]\n",
    "# mahalanobis_N_range = [20, 40, 60,80, 100,120]\n",
    "if_N_range = np.sort(mahalanobis_N_range * 5)\n",
    "N_range = np.sort(mahalanobis_N_range *10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load HTTP dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567498\n",
      "2211.0\n",
      "0.003896048972859816\n"
     ]
    }
   ],
   "source": [
    "import hdf5storage\n",
    "mat = hdf5storage.loadmat('http.mat')\n",
    "X = mat['X']\n",
    "y = mat['y']\n",
    "print(len(y))\n",
    "print(np.sum(y))\n",
    "print(np.sum(y)/len(y))\n",
    "lof_krange = list(range(10,110,10)) * 6\n",
    "knn_krange = list(range(10,110,10)) * 6\n",
    "if_range = [0.5, 0.6, 0.7, 0.8, 0.9] * 6\n",
    "# mahalanobis_N_range=[1500, 2000, 2500, 3000, 3500, 4000]\n",
    "mahalanobis_N_range=[5000, 10000, 15000,20000, 25000, 30000]\n",
    "# mahalanobis_N_range=[10000, 15000, 20000, 25000, 30000, 35000]\n",
    "# mahalanobis_N_range = [20, 40, 60,80, 100,120]\n",
    "if_N_range = np.sort(mahalanobis_N_range * 5)\n",
    "N_range = np.sort(mahalanobis_N_range *10)\n",
    "\n",
    "# # remove duplicates\n",
    "# newdata = pd.DataFrame(np.concatenate((X,y), axis = 1)).drop_duplicates()\n",
    "# X = newdata[[0,1,2]].values\n",
    "# y = np.array([1 if i==1.0 else 0 for i in newdata[[3]].values])\n",
    "# print('Remove duplicates: ', len(y))\n",
    "\n",
    "# # normalize\n",
    "# from sklearn.preprocessing import Normalizer\n",
    "# transformer = Normalizer().fit(X) \n",
    "# X = transformer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load ForestCover Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2747\n",
      "286048\n"
     ]
    }
   ],
   "source": [
    "import hdf5storage\n",
    "import pickle\n",
    "dataset = pickle.load(open(\"cover_dataset.pickle\", \"rb\"))\n",
    "X = dataset['X']\n",
    "y = dataset['y']\n",
    "print(np.sum(y))\n",
    "print(len(y))\n",
    "lof_krange = list(range(10,110,10)) * 6\n",
    "knn_krange = list(range(10,110,10)) * 6\n",
    "if_range = [0.5, 0.6, 0.7, 0.8, 0.9] * 6\n",
    "mahalanobis_N_range=[5000, 8000, 10000, 12000, 15000, 18000]\n",
    "# mahalanobis_N_range = [20, 40, 60,80, 100,120]\n",
    "if_N_range = np.sort(mahalanobis_N_range * 5)\n",
    "N_range = np.sort(mahalanobis_N_range *10)\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "transformer = RobustScaler().fit(X)\n",
    "X_transformed = transformer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Annthyroid dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7129, 21) (7129,)\n",
      "0.07490531631364848\n"
     ]
    }
   ],
   "source": [
    "filename = './Annthyroid_withoutdupl_norm_07.arff'\n",
    "X, y = load_dataset(filename=filename)\n",
    "print(np.shape(X), np.shape(y))\n",
    "print(sum(y)/len(y))\n",
    "N = 534\n",
    "num_outliers = [N, N, N, N]\n",
    "class_balance = [1- N/7129.0, N/7129.0]\n",
    "lof_krange = list(range(10,110,10)) * 6\n",
    "knn_krange = list(range(10,110,10)) * 6\n",
    "if_range = [0.5, 0.6, 0.7, 0.8, 0.9] * 6\n",
    "# mahalanobis_N_range=[N]\n",
    "mahalanobis_N_range=[300, 400,500,600,700,800]\n",
    "if_N_range = np.sort(mahalanobis_N_range * 5)\n",
    "N_range = np.sort(mahalanobis_N_range *10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Musk dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3062\n",
      "97.0\n",
      "0.03167864141084259\n",
      "(3062, 166)\n"
     ]
    }
   ],
   "source": [
    "import hdf5storage\n",
    "mat = hdf5storage.loadmat('musk.mat')\n",
    "X = mat['X']\n",
    "y = mat['y']\n",
    "print(len(y))\n",
    "print(np.sum(y))\n",
    "print(np.sum(y)/len(y))\n",
    "lof_krange = list(range(10,110,10)) * 6\n",
    "knn_krange = list(range(10,110,10)) * 6\n",
    "if_range = [0.5, 0.6, 0.7, 0.8, 0.9] * 6\n",
    "mahalanobis_N_range=[100, 120, 140, 160, 180, 200]\n",
    "# mahalanobis_N_range = [20, 40, 60,80, 100,120]\n",
    "if_N_range = np.sort(mahalanobis_N_range * 5)\n",
    "N_range = np.sort(mahalanobis_N_range *10)\n",
    "print(np.shape(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Satimage-2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5803\n",
      "71.0\n",
      "0.0122350508357746\n",
      "(5803, 36)\n"
     ]
    }
   ],
   "source": [
    "import hdf5storage\n",
    "mat = hdf5storage.loadmat('satimage-2.mat')\n",
    "X = mat['X']\n",
    "y = mat['y']\n",
    "print(len(y))\n",
    "print(np.sum(y))\n",
    "print(np.sum(y)/len(y))\n",
    "lof_krange = list(range(10,110,10)) * 6\n",
    "knn_krange = list(range(10,110,10)) * 6\n",
    "if_range = [0.5, 0.6, 0.7, 0.8, 0.9] * 6\n",
    "mahalanobis_N_range=[60, 80, 100, 120, 140, 160]\n",
    "# mahalanobis_N_range = [20, 40, 60,80, 100,120]\n",
    "if_N_range = np.sort(mahalanobis_N_range * 5)\n",
    "N_range = np.sort(mahalanobis_N_range *10)\n",
    "print(np.shape(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pendigits Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdf5storage\n",
    "mat = hdf5storage.loadmat('pendigits.mat')\n",
    "X = mat['X']\n",
    "y = mat['y']\n",
    "print(len(y))\n",
    "print(np.sum(y))\n",
    "print(np.sum(y)/len(y))\n",
    "lof_krange = list(range(10,110,10)) * 6\n",
    "knn_krange = list(range(10,110,10)) * 6\n",
    "if_range = [0.5, 0.6, 0.7, 0.8, 0.9] * 6\n",
    "mahalanobis_N_range=[150,200,250,300,350,400]\n",
    "# mahalanobis_N_range = [20, 40, 60,80, 100,120]\n",
    "if_N_range = np.sort(mahalanobis_N_range * 5)\n",
    "N_range = np.sort(mahalanobis_N_range *10)\n",
    "print(np.shape(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Outlier Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOF k = 10, best F-1 = 0.39999999999999997\n",
      "LOF k = 20, best F-1 = 0.4520833333333334\n",
      "LOF k = 30, best F-1 = 0.42499999999999993\n",
      "LOF k = 40, best F-1 = 0.4547169811320755\n",
      "LOF k = 50, best F-1 = 0.49056603773584906\n",
      "LOF k = 60, best F-1 = 0.5020833333333333\n",
      "LOF k = 70, best F-1 = 0.5018867924528301\n",
      "LOF k = 80, best F-1 = 0.5120689655172415\n",
      "LOF k = 90, best F-1 = 0.4862068965517241\n",
      "LOF k = 100, best F-1 = 0.4773584905660378\n",
      "Best LOF F-1 = 0.5120689655172415\n",
      "100\n",
      "70\n",
      "40\n",
      "10\n",
      "80\n",
      "50\n",
      "20\n",
      "90\n",
      "60\n",
      "30\n",
      "KNN k = 10, best F-1 = 0.39841269841269844\n",
      "KNN k = 20, best F-1 = 0.42698412698412697\n",
      "KNN k = 30, best F-1 = 0.42698412698412697\n",
      "KNN k = 40, best F-1 = 0.43333333333333335\n",
      "KNN k = 50, best F-1 = 0.4317460317460317\n",
      "KNN k = 60, best F-1 = 0.42758620689655175\n",
      "KNN k = 70, best F-1 = 0.4241379310344828\n",
      "KNN k = 80, best F-1 = 0.4190476190476191\n",
      "KNN k = 90, best F-1 = 0.4174603174603174\n",
      "KNN k = 100, best F-1 = 0.41764705882352937\n",
      "Best KNN F-1 = 0.43333333333333335\n",
      "0.5\n",
      "0.6\n",
      "0.8\n",
      "0.9\n",
      "0.7\n",
      "IF = 0.5, best F-1 = 0.4094339622641509\n",
      "IF = 0.6, best F-1 = 0.43396226415094336\n",
      "IF = 0.7, best F-1 = 0.4037735849056604\n",
      "IF = 0.8, best F-1 = 0.38113207547169814\n",
      "IF = 0.9, best F-1 = 0.4113207547169811\n",
      "Best IF F-1 = 0.43396226415094336\n",
      "mahalanobis = 0.5411764705882354\n",
      "Best Mahala F-1 = 0.5411764705882354\n"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "all_scores = []\n",
    "f1s = []\n",
    "\n",
    "temp_lof_results = dict()\n",
    "unique_lof_ks = list(set(lof_krange)) \n",
    "for k in unique_lof_ks:\n",
    "#     print(k)\n",
    "    lof_scores = run_lof(X, y, k=k)\n",
    "    temp_lof_results[k] = lof_scores\n",
    "for i in range(len(lof_krange)):\n",
    "    lof_predictions, lof_scores, f1 = get_predictions(temp_lof_results[lof_krange[i]], num_outliers=N_range[i], method_name='LOF')\n",
    "    all_results.append(lof_predictions)\n",
    "    all_scores.append(lof_scores)\n",
    "    f1s.append(f1)\n",
    "best_lof_f1 = 0\n",
    "for i in np.sort(unique_lof_ks):\n",
    "    temp_f1 = max(np.array(f1s[0:60])[np.where(np.array(lof_krange) == i)[0]])\n",
    "    print('LOF k = {}, best F-1 = {}'.format(i, temp_f1))\n",
    "    best_lof_f1 = max(best_lof_f1, temp_f1)\n",
    "print('Best LOF F-1 = {}'.format(best_lof_f1))\n",
    "\n",
    "temp_knn_results = dict()\n",
    "unique_knn_ks = list(set(knn_krange)) \n",
    "for k in unique_knn_ks:\n",
    "    print(k)\n",
    "    knn_scores = run_knn(X, y, k=k)\n",
    "    temp_knn_results[k] = knn_scores\n",
    "for i in range(len(knn_krange)):\n",
    "    knn_predictions, knn_scores,f1 = get_predictions(temp_knn_results[knn_krange[i]], num_outliers=N_range[i], method_name='KNN')\n",
    "    all_results.append(knn_predictions)\n",
    "    all_scores.append(knn_scores)\n",
    "    f1s.append(f1)\n",
    "best_knn_f1 = 0\n",
    "for i in np.sort(unique_knn_ks):\n",
    "    temp_f1 = max(np.array(f1s[60:120])[np.where(np.array(knn_krange) == i)[0]])\n",
    "    print('KNN k = {}, best F-1 = {}'.format(i, temp_f1))\n",
    "    best_knn_f1 = max(best_knn_f1, temp_f1)\n",
    "print('Best KNN F-1 = {}'.format(best_knn_f1))\n",
    "    \n",
    "temp_if_results = dict()\n",
    "unique_if_features = list(set(if_range)) \n",
    "for k in unique_if_features:\n",
    "    print(k)\n",
    "    if_scores = run_isolation_forest(X, y, max_features=k)\n",
    "    temp_if_results[k] = if_scores\n",
    "for i in range(len(if_range)):\n",
    "    if_predictions, if_scores,f1 = get_predictions(temp_if_results[if_range[i]], num_outliers=N_range[i], method_name='IF')\n",
    "    all_results.append(if_predictions)\n",
    "    all_scores.append(if_scores)\n",
    "    f1s.append(f1)\n",
    "best_if_f1 = 0\n",
    "for i in np.sort(unique_if_features):\n",
    "    temp_f1 = max(np.array(f1s[120:150])[np.where(np.array(if_range) == i)[0]])\n",
    "    print('IF = {}, best F-1 = {}'.format(i, temp_f1))\n",
    "    best_if_f1 = max(best_if_f1, temp_f1)\n",
    "print('Best IF F-1 = {}'.format(best_if_f1))\n",
    "    \n",
    "mahalanobis_scores = run_mahalanobis(X, y)\n",
    "best_mahala_f1 = 0\n",
    "for i in range(len(mahalanobis_N_range)):\n",
    "    mahalanobis_predictions,mahalanobis_scores,f1 = get_predictions(mahalanobis_scores, num_outliers=mahalanobis_N_range[i], method_name='mahala')\n",
    "    all_results.append(mahalanobis_predictions)\n",
    "    all_scores.append(mahalanobis_scores)\n",
    "    best_mahala_f1 = max(best_mahala_f1, f1)\n",
    "    f1s.append(f1)\n",
    "print('mahalanobis = {}'.format(max(np.array(f1s[150:]))))\n",
    "print('Best Mahala F-1 = {}'.format(best_mahala_f1))\n",
    "L = np.stack(all_results).T\n",
    "scores = np.stack(all_scores).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5473, 156)\n",
      "(5473, 156)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(L))\n",
    "print(np.shape(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Majority Vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 for MV: 0.10857538035961271\n"
     ]
    }
   ],
   "source": [
    "mid = np.shape(L)[1]/2\n",
    "predictions = np.full((len(y)), 0)\n",
    "predictions[np.sum(L, axis = 1) > mid] = 1\n",
    "print('F1 for MV:', metrics.f1_score(y, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and load outlier scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "dataset_results = {'L': L, 'scores': scores, 'f1s': f1s}\n",
    "pickle.dump(dataset_results, open(\"smtp.pickle\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the outlier scores have already been computed, load it here\n",
    "import pickle\n",
    "dataset_results = pickle.load(open(\"kdd_large.pickle\", \"rb\"))\n",
    "L = dataset_results['L']\n",
    "scores = dataset_results['scores']\n",
    "f1s = dataset_results['f1s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_prev = L\n",
    "scores_prev = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = L_prev\n",
    "scores = scores_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5411764705882354\n",
      "(5473, 156)\n"
     ]
    }
   ],
   "source": [
    "print(max(f1s)) \n",
    "print(np.shape(L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_result_list = []\n",
    "classifier_result_list = []\n",
    "prediction_list = []\n",
    "cur_f1_scores = []\n",
    "prediction_high_conf_outliers = np.array([])\n",
    "prediction_high_conf_inliers = np.array([])\n",
    "prediction_classifier_disagree = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_range = np.array([[0, 60], [60, 120], [120, 150], [150, 156]])\n",
    "coef_index_range = np.array([[0, 10], [10, 20], [20, 25], [25, 26]])\n",
    "coef_remain_index = range(156)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(60):\n",
    "#     scores[scores[:,i] > 100,i] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index_range = np.array([[0, 120], [120, 240], [240, 270], [270, 276]])\n",
    "# coef_index_range = np.array([[0, 20], [20, 40], [40, 45], [45, 46]])\n",
    "# coef_remain_index = range(276)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_scores = []\n",
    "# for i in range(np.shape(scores)[1]):\n",
    "#     new_scores.append(np.argsort(np.argsort(scores[:,i]))/len(scores[:,i]))\n",
    "# scores = np.stack(new_scores).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 120, 121, 122, 123, 124, 150]\n",
      "(5473, 26)\n"
     ]
    }
   ],
   "source": [
    "# For knn, we run it on 10 different k values and 6 different Ns. However, we don't need to use all the 60 values \n",
    "# to train the two models, we just need to run the training on 10 columns with different k values. \n",
    "# (Because if k is fixed, the knn score is fixed. N is just used to determine whether the point is an outlier or not.)\n",
    "scores_for_training_indexes = []\n",
    "for i in range(len(index_range)):\n",
    "    start=index_range[i][0]\n",
    "    temp_range = coef_index_range[i][1]-coef_index_range[i][0]\n",
    "    scores_for_training_indexes  = scores_for_training_indexes + list(range(start, start+temp_range))\n",
    "print(scores_for_training_indexes) \n",
    "scores_for_training = scores[:, np.array(scores_for_training_indexes)]\n",
    "print(np.shape(scores_for_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_coef_proba(coef):\n",
    "    # weights --> ranking\n",
    "    pos_ranking = np.argsort(np.argsort(coef)) + 1\n",
    "#     print(pos_ranking)\n",
    "\n",
    "    # ranking --> u_k\n",
    "    num_coefs = len(coef)\n",
    "    normalize_factor = np.sum([max(0, np.log(num_coefs + 1) - np.log(j)) for j in range(1, num_coefs + 1)])\n",
    "    u_k = [max(0, np.log(num_coefs + 1) - np.log(j))/normalize_factor - 1/num_coefs for j in pos_ranking]\n",
    "    \n",
    "    # normalize u_k\n",
    "    normalized_u_k = (u_k-min(u_k))/(max(u_k)-min(u_k))\n",
    "#     print(normalized_u_k)\n",
    "\n",
    "    # u_k to probabilities\n",
    "    import scipy.stats\n",
    "    pdfs = np.array([scipy.stats.norm(0, np.std(normalized_u_k)).pdf(j) for j in normalized_u_k])\n",
    "#     preservation_rate = 0.8\n",
    "#     probabilities = pdfs * preservation_rate * num_coefs/np.sum(pdfs)\n",
    "    probabilities = (pdfs-min(pdfs))/(max(pdfs)-min(pdfs))\n",
    "#     probabilities = pdfs\n",
    "    print(probabilities)\n",
    "    return probabilities\n",
    "\n",
    "def filter_update_list(R, value_list):\n",
    "    P = 2\n",
    "    prediction = 0\n",
    "    for new_value in value_list:\n",
    "        K = P / (P + R)\n",
    "        prediction = prediction + K * (new_value - prediction)\n",
    "        P = (1 - K) * P\n",
    "    return prediction\n",
    "\n",
    "def get_kf_results(proba_list):\n",
    "    results = []\n",
    "    for i in range(np.shape(proba_list)[1]):\n",
    "        results.append(filter_update_list(0.1, proba_list[:, i]))\n",
    "    return np.array(results)\n",
    "\n",
    "def generate_decision_on_proba(probabilities):\n",
    "    return np.array([np.random.binomial(n=1, p = min(1, proba)) for proba in probabilities])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative train LR and classifier(SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n",
      "Iteration = 0, L shape = (620098, 156)\n",
      "num of inliers = 600280\n",
      "num of outliers = 0\n",
      "num of outliers = 1378\n",
      "Training data shape:  (601658, 26)\n",
      "Training data F-1 0.5953109072375128\n",
      "Training data accuracy: 0.9986803134006362\n",
      "Training data outlier accuracy: 0.42380261248185774\n",
      "(601658, 29)\n",
      "(601658,)\n"
     ]
    }
   ],
   "source": [
    "# stable version\n",
    "# Please note, to determine whether the point is outliers/inliers, we use all the 60 values, to actually train the \n",
    "# models, we use 10 values with different ks. \n",
    "high_confidence_threshold = 0.99\n",
    "low_confidence_threshold = 0.01\n",
    "LR_threshold = 0.5\n",
    "max_iter = 500\n",
    "union_inliers = False\n",
    "remain_params_tracking = np.array(range(0,np.max(coef_index_range)))\n",
    "training_data_F1 = []\n",
    "two_prediction_corr = []\n",
    "\n",
    "min_max_diff = []\n",
    "N_size = 6\n",
    "\n",
    "last_training_data_indexes = []\n",
    "counter = 0\n",
    "\n",
    "for i_range in range(0, 50):\n",
    "    print(\"##################################################################\")\n",
    "    print('Iteration = {}, L shape = {}'.format(i_range, np.shape(L)))\n",
    "    num_methods = np.shape(L)[1]\n",
    "    \n",
    "    agree_outlier_indexes = np.sum(L,axis=1)==np.shape(L)[1]\n",
    "#     print('All agree, Number of outliers = {}'.format(sum(agree_outlier_indexes)))\n",
    "    agree_inlier_indexes = np.sum(L,axis=1)==0\n",
    "#     print('All agree, Number of inliers = {}'.format(sum(agree_inlier_indexes)))\n",
    "\n",
    "    disagree_indexes = np.where(np.logical_or(np.sum(L,axis = 1)==0, np.sum(L,axis = 1)==num_methods)==0)[0]\n",
    "    \n",
    "#     all_inlier_indexes = np.where(agree_inlier_indexes)[0]\n",
    "    all_inlier_indexes = np.setdiff1d(np.where(agree_inlier_indexes)[0], prediction_high_conf_outliers)\n",
    "    if len(prediction_high_conf_inliers) >0:\n",
    "        if union_inliers:\n",
    "            all_inlier_indexes = np.union1d(np.setdiff1d(np.where(agree_inlier_indexes)[0], prediction_high_conf_outliers), prediction_high_conf_inliers)\n",
    "        else:\n",
    "            all_inlier_indexes = np.intersect1d(np.setdiff1d(np.where(agree_inlier_indexes)[0], prediction_high_conf_outliers), prediction_high_conf_inliers)\n",
    "    print('num of inliers = {}'.format(np.shape(all_inlier_indexes)[0]))\n",
    "#     all_outlier_indexes = np.union1d(np.where(agree_outlier_indexes)[0], self_agree_index_list)\n",
    "\n",
    "#     if(len(np.intersect1d(np.where(agree_outlier_indexes)[0], prediction_high_conf_outliers)) > 0 and\n",
    "#       (len(np.union1d(np.where(agree_outlier_indexes)[0], prediction_high_conf_outliers)) > 2000)):\n",
    "#         all_outlier_indexes = np.intersect1d(np.where(agree_outlier_indexes)[0], prediction_high_conf_outliers)\n",
    "#     else:\n",
    "    all_outlier_indexes = np.union1d(np.where(agree_outlier_indexes)[0], prediction_high_conf_outliers)\n",
    "#     if(len(all_outlier_indexes) > 1000):\n",
    "#         all_outlier_indexes = np.random.RandomState(1).permutation(all_outlier_indexes)[:1000]\n",
    "        \n",
    "#     all_outlier_indexes = np.union1d(np.where(agree_outlier_indexes)[0], prediction_high_conf_outliers)\n",
    "    print('num of outliers = {}'.format(np.shape(all_outlier_indexes)[0]))\n",
    "    all_inlier_indexes = np.setdiff1d(all_inlier_indexes, prediction_classifier_disagree)\n",
    "    \n",
    "    self_agree_index_list = []\n",
    "    if((len(all_outlier_indexes) == 0) or (len(all_inlier_indexes)/ len(all_outlier_indexes) > 1000)):\n",
    "        for i in range(0, len(index_range)):\n",
    "            if(index_range[i,1]-index_range[i,0] <= 6):\n",
    "                continue\n",
    "            temp_index = disagree_indexes[np.where(np.sum(L[disagree_indexes][:,index_range[i,0]: index_range[i,1]], axis = 1)==(index_range[i,1]-index_range[i,0]))[0]]\n",
    "            self_agree_index_list = np.union1d(self_agree_index_list, temp_index)\n",
    "        self_agree_index_list = [int(i) for i in self_agree_index_list]\n",
    "#     self_agree_index_list = np.random.RandomState(1).permutation(self_agree_index_list)[:500]\n",
    "    all_outlier_indexes = np.union1d(all_outlier_indexes, self_agree_index_list)\n",
    "    all_outlier_indexes = np.setdiff1d(all_outlier_indexes, prediction_classifier_disagree)\n",
    "    print('num of outliers = {}'.format(np.shape(all_outlier_indexes)[0]))\n",
    "    \n",
    "    \n",
    "    from sklearn.preprocessing import RobustScaler\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    data_indexes = np.concatenate((all_inlier_indexes, all_outlier_indexes), axis = 0)\n",
    "    data_indexes = np.array([int(i) for i in data_indexes])\n",
    "    labels = np.concatenate((np.zeros(len(all_inlier_indexes)), np.ones(len(all_outlier_indexes))), axis = 0)\n",
    "    transformer = RobustScaler().fit(scores_for_training)\n",
    "    scores_transformed = transformer.transform(scores_for_training)\n",
    "    training_data = scores_transformed[data_indexes]\n",
    "    print('Training data shape: ', np.shape(training_data))\n",
    "    training_data_F1.append(metrics.f1_score(y[data_indexes], labels))\n",
    "    print('Training data F-1', metrics.f1_score(y[data_indexes], labels))\n",
    "    accurate_training = (np.array(y[data_indexes]) == np.array(labels))\n",
    "    print('Training data accuracy:', sum(accurate_training)/len(labels))\n",
    "    print('Training data outlier accuracy:', sum(y[data_indexes][-int(sum(labels)):])/sum(labels))\n",
    "    \n",
    "    transformer = RobustScaler().fit(X)\n",
    "    X_transformed = transformer.transform(X)\n",
    "    X_training_data = X_transformed[data_indexes]\n",
    "    print(np.shape(X_training_data))\n",
    "    print(np.shape(labels))\n",
    "    \n",
    "    from sklearn.svm import SVC\n",
    "    clf_X = SVC(gamma='auto', probability=True, random_state=0)\n",
    "    clf_X.fit(X_training_data, labels)\n",
    "    clf_predictions_X = clf_X.predict(X_transformed)\n",
    "    clf_predict_proba_X = clf_X.predict_proba(X_transformed)[:,1]\n",
    "    SVM_threshold = 0.5\n",
    "    print(\"F-1 score from SVM:\",metrics.f1_score(y, np.array([int(i) for i in clf_predict_proba_X > SVM_threshold])))\n",
    "    cur_f1_scores.append(metrics.f1_score(y, np.array([int(i) for i in clf_predict_proba_X > SVM_threshold])))\n",
    "    print('Number of outliers by SVM:', sum(np.array([int(i) for i in clf_predict_proba_X > SVM_threshold])))\n",
    "    \n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    clf = LogisticRegression(random_state=0, penalty='l2', max_iter=max_iter).fit(training_data, labels) \n",
    "    clf_predictions = clf.predict(scores_transformed)\n",
    "    clf_predict_proba = clf.predict_proba(scores_transformed)[:,1]\n",
    "#     LR_threshold = np.array(np.sort(clf_predict_proba)[::-1])[int(sum(clf_predictions_X))]\n",
    "    print(\"F-1 score from LR:\",metrics.f1_score(y, np.array([int(i) for i in clf_predict_proba > LR_threshold])))\n",
    "    print('Number of outliers by LR: ', sum(np.array([int(i) for i in clf_predict_proba > LR_threshold])))\n",
    "    \n",
    "    agreed_outlier_indexes = np.where(np.sum(L,axis=1)==np.shape(L)[1])[0]\n",
    "    agreed_inlier_indexes = np.where(np.sum(L,axis=1)==0)[0]\n",
    "        \n",
    "    prediction_result_list.append(clf_predict_proba)\n",
    "    classifier_result_list.append(clf_predict_proba_X)\n",
    "    \n",
    "    prediction_list.append(np.array([int(i) for i in clf_predictions]))\n",
    "    \n",
    "    prediction_high_conf_outliers = np.intersect1d(np.where(prediction_result_list[-1] > high_confidence_threshold)[0],\n",
    "                                                   np.where(classifier_result_list[-1] > high_confidence_threshold)[0])\n",
    "#     print('length of prediction_high_conf_outliers:' , len(prediction_high_conf_outliers))\n",
    "    prediction_high_conf_inliers = np.intersect1d(np.where(prediction_result_list[-1] < low_confidence_threshold)[0],\n",
    "                                                   np.where(classifier_result_list[-1] < low_confidence_threshold)[0])\n",
    "#     print('length of prediction high conf inliers: ', len(prediction_high_conf_inliers))\n",
    "    \n",
    "    temp_prediction = np.array([int(i) for i in prediction_result_list[-1] > LR_threshold])\n",
    "    temp_classifier = np.array([int(i) for i in classifier_result_list[-1] > SVM_threshold])\n",
    "    prediction_classifier_disagree = np.where(temp_prediction != temp_classifier)[0]\n",
    "#     print('length of prediction-classifier disagree: {}'.format(len(prediction_classifier_disagree)))\n",
    "#     print('length of prediction-classifier disagree in training: {}'.format(len(np.where(temp_prediction[data_indexes] != temp_classifier[data_indexes])[0])))\n",
    "    print(np.corrcoef(clf_predict_proba,clf_predict_proba_X))\n",
    "    two_prediction_corr.append(np.corrcoef(clf_predict_proba,clf_predict_proba_X)[0,1])\n",
    "\n",
    "    if np.max(coef_index_range) >= 2:\n",
    "        if(len(prediction_high_conf_outliers) > 0 and len(prediction_high_conf_inliers) > 0):\n",
    "            new_data_indexes = np.concatenate((prediction_high_conf_outliers, prediction_high_conf_inliers), axis = 0)\n",
    "            new_data_indexes = np.array([int(i) for i in new_data_indexes])\n",
    "            new_labels = np.concatenate((np.ones(len(prediction_high_conf_outliers)), np.zeros(len(prediction_high_conf_inliers))), axis = 0)\n",
    "            clf_prune_2 = LogisticRegression(random_state=0, penalty='l2', max_iter=max_iter).fit(scores_transformed[new_data_indexes], new_labels) \n",
    "#             print(\"F-1 score from both LR and SVM:\",metrics.f1_score(y, np.array([int(i) for i in clf_prune_2.predict_proba(scores_transformed)[:,1] > 0.5])))\n",
    "#             print('Coef from both LR and SVM: ', clf_prune_2.coef_[0])\n",
    "            combined_coef = clf_prune_2.coef_[0]  \n",
    "        else:\n",
    "#             print('Coef from normal training: ', clf.coef_[0])\n",
    "            combined_coef = clf.coef_[0]\n",
    "#             print('Combined Coef: ',  combined_coef)\n",
    "\n",
    "        if(np.max(coef_index_range) >= 2 or \n",
    "           ((np.max(combined_coef)/np.min(combined_coef) >= 1.1) and np.max(coef_index_range) >= 2)):\n",
    "            if(len(set(combined_coef)) > 1):\n",
    "                cur_clf_coef = combined_coef \n",
    "                cutoff = max(max(0, np.mean(combined_coef)-np.std(combined_coef)),min(combined_coef))\n",
    "#                 print(cutoff)\n",
    "\n",
    "                remain_indexes_after_cond = (cur_clf_coef > cutoff) #np.logical_and(cur_clf_coef > cutoff, abs(cur_clf_coef) > 0.01) # # \n",
    "                remain_params_tracking = remain_params_tracking[remain_indexes_after_cond]\n",
    "                print(remain_params_tracking)\n",
    "                remain_indexes_after_cond_expanded = []\n",
    "                for i in range(0, len(coef_index_range)): #\n",
    "                    s_e_range = coef_index_range[i,1]-coef_index_range[i,0]\n",
    "                    s1, e1 = coef_index_range[i,0], coef_index_range[i,1]\n",
    "                    s2, e2 = index_range[i,0], index_range[i,1]\n",
    "                    saved_indexes = np.where(cur_clf_coef[s1:e1] > cutoff)[0]\n",
    "                    for j in range(N_size):\n",
    "                        remain_indexes_after_cond_expanded.extend(np.array(saved_indexes) + j * s_e_range + s2)\n",
    "\n",
    "                new_coef_index_range_seq = []\n",
    "                for i in range(0, len(coef_index_range)): #\n",
    "                    s, e = coef_index_range[i,0], coef_index_range[i,1]\n",
    "                    new_coef_index_range_seq.append(sum((remain_indexes_after_cond)[s:e]))\n",
    "\n",
    "                coef_index_range = []\n",
    "                index_range = []\n",
    "                cur_sum = 0\n",
    "                for i in range(0, len(new_coef_index_range_seq)):\n",
    "                    coef_index_range.append([cur_sum, cur_sum + new_coef_index_range_seq[i]])\n",
    "                    index_range.append([cur_sum * 6, 6 * (cur_sum + new_coef_index_range_seq[i])])\n",
    "                    cur_sum += new_coef_index_range_seq[i]\n",
    "\n",
    "                coef_index_range = np.array(coef_index_range)\n",
    "                index_range = np.array(index_range)\n",
    "                print(coef_index_range)\n",
    "#                 print(index_range)\n",
    "\n",
    "                L=L[:,remain_indexes_after_cond_expanded]\n",
    "                scores_for_training = scores_for_training[:, remain_indexes_after_cond]\n",
    "    if((len(last_training_data_indexes) == len(data_indexes)) and \n",
    "       (sum(last_training_data_indexes == data_indexes) == len(data_indexes)) and \n",
    "       (np.max(coef_index_range) < 2)):\n",
    "        counter =  counter + 1\n",
    "    else:\n",
    "        counter = 0\n",
    "    if(counter > 3):\n",
    "        break\n",
    "    last_training_data_indexes = data_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following code is used to run the previous code for multiple times, but the results do not vary that much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 120, 121, 122, 123, 124, 150]\n",
      "(6435, 26)\n",
      "Training data shape:  (4972, 26)\n",
      "Training data F-1 0.6486090775988287\n",
      "(4972, 36)\n",
      "(4972,)\n",
      "F-1 score from SVM: 0.6798226164079822\n",
      "F-1 score from LR: 0.5684425184807546\n",
      "Number of outliers by LR:  1887\n",
      "length of prediction_high_conf_outliers: 1524\n",
      "length of prediction high conf inliers:  3645\n",
      "[[1.        0.8054605]\n",
      " [0.8054605 1.       ]]\n",
      "F-1 score from both LR and SVM: 0.5758263941458491\n",
      "Coef from both LR and SVM:  [-2.83100263e-01 -3.99856073e-01 -2.68811602e-01 -1.49840618e-01\n",
      " -1.24860456e-01 -4.86986570e-02  1.51851196e-03  1.12958221e-01\n",
      "  1.98108632e-01  2.77749017e-01  1.60904209e+00  1.72673143e+00\n",
      "  1.82370081e+00  1.81498134e+00  1.80672879e+00  1.81666636e+00\n",
      "  1.80077497e+00  1.82487648e+00  1.81047103e+00  1.79117451e+00\n",
      "  6.18081682e-01  4.08150387e-01  6.69212781e-02  8.25602491e-01\n",
      "  5.15766889e-01  2.28698657e-01]\n",
      "0\n",
      "[ 6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25]\n",
      "[[ 0  4]\n",
      " [ 4 14]\n",
      " [14 19]\n",
      " [19 20]]\n",
      "[[  0  24]\n",
      " [ 24  84]\n",
      " [ 84 114]\n",
      " [114 120]]\n"
     ]
    }
   ],
   "source": [
    "index_range = np.array([[0, 60], [60, 120], [120, 150], [150, 156]])\n",
    "coef_index_range = np.array([[0, 10], [10, 20], [20, 25], [25, 26]])\n",
    "coef_remain_index = range(156)\n",
    "\n",
    "scores_for_training_indexes = []\n",
    "for i in range(len(index_range)):\n",
    "    start=index_range[i][0]\n",
    "    temp_range = coef_index_range[i][1]-coef_index_range[i][0]\n",
    "    scores_for_training_indexes  = scores_for_training_indexes + list(range(start, start+temp_range))\n",
    "print(scores_for_training_indexes) \n",
    "scores_for_training = scores[:, np.array(scores_for_training_indexes)]\n",
    "print(np.shape(scores_for_training))\n",
    "\n",
    "transformer = RobustScaler().fit(scores_for_training)\n",
    "scores_transformed = transformer.transform(scores_for_training)\n",
    "training_data = scores_transformed[data_indexes]\n",
    "print('Training data shape: ', np.shape(training_data))\n",
    "training_data_F1.append(metrics.f1_score(y[data_indexes], labels))\n",
    "print('Training data F-1', metrics.f1_score(y[data_indexes], labels))\n",
    "\n",
    "transformer = RobustScaler().fit(X)\n",
    "X_transformed = transformer.transform(X)\n",
    "X_training_data = X_transformed[data_indexes]\n",
    "print(np.shape(X_training_data))\n",
    "print(np.shape(labels))\n",
    "\n",
    "clf_X = SVC(gamma='auto', probability=True, random_state=0)\n",
    "clf_X.fit(X_training_data, labels)\n",
    "clf_predictions_X = clf_X.predict(X_transformed)\n",
    "clf_predict_proba_X = clf_X.predict_proba(X_transformed)[:,1]\n",
    "print(\"F-1 score from SVM:\",metrics.f1_score(y, np.array([int(i) for i in clf_predict_proba_X > 0.5])))\n",
    "cur_f1_scores.append(metrics.f1_score(y, np.array([int(i) for i in clf_predict_proba_X > 0.5])))\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "clf = LogisticRegression(random_state=0, penalty='l2', max_iter=max_iter).fit(training_data, labels) \n",
    "clf_predictions = clf.predict(scores_transformed)\n",
    "clf_predict_proba = clf.predict_proba(scores_transformed)[:,1]\n",
    "# LR_threshold = np.array(np.sort(clf_predict_proba)[::-1])[int(sum(clf_predictions_X))]\n",
    "print(\"F-1 score from LR:\",metrics.f1_score(y, np.array([int(i) for i in clf_predict_proba > LR_threshold])))\n",
    "print('Number of outliers by LR: ', sum(np.array([int(i) for i in clf_predict_proba > LR_threshold])))\n",
    "\n",
    "\n",
    "agreed_outlier_indexes = np.where(np.sum(L,axis=1)==np.shape(L)[1])[0]\n",
    "agreed_inlier_indexes = np.where(np.sum(L,axis=1)==0)[0]\n",
    "\n",
    "prediction_result_list.append(clf_predict_proba)\n",
    "classifier_result_list.append(clf_predict_proba_X)\n",
    "\n",
    "prediction_list.append(np.array([int(i) for i in clf_predictions]))\n",
    "\n",
    "prediction_high_conf_outliers = np.intersect1d(np.where(prediction_result_list[-1] > high_confidence_threshold)[0],\n",
    "                                               np.where(classifier_result_list[-1] > high_confidence_threshold)[0])\n",
    "print('length of prediction_high_conf_outliers:' , len(prediction_high_conf_outliers))\n",
    "prediction_high_conf_inliers = np.intersect1d(np.where(prediction_result_list[-1] < low_confidence_threshold)[0],\n",
    "                                               np.where(classifier_result_list[-1] < low_confidence_threshold)[0])\n",
    "print('length of prediction high conf inliers: ', len(prediction_high_conf_inliers))\n",
    "\n",
    "temp_prediction = np.array([int(i) for i in prediction_result_list[-1] > LR_threshold])\n",
    "temp_classifier = np.array([int(i) for i in classifier_result_list[-1] > 0.5])\n",
    "prediction_classifier_disagree = np.where(temp_prediction != temp_classifier)[0]\n",
    "print(np.corrcoef(clf_predict_proba,clf_predict_proba_X))\n",
    "\n",
    "L = L_prev\n",
    "remain_params_tracking = np.array(range(0,np.max(coef_index_range)))\n",
    "\n",
    "if np.max(coef_index_range) >= 2:\n",
    "    if(len(prediction_high_conf_outliers) > 0 and len(prediction_high_conf_inliers) > 0):\n",
    "        new_data_indexes = np.concatenate((prediction_high_conf_outliers, prediction_high_conf_inliers), axis = 0)\n",
    "        new_data_indexes = np.array([int(i) for i in new_data_indexes])\n",
    "        new_labels = np.concatenate((np.ones(len(prediction_high_conf_outliers)), np.zeros(len(prediction_high_conf_inliers))), axis = 0)\n",
    "        clf_prune_2 = LogisticRegression(random_state=0, penalty='l2', max_iter=max_iter).fit(scores_transformed[new_data_indexes], new_labels) \n",
    "        print(\"F-1 score from both LR and SVM:\",metrics.f1_score(y, np.array([int(i) for i in clf_prune_2.predict_proba(scores_transformed)[:,1] > 0.5])))\n",
    "        print('Coef from both LR and SVM: ', clf_prune_2.coef_[0])\n",
    "        combined_coef = clf_prune_2.coef_[0]  \n",
    "    else:\n",
    "        print('Coef from normal training: ', clf.coef_[0])\n",
    "        combined_coef = clf.coef_[0]\n",
    "        print('Combined Coef: ',  combined_coef)\n",
    "\n",
    "    if(np.max(coef_index_range) > 2 or \n",
    "       ((np.max(combined_coef)/np.min(combined_coef) >= 1.1) and np.max(coef_index_range) >= 2)):\n",
    "        if(len(set(combined_coef)) > 1):\n",
    "            cur_clf_coef = combined_coef \n",
    "            cutoff = max(max(0, np.mean(combined_coef)-np.std(combined_coef)),min(combined_coef))\n",
    "            print(cutoff)\n",
    "\n",
    "            remain_indexes_after_cond = (cur_clf_coef > cutoff) #np.logical_and(cur_clf_coef > cutoff, abs(cur_clf_coef) > 0.01) # # \n",
    "            remain_params_tracking = remain_params_tracking[remain_indexes_after_cond]\n",
    "            print(remain_params_tracking)\n",
    "            remain_indexes_after_cond_expanded = []\n",
    "            for i in range(0, len(coef_index_range)): #\n",
    "                s_e_range = coef_index_range[i,1]-coef_index_range[i,0]\n",
    "                s1, e1 = coef_index_range[i,0], coef_index_range[i,1]\n",
    "                s2, e2 = index_range[i,0], index_range[i,1]\n",
    "                saved_indexes = np.where(cur_clf_coef[s1:e1] > cutoff)[0]\n",
    "                for j in range(N_size):\n",
    "                    remain_indexes_after_cond_expanded.extend(np.array(saved_indexes) + j * s_e_range + s2)\n",
    "\n",
    "            new_coef_index_range_seq = []\n",
    "            for i in range(0, len(coef_index_range)): #\n",
    "                s, e = coef_index_range[i,0], coef_index_range[i,1]\n",
    "                new_coef_index_range_seq.append(sum((remain_indexes_after_cond)[s:e]))\n",
    "\n",
    "            coef_index_range = []\n",
    "            index_range = []\n",
    "            cur_sum = 0\n",
    "            for i in range(0, len(new_coef_index_range_seq)):\n",
    "                coef_index_range.append([cur_sum, cur_sum + new_coef_index_range_seq[i]])\n",
    "                index_range.append([cur_sum * 6, 6 * (cur_sum + new_coef_index_range_seq[i])])\n",
    "                cur_sum += new_coef_index_range_seq[i]\n",
    "\n",
    "            coef_index_range = np.array(coef_index_range)\n",
    "            index_range = np.array(index_range)\n",
    "            print(coef_index_range)\n",
    "            print(index_range)\n",
    "\n",
    "            L=L[:,remain_indexes_after_cond_expanded]\n",
    "            scores_for_training = scores_for_training[:, remain_indexes_after_cond]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n",
      "Iteration = 0, L shape = (6435, 120)\n",
      "(3868, 36)\n",
      "(3868,)\n",
      "F-1 score from SVM: 0.6676802780191139\n",
      "F-1 score from LR: 0.5887546468401488\n",
      "Number of outliers by LR:  2268\n",
      "[[1.         0.83877918]\n",
      " [0.83877918 1.        ]]\n",
      "[ 8  9 10 11 12 13 14 15 16 17 18 19 20 21 23 24]\n",
      "[[ 0  2]\n",
      " [ 2 12]\n",
      " [12 16]\n",
      " [16 16]]\n",
      "##################################################################\n",
      "Iteration = 1, L shape = (6435, 96)\n",
      "(4505, 36)\n",
      "(4505,)\n",
      "F-1 score from SVM: 0.6648032681143841\n",
      "F-1 score from LR: 0.6057273768613973\n",
      "Number of outliers by LR:  2329\n",
      "[[1.         0.84494523]\n",
      " [0.84494523 1.        ]]\n",
      "[ 8  9 10 11 12 13 14 15 16 17 18 19]\n",
      "[[ 0  2]\n",
      " [ 2 12]\n",
      " [12 12]\n",
      " [12 12]]\n",
      "##################################################################\n",
      "Iteration = 2, L shape = (6435, 72)\n",
      "(4939, 36)\n",
      "(4939,)\n",
      "F-1 score from SVM: 0.6626865671641791\n",
      "F-1 score from LR: 0.5794010889292197\n",
      "Number of outliers by LR:  2372\n",
      "[[1.         0.82674177]\n",
      " [0.82674177 1.        ]]\n",
      "[ 8  9 12 13 14 15 16 17 18 19]\n",
      "[[ 0  2]\n",
      " [ 2 10]\n",
      " [10 10]\n",
      " [10 10]]\n",
      "##################################################################\n",
      "Iteration = 3, L shape = (6435, 60)\n",
      "(4658, 36)\n",
      "(4658,)\n",
      "F-1 score from SVM: 0.657516891891892\n",
      "F-1 score from LR: 0.5660124888492417\n",
      "Number of outliers by LR:  2448\n",
      "[[1.         0.81999699]\n",
      " [0.81999699 1.        ]]\n",
      "[12 13 14 15 16 17 18 19]\n",
      "[[0 0]\n",
      " [0 8]\n",
      " [8 8]\n",
      " [8 8]]\n",
      "##################################################################\n",
      "Iteration = 4, L shape = (6435, 48)\n",
      "(5068, 36)\n",
      "(5068,)\n",
      "F-1 score from SVM: 0.6500519210799585\n",
      "F-1 score from LR: 0.5679771579178563\n",
      "Number of outliers by LR:  2517\n",
      "[[1.         0.82426019]\n",
      " [0.82426019 1.        ]]\n",
      "[14 15 16 17 18 19]\n",
      "[[0 0]\n",
      " [0 6]\n",
      " [6 6]\n",
      " [6 6]]\n",
      "##################################################################\n",
      "Iteration = 5, L shape = (6435, 36)\n",
      "(5203, 36)\n",
      "(5203,)\n",
      "F-1 score from SVM: 0.6443531827515401\n",
      "F-1 score from LR: 0.5730968858131489\n",
      "Number of outliers by LR:  2588\n",
      "[[1.         0.84173353]\n",
      " [0.84173353 1.        ]]\n",
      "[15 16 17 18 19]\n",
      "[[0 0]\n",
      " [0 5]\n",
      " [5 5]\n",
      " [5 5]]\n",
      "##################################################################\n",
      "Iteration = 6, L shape = (6435, 30)\n",
      "(5246, 36)\n",
      "(5246,)\n",
      "F-1 score from SVM: 0.6346386758175212\n",
      "F-1 score from LR: 0.5784563189143341\n",
      "Number of outliers by LR:  2680\n",
      "[[1.         0.84870119]\n",
      " [0.84870119 1.        ]]\n",
      "[16 17 18 19]\n",
      "[[0 0]\n",
      " [0 4]\n",
      " [4 4]\n",
      " [4 4]]\n",
      "##################################################################\n",
      "Iteration = 7, L shape = (6435, 24)\n",
      "(5263, 36)\n",
      "(5263,)\n",
      "F-1 score from SVM: 0.6316636491651578\n",
      "F-1 score from LR: 0.5814294697128485\n",
      "Number of outliers by LR:  2735\n",
      "[[1.         0.85304298]\n",
      " [0.85304298 1.        ]]\n",
      "[17 18 19]\n",
      "[[0 0]\n",
      " [0 3]\n",
      " [3 3]\n",
      " [3 3]]\n",
      "##################################################################\n",
      "Iteration = 8, L shape = (6435, 18)\n",
      "(5250, 36)\n",
      "(5250,)\n",
      "F-1 score from SVM: 0.6275999999999999\n",
      "F-1 score from LR: 0.5832290362953693\n",
      "Number of outliers by LR:  2758\n",
      "[[1.         0.85594167]\n",
      " [0.85594167 1.        ]]\n",
      "[18 19]\n",
      "[[0 0]\n",
      " [0 2]\n",
      " [2 2]\n",
      " [2 2]]\n",
      "##################################################################\n",
      "Iteration = 9, L shape = (6435, 12)\n",
      "(5213, 36)\n",
      "(5213,)\n",
      "F-1 score from SVM: 0.6255489021956088\n",
      "F-1 score from LR: 0.5880398671096346\n",
      "Number of outliers by LR:  2780\n",
      "[[1.         0.86083359]\n",
      " [0.86083359 1.        ]]\n",
      "[19]\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [1 1]]\n",
      "##################################################################\n",
      "Iteration = 10, L shape = (6435, 6)\n",
      "(5155, 36)\n",
      "(5155,)\n",
      "F-1 score from SVM: 0.6285829632620106\n",
      "F-1 score from LR: 0.5904761904761905\n",
      "Number of outliers by LR:  2794\n",
      "[[1.         0.86806797]\n",
      " [0.86806797 1.        ]]\n",
      "##################################################################\n",
      "Iteration = 11, L shape = (6435, 6)\n",
      "(5016, 36)\n",
      "(5016,)\n",
      "F-1 score from SVM: 0.6164630482675746\n",
      "F-1 score from LR: 0.5905767668562144\n",
      "Number of outliers by LR:  2888\n",
      "[[1.         0.85120331]\n",
      " [0.85120331 1.        ]]\n",
      "##################################################################\n",
      "Iteration = 12, L shape = (6435, 6)\n",
      "(4873, 36)\n",
      "(4873,)\n",
      "F-1 score from SVM: 0.6088488645262333\n",
      "F-1 score from LR: 0.5908449284129864\n",
      "Number of outliers by LR:  2923\n",
      "[[1.         0.83801486]\n",
      " [0.83801486 1.        ]]\n",
      "##################################################################\n",
      "Iteration = 13, L shape = (6435, 6)\n",
      "(4896, 36)\n",
      "(4896,)\n",
      "F-1 score from SVM: 0.6075801749271136\n",
      "F-1 score from LR: 0.5902232951116475\n",
      "Number of outliers by LR:  2935\n",
      "[[1.         0.83911608]\n",
      " [0.83911608 1.        ]]\n",
      "##################################################################\n",
      "Iteration = 14, L shape = (6435, 6)\n",
      "(4902, 36)\n",
      "(4902,)\n",
      "F-1 score from SVM: 0.6071290197597831\n",
      "F-1 score from LR: 0.5900783289817232\n",
      "Number of outliers by LR:  2943\n",
      "[[1.         0.83803367]\n",
      " [0.83803367 1.        ]]\n",
      "##################################################################\n",
      "Iteration = 15, L shape = (6435, 6)\n",
      "(4888, 36)\n",
      "(4888,)\n",
      "F-1 score from SVM: 0.6061074603788172\n",
      "F-1 score from LR: 0.5900783289817232\n",
      "Number of outliers by LR:  2943\n",
      "[[1.         0.83729808]\n",
      " [0.83729808 1.        ]]\n",
      "##################################################################\n",
      "Iteration = 16, L shape = (6435, 6)\n",
      "(4892, 36)\n",
      "(4892,)\n",
      "F-1 score from SVM: 0.6057915057915059\n",
      "F-1 score from LR: 0.5904073851093719\n",
      "Number of outliers by LR:  2947\n",
      "[[1.         0.83717154]\n",
      " [0.83717154 1.        ]]\n",
      "##################################################################\n",
      "Iteration = 17, L shape = (6435, 6)\n",
      "(4888, 36)\n",
      "(4888,)\n"
     ]
    }
   ],
   "source": [
    "last_training_data_indexes = []\n",
    "counter = 0\n",
    "\n",
    "for i_range in range(0, 50):\n",
    "    print(\"##################################################################\")\n",
    "    print('Iteration = {}, L shape = {}'.format(i_range, np.shape(L)))\n",
    "    num_methods = np.shape(L)[1]\n",
    "\n",
    "#     agree_outlier_indexes = (np.sum(L,axis=1)==np.shape(L)[1])\n",
    "#     print('All agree, Number of outliers = {}'.format(sum(agree_outlier_indexes)))\n",
    "#     agree_inlier_indexes = (np.sum(L,axis=1)==0)\n",
    "#     print('All agree, Number of inliers = {}'.format(sum(agree_inlier_indexes)))\n",
    "\n",
    "#     all_inlier_indexes = np.union1d(np.setdiff1d(np.where(agree_inlier_indexes)[0], prediction_high_conf_outliers), prediction_high_conf_inliers)\n",
    "#     print('num of inliers = {}'.format(np.shape(all_inlier_indexes)[0]))\n",
    "#     all_outlier_indexes = np.union1d(np.where(agree_outlier_indexes)[0], prediction_high_conf_outliers)\n",
    "#     print('num of outliers = {}'.format(np.shape(all_outlier_indexes)[0]))\n",
    "\n",
    "#     disagree_indexes = np.where(np.logical_or(np.sum(L,axis = 1)==0, np.sum(L,axis = 1)==num_methods)==0)[0]\n",
    "\n",
    "    ########################################################################\n",
    "\n",
    "    agree_outlier_indexes = np.sum(L,axis=1)==np.shape(L)[1]\n",
    "#     print('All agree, Number of outliers = {}'.format(sum(agree_outlier_indexes)))\n",
    "    agree_inlier_indexes = np.sum(L,axis=1)==0\n",
    "#     print('All agree, Number of inliers = {}'.format(sum(agree_inlier_indexes)))\n",
    "\n",
    "    disagree_indexes = np.where(np.logical_or(np.sum(L,axis = 1)==0, np.sum(L,axis = 1)==num_methods)==0)[0]\n",
    "    # print('Number of disagreed points = {}'.format(len(disagree_indexes)))\n",
    "    # print('Number of disagreed points (true outliers) = {}'.format(sum(y[disagree_indexes] == 1)))\n",
    "    # print('Number of disagreed points (true inliers) = {}'.format(sum(y[disagree_indexes] == 0)))\n",
    "\n",
    "#     all_inlier_indexes = np.where(agree_inlier_indexes)[0]\n",
    "    all_inlier_indexes = np.setdiff1d(np.where(agree_inlier_indexes)[0], prediction_high_conf_outliers)\n",
    "    if len(prediction_high_conf_inliers) >0:\n",
    "        all_inlier_indexes = np.intersect1d(np.setdiff1d(np.where(agree_inlier_indexes)[0], prediction_high_conf_outliers), prediction_high_conf_inliers)\n",
    "#     print('num of inliers = {}'.format(np.shape(all_inlier_indexes)[0]))\n",
    "#     all_outlier_indexes = np.union1d(np.where(agree_outlier_indexes)[0], self_agree_index_list)\n",
    "\n",
    "#     if(len(np.intersect1d(np.where(agree_outlier_indexes)[0], prediction_high_conf_outliers)) > 0 and\n",
    "#       (len(np.union1d(np.where(agree_outlier_indexes)[0], prediction_high_conf_outliers)) > 2000)):\n",
    "#         all_outlier_indexes = np.intersect1d(np.where(agree_outlier_indexes)[0], prediction_high_conf_outliers)\n",
    "#     else:\n",
    "    all_outlier_indexes = np.union1d(np.where(agree_outlier_indexes)[0], prediction_high_conf_outliers)\n",
    "#     if(len(all_outlier_indexes) > 1000):\n",
    "#         all_outlier_indexes = np.random.RandomState(1).permutation(all_outlier_indexes)[:1000]\n",
    "        \n",
    "#     all_outlier_indexes = np.union1d(np.where(agree_outlier_indexes)[0], prediction_high_conf_outliers)\n",
    "#     print('num of outliers = {}'.format(np.shape(all_outlier_indexes)[0]))\n",
    "    all_inlier_indexes = np.setdiff1d(all_inlier_indexes, prediction_classifier_disagree)\n",
    "    \n",
    "    self_agree_index_list = []\n",
    "    if((len(all_outlier_indexes) == 0) or (len(all_inlier_indexes)/ len(all_outlier_indexes) > 1000)):\n",
    "        for i in range(0, len(index_range)):\n",
    "            if(index_range[i,1]-index_range[i,0] <= 6):\n",
    "                continue\n",
    "            temp_index = disagree_indexes[np.where(np.sum(L[disagree_indexes][:,index_range[i,0]: index_range[i,1]], axis = 1)==(index_range[i,1]-index_range[i,0]))[0]]\n",
    "            self_agree_index_list = np.union1d(self_agree_index_list, temp_index)\n",
    "        self_agree_index_list = [int(i) for i in self_agree_index_list]\n",
    "#     self_agree_index_list = np.random.RandomState(1).permutation(self_agree_index_list)[:500]\n",
    "    all_outlier_indexes = np.union1d(all_outlier_indexes, self_agree_index_list)\n",
    "    all_outlier_indexes = np.setdiff1d(all_outlier_indexes, prediction_classifier_disagree)\n",
    "#     print('num of outliers = {}'.format(np.shape(all_outlier_indexes)[0]))\n",
    "    \n",
    "    \n",
    "    from sklearn.preprocessing import RobustScaler\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    data_indexes = np.concatenate((all_inlier_indexes, all_outlier_indexes), axis = 0)\n",
    "    data_indexes = np.array([int(i) for i in data_indexes])\n",
    "    labels = np.concatenate((np.zeros(len(all_inlier_indexes)), np.ones(len(all_outlier_indexes))), axis = 0)\n",
    "    transformer = RobustScaler().fit(scores_for_training)\n",
    "    scores_transformed = transformer.transform(scores_for_training)\n",
    "    training_data = scores_transformed[data_indexes]\n",
    "#     print('Training data shape: ', np.shape(training_data))\n",
    "    training_data_F1.append(metrics.f1_score(y[data_indexes], labels))\n",
    "#     print('Training data F-1', metrics.f1_score(y[data_indexes], labels))\n",
    "    \n",
    "    transformer = RobustScaler().fit(X)\n",
    "    X_transformed = transformer.transform(X)\n",
    "    X_training_data = X_transformed[data_indexes]\n",
    "    print(np.shape(X_training_data))\n",
    "    print(np.shape(labels))\n",
    "\n",
    "    clf_X = SVC(gamma='auto', probability=True, random_state=0)\n",
    "    clf_X.fit(X_training_data, labels)\n",
    "    clf_predictions_X = clf_X.predict(X_transformed)\n",
    "    clf_predict_proba_X = clf_X.predict_proba(X_transformed)[:,1]\n",
    "    print(\"F-1 score from SVM:\",metrics.f1_score(y, np.array([int(i) for i in clf_predict_proba_X > 0.5])))\n",
    "    cur_f1_scores.append(metrics.f1_score(y, np.array([int(i) for i in clf_predict_proba_X > 0.5])))\n",
    "    \n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.svm import SVC\n",
    "    clf = LogisticRegression(random_state=0, penalty='l2', max_iter=max_iter).fit(training_data, labels) \n",
    "    clf_predictions = clf.predict(scores_transformed)\n",
    "    clf_predict_proba = clf.predict_proba(scores_transformed)[:,1]\n",
    "    LR_threshold = np.array(np.sort(clf_predict_proba)[::-1])[int(sum(clf_predictions_X))]\n",
    "    print(\"F-1 score from LR:\",metrics.f1_score(y, np.array([int(i) for i in clf_predict_proba > LR_threshold])))\n",
    "    print('Number of outliers by LR: ', sum(np.array([int(i) for i in clf_predict_proba > LR_threshold])))\n",
    "    \n",
    "    \n",
    "    agreed_outlier_indexes = np.where(np.sum(L,axis=1)==np.shape(L)[1])[0]\n",
    "    agreed_inlier_indexes = np.where(np.sum(L,axis=1)==0)[0]\n",
    "        \n",
    "    prediction_result_list.append(clf_predict_proba)\n",
    "    classifier_result_list.append(clf_predict_proba_X)\n",
    "    \n",
    "    prediction_list.append(np.array([int(i) for i in clf_predictions]))\n",
    "    print(np.corrcoef(clf_predict_proba,clf_predict_proba_X))\n",
    "    \n",
    "    prediction_high_conf_outliers = np.intersect1d(np.where(prediction_result_list[-1] > high_confidence_threshold)[0],\n",
    "                                                   np.where(classifier_result_list[-1] > high_confidence_threshold)[0])\n",
    "#     print('length of prediction_high_conf_outliers:' , len(prediction_high_conf_outliers))\n",
    "    prediction_high_conf_inliers = np.intersect1d(np.where(prediction_result_list[-1] < low_confidence_threshold)[0],\n",
    "                                                   np.where(classifier_result_list[-1] < low_confidence_threshold)[0])\n",
    "#     print('length of prediction high conf inliers: ', len(prediction_high_conf_inliers))\n",
    "    \n",
    "    temp_prediction = np.array([int(i) for i in prediction_result_list[-1] > LR_threshold])\n",
    "    temp_classifier = np.array([int(i) for i in classifier_result_list[-1] > 0.5])\n",
    "    prediction_classifier_disagree = np.where(temp_prediction != temp_classifier)[0]\n",
    "    \n",
    "    if np.max(coef_index_range) >= 2:\n",
    "        if(len(prediction_high_conf_outliers) > 0 and len(prediction_high_conf_inliers) > 0):\n",
    "            new_data_indexes = np.concatenate((prediction_high_conf_outliers, prediction_high_conf_inliers), axis = 0)\n",
    "            new_data_indexes = np.array([int(i) for i in new_data_indexes])\n",
    "            new_labels = np.concatenate((np.ones(len(prediction_high_conf_outliers)), np.zeros(len(prediction_high_conf_inliers))), axis = 0)\n",
    "            clf_prune_2 = LogisticRegression(random_state=0, penalty='l2', max_iter=max_iter).fit(scores_transformed[new_data_indexes], new_labels) \n",
    "#             print(\"F-1 score from both LR and SVM:\",metrics.f1_score(y, np.array([int(i) for i in clf_prune_2.predict_proba(scores_transformed)[:,1] > 0.5])))\n",
    "#             print('Coef from both LR and SVM: ', clf_prune_2.coef_[0])\n",
    "            combined_coef = clf_prune_2.coef_[0]  \n",
    "        else:\n",
    "#             print('Coef from normal training: ', clf.coef_[0])\n",
    "            combined_coef = clf.coef_[0]\n",
    "#             print('Combined Coef: ',  combined_coef)\n",
    "\n",
    "        if(np.max(coef_index_range) >= 2 or \n",
    "           ((np.max(combined_coef)/np.min(combined_coef) >= 1.1) and np.max(coef_index_range) >= 2)):\n",
    "            if(len(set(combined_coef)) > 1):\n",
    "                cur_clf_coef = combined_coef \n",
    "                cutoff = max(max(0, np.mean(combined_coef)-np.std(combined_coef)),min(combined_coef))\n",
    "#                 print(cutoff)\n",
    "\n",
    "                remain_indexes_after_cond = (cur_clf_coef > cutoff) #np.logical_and(cur_clf_coef > cutoff, abs(cur_clf_coef) > 0.01) # # \n",
    "                remain_params_tracking = remain_params_tracking[remain_indexes_after_cond]\n",
    "                print(remain_params_tracking)\n",
    "                remain_indexes_after_cond_expanded = []\n",
    "                for i in range(0, len(coef_index_range)): #\n",
    "                    s_e_range = coef_index_range[i,1]-coef_index_range[i,0]\n",
    "                    s1, e1 = coef_index_range[i,0], coef_index_range[i,1]\n",
    "                    s2, e2 = index_range[i,0], index_range[i,1]\n",
    "                    saved_indexes = np.where(cur_clf_coef[s1:e1] > cutoff)[0]\n",
    "                    for j in range(N_size):\n",
    "                        remain_indexes_after_cond_expanded.extend(np.array(saved_indexes) + j * s_e_range + s2)\n",
    "\n",
    "                new_coef_index_range_seq = []\n",
    "                for i in range(0, len(coef_index_range)): #\n",
    "                    s, e = coef_index_range[i,0], coef_index_range[i,1]\n",
    "                    new_coef_index_range_seq.append(sum((remain_indexes_after_cond)[s:e]))\n",
    "\n",
    "                coef_index_range = []\n",
    "                index_range = []\n",
    "                cur_sum = 0\n",
    "                for i in range(0, len(new_coef_index_range_seq)):\n",
    "                    coef_index_range.append([cur_sum, cur_sum + new_coef_index_range_seq[i]])\n",
    "                    index_range.append([cur_sum * 6, 6 * (cur_sum + new_coef_index_range_seq[i])])\n",
    "                    cur_sum += new_coef_index_range_seq[i]\n",
    "\n",
    "                coef_index_range = np.array(coef_index_range)\n",
    "                index_range = np.array(index_range)\n",
    "                print(coef_index_range)\n",
    "#                 print(index_range)\n",
    "\n",
    "                L=L[:,remain_indexes_after_cond_expanded]\n",
    "                scores_for_training = scores_for_training[:, remain_indexes_after_cond]\n",
    "    if((len(last_training_data_indexes) == len(data_indexes)) and \n",
    "       (sum(last_training_data_indexes == data_indexes) == len(data_indexes)) and \n",
    "       (np.max(coef_index_range) < 2)):\n",
    "        counter =  counter + 1\n",
    "    else:\n",
    "        counter = 0\n",
    "    if(counter > 3):\n",
    "        break\n",
    "    last_training_data_indexes = data_indexes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
