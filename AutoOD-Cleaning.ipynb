{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn import metrics\n",
    "import scipy as sp\n",
    "import logging\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "    \n",
    "def run_lof(X, y, k=60):\n",
    "    clf = LocalOutlierFactor(n_neighbors=k)\n",
    "    clf.fit(X)\n",
    "    lof_scores = -clf.negative_outlier_factor_\n",
    "    return lof_scores\n",
    "\n",
    "def get_predictions(scores, num_outliers = 400, method_name = 'LOF'):\n",
    "    threshold = np.sort(scores)[::-1][num_outliers]\n",
    "    # threshold, max_f1 = get_best_f1_score(y, lof_scores)\n",
    "    predictions = np.array(scores > threshold)\n",
    "    predictions = np.array([int(i) for i in predictions])\n",
    "#     print('F1 for {} : {}'.format(method_name, metrics.f1_score(y, predictions)))\n",
    "    return predictions, scores, metrics.f1_score(y, predictions)\n",
    "\n",
    "def get_best_F1(scores):\n",
    "    best_f1 = 0\n",
    "    for i in range(np.shape(scores)[0]):\n",
    "        threshold = np.sort(scores)[::-1][i]\n",
    "        predictions = np.array(scores > threshold)\n",
    "        predictions = np.array([int(i) for i in predictions])\n",
    "        cur_f1 = metrics.f1_score(y, predictions)\n",
    "        best_f1 = max(cur_f1, best_f1)\n",
    "    return best_f1\n",
    "\n",
    "def run_knn(X, y, k=60):\n",
    "    neigh = NearestNeighbors(n_neighbors=k)\n",
    "    neigh.fit(X)\n",
    "    knn_dists = neigh.kneighbors(X)[0][:,-1]\n",
    "    return knn_dists\n",
    "\n",
    "def run_isolation_forest(X, y, max_features = 1.0):\n",
    "    # training the model\n",
    "    clf = IsolationForest(random_state=42,max_features=max_features)\n",
    "    clf.fit(X)\n",
    "    # predictions\n",
    "    sklearn_score_anomalies = clf.decision_function(X)\n",
    "    if_scores = [-1*s + 0.5 for s in sklearn_score_anomalies]\n",
    "    return if_scores\n",
    "\n",
    "def mahalanobis(x):\n",
    "    \"\"\"Compute the Mahalanobis Distance between each row of x and the data\n",
    "    \"\"\"\n",
    "    x_minus_mu = x - np.mean(x)\n",
    "    cov = np.cov(x.T)\n",
    "    det = np.linalg.det(cov)\n",
    "    if det != 0:\n",
    "        inv_covmat = sp.linalg.inv(cov)\n",
    "    else:\n",
    "        inv_covmat = sp.linalg.pinv(cov)\n",
    "    results = []\n",
    "    x_minus_mu = np.array(x_minus_mu)\n",
    "    for i in range(np.shape(x)[0]):\n",
    "        cur_data = x_minus_mu[i,:]\n",
    "        results.append(np.dot(np.dot(x_minus_mu[i,:], inv_covmat), x_minus_mu[i,:].T))\n",
    "    return np.array(results)\n",
    "#     left_term = np.dot(x_minus_mu, inv_covmat)\n",
    "#     mahal = np.dot(left_term, x_minus_mu.T)\n",
    "#     print(mahal.diagonal())\n",
    "#     return mahal.diagonal()\n",
    "\n",
    "def run_mahalanobis(X, y):\n",
    "    # training the model\n",
    "    dist = mahalanobis(x=X)\n",
    "    return dist\n",
    "\n",
    "def load_dataset(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        data, meta = arff.loadarff(f)\n",
    "    data = pd.DataFrame(data)\n",
    "    X = data.drop(columns=['id', 'outlier'])\n",
    "    # Map dataframe to encode values and put values into a numpy array\n",
    "    y = data[\"outlier\"].map(lambda x: 1 if x == b'yes' else 0).values\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "log_interval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils import data\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "torch.manual_seed(0)\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, dim = 10):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(dim, 50)\n",
    "        self.fc2 = nn.Linear(50, 100)\n",
    "        self.fc3 = nn.Linear(100,50)\n",
    "        self.fc4 = nn.Linear(50,2)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def inference_NN(net, testing_X, testing_y = None):\n",
    "    test_dataloader = data.DataLoader(data.TensorDataset(torch.tensor(testing_X), torch.tensor(testing_y)), \n",
    "                                      batch_size=100, shuffle=False) \n",
    "    net.eval()\n",
    "    predict_proba = []\n",
    "    for batch_idx, (input_data, target) in enumerate(test_dataloader):\n",
    "        input_data = Variable(input_data)\n",
    "        net_out = net(input_data.float())\n",
    "        predict_proba.append(F.softmax(net_out, dim=1).data.numpy())\n",
    "    return np.concatenate(predict_proba)\n",
    "\n",
    "def run_NN(X,y, epochs = 3,  dim = 10):\n",
    "    net = Net(dim)\n",
    "    # create a stochastic gradient descent optimizer\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-4)\n",
    "    # create a loss function\n",
    "    criterion = nn.NLLLoss()\n",
    "    # create dataset\n",
    "    tensor_x = torch.tensor(X) # transform to torch tensor\n",
    "    tensor_y = torch.tensor(y,dtype=torch.long)\n",
    "    my_dataset = data.TensorDataset(tensor_x,tensor_y) # create your datset\n",
    "#     class_sample_count = np.array([len(np.where(y == t)[0]) for t in np.unique(y)])\n",
    "#     weight = 1. / class_sample_count\n",
    "#     samples_weight = np.array([weight[t] for t in y])\n",
    "#     samples_weight = torch.from_numpy(samples_weight)\n",
    "#     samples_weight = samples_weight.double()\n",
    "#     sampler = data.WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    train_dataloader = data.DataLoader(my_dataset, batch_size=100, shuffle = True) # create your dataloader\n",
    "    \n",
    "    # run the main training loop\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "        for batch_idx, (input_data, target) in enumerate(train_dataloader):\n",
    "            input_data, target = Variable(input_data), Variable(target)\n",
    "            net_out = net(input_data.float())\n",
    "            loss = criterion(net_out, target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        net.eval()\n",
    "        losses = AverageMeter()\n",
    "        top1 = AverageMeter()\n",
    "        top2 = AverageMeter()\n",
    "    \n",
    "    net.eval()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top2 = AverageMeter()\n",
    "    loss_list = []\n",
    "    for batch_idx, (input_data, target) in enumerate(data.DataLoader(my_dataset, batch_size=1, shuffle=False)):\n",
    "        input_data, target = Variable(input_data), Variable(target)\n",
    "        net_out = net(input_data.float())\n",
    "#             print(F.softmax(net_out, dim=1))\n",
    "        loss = criterion(net_out, target)\n",
    "        prec = accuracy(net_out.data, target)\n",
    "        losses.update(loss.data, input_data.size(0))\n",
    "        loss_list.append(loss.data.numpy())\n",
    "        top1.update(prec[0], input_data.size(0))\n",
    "    print('Final Training Result: Loss {loss.avg:.8f}\\t'\n",
    "              'Prec @ 1 {top1.avg:.3f}%'.format(\n",
    "               loss=losses, top1=top1))   \n",
    "    return np.array(loss_list), net, losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_proba = run_NN(X.values,y, 10, get_prediction = True, testing_X = X.values, testing_y = y)\n",
    "# sum(predict_proba[:,1] > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load SpamBase dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4207, 57) (4207,)\n",
      "0.39909674352270025\n"
     ]
    }
   ],
   "source": [
    "filename = './SpamBase_withoutdupl_norm_40.arff'\n",
    "X, y = load_dataset(filename=filename)\n",
    "print(np.shape(X), np.shape(y))\n",
    "K = 9\n",
    "N = 1679\n",
    "class_balance = [1- N/4207.0, N/4207.0]\n",
    "lof_krange = list(range(10,110,10)) * 6\n",
    "knn_krange = list(range(10,110,10)) * 6\n",
    "if_range = [0.5, 0.6, 0.7, 0.8, 0.9] * 6\n",
    "# mahalanobis_N_range=[N]\n",
    "mahalanobis_N_range = [1400, 1500, 1600, 1700, 1800, 1900]\n",
    "if_N_range = np.sort(mahalanobis_N_range * 5)\n",
    "N_range = np.sort(mahalanobis_N_range * 10) \n",
    "print(N/len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pageblock dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5473, 10) (5473,)\n"
     ]
    }
   ],
   "source": [
    "filename = './PageBlocks/PageBlocks_norm_10.arff'\n",
    "X, y = load_dataset(filename=filename)\n",
    "print(np.shape(X), np.shape(y))\n",
    "K = 80\n",
    "N = 560\n",
    "# num_outliers = [N, N, N, N]\n",
    "class_balance = [0.9, 0.1]\n",
    "# lof_krange = [55, 60, 65, 70, 75] \n",
    "# knn_krange = [55, 60, 65, 70, 75] \n",
    "# if_range = [0.5, 0.6,0.7, 0.8,0.9]\n",
    "lof_krange = list(range(10,110,10)) * 6\n",
    "knn_krange = list(range(10,110,10)) * 6\n",
    "# lof_krange = range(70,90,4) \n",
    "# knn_krange = [60, 70, 80, 90, 100] \n",
    "if_range = [0.5, 0.6, 0.7, 0.8, 0.9] * 6\n",
    "# mahalanobis_N_range=[560]\n",
    "mahalanobis_N_range = [300, 400, 500, 600, 700, 800]\n",
    "# mahalanobis_N_range = [550, 560, 570, 580, 590, 600]\n",
    "N_size = 6\n",
    "if_N_range = np.sort(mahalanobis_N_range * 5)\n",
    "N_range = np.sort(mahalanobis_N_range * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pima dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 8) (768,)\n",
      "0.3489583333333333\n"
     ]
    }
   ],
   "source": [
    "filename = './Pima_withoutdupl_norm_35.arff'\n",
    "X, y = load_dataset(filename=filename)\n",
    "print(np.shape(X), np.shape(y))\n",
    "K = 100\n",
    "N = 268\n",
    "print(N/len(y))\n",
    "num_outliers = [N, N, N, N]\n",
    "class_balance = [1- N/768.0, N/768.0]\n",
    "lof_krange = list(range(10,210,10)) * 6\n",
    "knn_krange = list(range(10,210,10)) * 6\n",
    "if_range = [0.5, 0.6, 0.7, 0.8, 0.9] * 6\n",
    "mahalanobis_N_range=[N]\n",
    "mahalanobis_N_range = [220,230,240,250,260,270]\n",
    "\n",
    "if_N_range = np.sort(mahalanobis_N_range * 5)\n",
    "N_range = np.sort(mahalanobis_N_range * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALOI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49534, 27) (49534,)\n",
      "1508\n"
     ]
    }
   ],
   "source": [
    "filename = './ALOI_withoutdupl_norm.arff'\n",
    "X, y = load_dataset(filename=filename)\n",
    "print(np.shape(X), np.shape(y))\n",
    "print(sum(y))\n",
    "N = 1508\n",
    "num_outliers = [N, N, N, N]\n",
    "class_balance = [1- N/49534.0, N/49534.0]\n",
    "lof_krange = list(range(10,110,10)) * 6\n",
    "knn_krange = list(range(10,110,10)) * 6\n",
    "if_range = [0.5, 0.6, 0.7, 0.8, 0.9] * 6\n",
    "# mahalanobis_N_range=[N]\n",
    "mahalanobis_N_range=[1200, 1400, 1600, 1800, 2000, 2200]\n",
    "if_N_range = np.sort(mahalanobis_N_range * 5)\n",
    "N_range = np.sort(mahalanobis_N_range *10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load InternetAds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1966, 1555) (1966,)\n",
      "368\n"
     ]
    }
   ],
   "source": [
    "filename = './InternetAds_withoutdupl_norm_19.arff'\n",
    "X, y = load_dataset(filename=filename)\n",
    "print(np.shape(X), np.shape(y))\n",
    "print(sum(y))\n",
    "N = 368\n",
    "num_outliers = [N, N, N, N]\n",
    "class_balance = [1- N/1966.0, N/1966.0]\n",
    "lof_krange = list(range(5,55,5)) * 6\n",
    "knn_krange = list(range(5,55,5)) * 6\n",
    "if_range = [0.5, 0.6, 0.7, 0.8, 0.9] * 6\n",
    "# mahalanobis_N_range=[N]\n",
    "mahalanobis_N_range = [300, 350, 400, 450, 500, 550]\n",
    "if_N_range = np.sort(mahalanobis_N_range * 5)\n",
    "N_range = np.sort(mahalanobis_N_range *10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load KDDCup 99 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48113, 40) (48113,)\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "filename = './KDDCup99_withoutdupl_norm_catremoved.arff'\n",
    "X, y = load_dataset(filename=filename)\n",
    "print(np.shape(X), np.shape(y))\n",
    "print(sum(y))\n",
    "N = 200\n",
    "num_outliers = [N, N, N, N]\n",
    "class_balance = [1- N/48113.0, N/48113.0]\n",
    "lof_krange = list(range(10,110,10)) * 6\n",
    "knn_krange = list(range(10,110,10)) * 6\n",
    "if_range = [0.5, 0.6, 0.7, 0.8, 0.9] * 6\n",
    "mahalanobis_N_range=[500,1000,1500,2000,2500,3000]\n",
    "# mahalanobis_N_range = [20, 40, 60,80, 100,120]\n",
    "if_N_range = np.sort(mahalanobis_N_range * 5)\n",
    "N_range = np.sort(mahalanobis_N_range *10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(620098, 29)\n",
      "1052\n"
     ]
    }
   ],
   "source": [
    "filename='kdd99-unsupervised-ad.csv'\n",
    "import pandas as pd\n",
    "data = pd.read_csv(filename, header=None)\n",
    "X = data.drop(columns=[29])\n",
    "print(np.shape(np.array(X)))\n",
    "# Map dataframe to encode values and put values into a numpy array\n",
    "y = data[29].map(lambda x: 1 if x == 'o' else 0).values\n",
    "print(sum(y))\n",
    "\n",
    "lof_krange = list(range(10,110,10)) * 6\n",
    "knn_krange = list(range(10,110,10)) * 6\n",
    "if_range = [0.5, 0.6, 0.7, 0.8, 0.9] * 6\n",
    "mahalanobis_N_range=[1000,1500,2000,2500,3000,3500]\n",
    "if_N_range = np.sort(mahalanobis_N_range * 5)\n",
    "N_range = np.sort(mahalanobis_N_range *10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load shuttle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49097\n",
      "0.0715114976475141\n",
      "(49097, 9)\n"
     ]
    }
   ],
   "source": [
    "import hdf5storage\n",
    "mat = hdf5storage.loadmat('shuttle.mat')\n",
    "X = mat['X']\n",
    "y = mat['y']\n",
    "print(len(y))\n",
    "print(np.sum(y)/len(y))\n",
    "lof_krange = list(range(10,110,10)) * 6\n",
    "knn_krange = list(range(10,110,10)) * 6\n",
    "if_range = [0.5, 0.6, 0.7, 0.8, 0.9] * 6\n",
    "mahalanobis_N_range=[1000,1500,2000, 2500,3000, 3500]\n",
    "# mahalanobis_N_range = [20, 40, 60,80, 100,120]\n",
    "if_N_range = np.sort(mahalanobis_N_range * 5)\n",
    "N_range = np.sort(mahalanobis_N_range *10)\n",
    "print(np.shape(X))\n",
    "# normalize\n",
    "# from sklearn.preprocessing import Normalizer\n",
    "# transformer = Normalizer().fit(X) \n",
    "# X = transformer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load mulcross dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(262144, 4) (262144,)\n",
      "26214\n",
      "0.09999847412109375\n"
     ]
    }
   ],
   "source": [
    "filename = './mulcross.arff'\n",
    "\n",
    "with open(filename, 'r') as f:\n",
    "    data, meta = arff.loadarff(f)\n",
    "data = pd.DataFrame(data)\n",
    "X = data.drop(columns=['Target'])\n",
    "y = data[\"Target\"].map(lambda x: 1 if x == b'Anomaly' else 0).values\n",
    "# X, y = load_dataset(filename=filename)\n",
    "print(np.shape(X), np.shape(y))\n",
    "print(sum(y))\n",
    "print(sum(y)/len(y))\n",
    "lof_krange = list(range(10,110,10)) * 6\n",
    "knn_krange = list(range(10,110,10)) * 6\n",
    "if_range = [0.5, 0.6, 0.7, 0.8, 0.9] * 6\n",
    "mahalanobis_N_range=[20000, 22000, 24000, 26000, 28000, 30000]\n",
    "# mahalanobis_N_range = [20, 40, 60,80, 100,120]\n",
    "if_N_range = np.sort(mahalanobis_N_range * 5)\n",
    "N_range = np.sort(mahalanobis_N_range *10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load HTTP dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567498\n",
      "2211.0\n",
      "0.003896048972859816\n"
     ]
    }
   ],
   "source": [
    "import hdf5storage\n",
    "mat = hdf5storage.loadmat('http.mat')\n",
    "X = mat['X']\n",
    "y = mat['y']\n",
    "print(len(y))\n",
    "print(np.sum(y))\n",
    "print(np.sum(y)/len(y))\n",
    "lof_krange = list(range(10,110,10)) * 6\n",
    "knn_krange = list(range(10,110,10)) * 6\n",
    "if_range = [0.5, 0.6, 0.7, 0.8, 0.9] * 6\n",
    "# mahalanobis_N_range=[1500, 2000, 2500, 3000, 3500, 4000]\n",
    "mahalanobis_N_range=[5000, 10000, 15000,20000, 25000, 30000]\n",
    "# mahalanobis_N_range=[10000, 15000, 20000, 25000, 30000, 35000]\n",
    "# mahalanobis_N_range = [20, 40, 60,80, 100,120]\n",
    "if_N_range = np.sort(mahalanobis_N_range * 5)\n",
    "N_range = np.sort(mahalanobis_N_range *10)\n",
    "\n",
    "# # remove duplicates\n",
    "# newdata = pd.DataFrame(np.concatenate((X,y), axis = 1)).drop_duplicates()\n",
    "# X = newdata[[0,1,2]].values\n",
    "# y = np.array([1 if i==1.0 else 0 for i in newdata[[3]].values])\n",
    "# print('Remove duplicates: ', len(y))\n",
    "\n",
    "# # normalize\n",
    "# from sklearn.preprocessing import Normalizer\n",
    "# transformer = Normalizer().fit(X) \n",
    "# X = transformer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load ForestCover Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2747\n",
      "286048\n"
     ]
    }
   ],
   "source": [
    "import hdf5storage\n",
    "import pickle\n",
    "dataset = pickle.load(open(\"cover_dataset.pickle\", \"rb\"))\n",
    "X = dataset['X']\n",
    "y = dataset['y']\n",
    "print(np.sum(y))\n",
    "print(len(y))\n",
    "lof_krange = list(range(10,110,10)) * 6\n",
    "knn_krange = list(range(10,110,10)) * 6\n",
    "if_range = [0.5, 0.6, 0.7, 0.8, 0.9] * 6\n",
    "mahalanobis_N_range=[5000, 10000, 15000, 20000, 25000, 30000]\n",
    "# mahalanobis_N_range = [20, 40, 60,80, 100,120]\n",
    "if_N_range = np.sort(mahalanobis_N_range * 5)\n",
    "N_range = np.sort(mahalanobis_N_range *10)\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "transformer = RobustScaler().fit(X)\n",
    "X_transformed = transformer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Annthyroid dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7129, 21) (7129,)\n",
      "0.07490531631364848\n"
     ]
    }
   ],
   "source": [
    "filename = './Annthyroid_withoutdupl_norm_07.arff'\n",
    "X, y = load_dataset(filename=filename)\n",
    "print(np.shape(X), np.shape(y))\n",
    "print(sum(y)/len(y))\n",
    "N = 534\n",
    "num_outliers = [N, N, N, N]\n",
    "class_balance = [1- N/7129.0, N/7129.0]\n",
    "lof_krange = list(range(10,110,10)) * 6\n",
    "knn_krange = list(range(10,110,10)) * 6\n",
    "if_range = [0.5, 0.6, 0.7, 0.8, 0.9] * 6\n",
    "# mahalanobis_N_range=[N]\n",
    "mahalanobis_N_range=[300, 400,500,600,700,800]\n",
    "if_N_range = np.sort(mahalanobis_N_range * 5)\n",
    "N_range = np.sort(mahalanobis_N_range *10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95156\n",
      "30.0\n"
     ]
    }
   ],
   "source": [
    "import hdf5storage\n",
    "mat = hdf5storage.loadmat('smtp.mat')\n",
    "X = mat['X']\n",
    "y = mat['y']\n",
    "print(len(y))\n",
    "print(np.sum(y))\n",
    "\n",
    "lof_krange = list(range(10,110,10)) * 6\n",
    "knn_krange = list(range(10,110,10)) * 6\n",
    "if_range = [0.5, 0.6, 0.7, 0.8, 0.9] * 6\n",
    "# mahalanobis_N_range=[20, 40, 60, 80, 100, 120]\n",
    "mahalanobis_N_range = [30, 40, 50, 60, 70,80]\n",
    "# mahalanobis_N_range = [200, 400, 600, 800, 1000, 1200]\n",
    "if_N_range = np.sort(mahalanobis_N_range * 5)\n",
    "N_range = np.sort(mahalanobis_N_range *10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1831\n",
      "0.0961223375204806\n",
      "(1831, 21)\n"
     ]
    }
   ],
   "source": [
    "import hdf5storage\n",
    "mat = hdf5storage.loadmat('cardio.mat')\n",
    "X = mat['X']\n",
    "y = mat['y']\n",
    "print(len(y))\n",
    "print(np.sum(y)/len(y))\n",
    "lof_krange = list(range(10,110,10)) * 6\n",
    "knn_krange = list(range(10,110,10)) * 6\n",
    "if_range = [0.5, 0.6, 0.7, 0.8, 0.9] * 6\n",
    "mahalanobis_N_range=[150,200,250,300,350,400]\n",
    "# mahalanobis_N_range = [20, 40, 60,80, 100,120]\n",
    "if_N_range = np.sort(mahalanobis_N_range * 5)\n",
    "N_range = np.sort(mahalanobis_N_range *10)\n",
    "print(np.shape(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Musk dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3062\n",
      "97.0\n",
      "0.03167864141084259\n",
      "(3062, 166)\n"
     ]
    }
   ],
   "source": [
    "import hdf5storage\n",
    "mat = hdf5storage.loadmat('musk.mat')\n",
    "X = mat['X']\n",
    "y = mat['y']\n",
    "print(len(y))\n",
    "print(np.sum(y))\n",
    "print(np.sum(y)/len(y))\n",
    "lof_krange = list(range(10,110,10)) * 6\n",
    "knn_krange = list(range(10,110,10)) * 6\n",
    "if_range = [0.5, 0.6, 0.7, 0.8, 0.9] * 6\n",
    "mahalanobis_N_range=[100, 120, 140, 160, 180, 200]\n",
    "# mahalanobis_N_range = [20, 40, 60,80, 100,120]\n",
    "if_N_range = np.sort(mahalanobis_N_range * 5)\n",
    "N_range = np.sort(mahalanobis_N_range *10)\n",
    "print(np.shape(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Satimage-2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5803\n",
      "71.0\n",
      "0.0122350508357746\n",
      "(5803, 36)\n"
     ]
    }
   ],
   "source": [
    "import hdf5storage\n",
    "mat = hdf5storage.loadmat('satimage-2.mat')\n",
    "X = mat['X']\n",
    "y = mat['y']\n",
    "print(len(y))\n",
    "print(np.sum(y))\n",
    "print(np.sum(y)/len(y))\n",
    "lof_krange = list(range(10,110,10)) * 6\n",
    "knn_krange = list(range(10,110,10)) * 6\n",
    "if_range = [0.5, 0.6, 0.7, 0.8, 0.9] * 6\n",
    "mahalanobis_N_range=[60, 80, 100, 120, 140, 160]\n",
    "# mahalanobis_N_range = [20, 40, 60,80, 100,120]\n",
    "if_N_range = np.sort(mahalanobis_N_range * 5)\n",
    "N_range = np.sort(mahalanobis_N_range *10)\n",
    "print(np.shape(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pendigits Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdf5storage\n",
    "mat = hdf5storage.loadmat('pendigits.mat')\n",
    "X = mat['X']\n",
    "y = mat['y']\n",
    "print(len(y))\n",
    "print(np.sum(y))\n",
    "print(np.sum(y)/len(y))\n",
    "lof_krange = list(range(10,110,10)) * 6\n",
    "knn_krange = list(range(10,110,10)) * 6\n",
    "if_range = [0.5, 0.6, 0.7, 0.8, 0.9] * 6\n",
    "mahalanobis_N_range=[150,200,250,300,350,400]\n",
    "# mahalanobis_N_range = [20, 40, 60,80, 100,120]\n",
    "if_N_range = np.sort(mahalanobis_N_range * 5)\n",
    "N_range = np.sort(mahalanobis_N_range *10)\n",
    "print(np.shape(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5216\n",
      "150.0\n",
      "0.028757668711656442\n",
      "(5216, 64)\n"
     ]
    }
   ],
   "source": [
    "import hdf5storage\n",
    "mat = hdf5storage.loadmat('optdigits.mat')\n",
    "X = mat['X']\n",
    "y = mat['y']\n",
    "print(len(y))\n",
    "print(np.sum(y))\n",
    "print(np.sum(y)/len(y))\n",
    "lof_krange = list(range(10,110,10)) * 6\n",
    "knn_krange = list(range(10,110,10)) * 6\n",
    "if_range = [0.5, 0.6, 0.7, 0.8, 0.9] * 6\n",
    "mahalanobis_N_range=[150,200,250,300,350,400]\n",
    "# mahalanobis_N_range = [20, 40, 60,80, 100,120]\n",
    "if_N_range = np.sort(mahalanobis_N_range * 5)\n",
    "N_range = np.sort(mahalanobis_N_range *10)\n",
    "print(np.shape(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "70\n",
      "40\n",
      "10\n",
      "80\n",
      "50\n",
      "20\n",
      "90\n",
      "60\n",
      "30\n",
      "0.5\n",
      "0.6\n",
      "0.8\n",
      "0.9\n",
      "0.7\n",
      "Best Mahala F-1 = 0\n"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "all_scores = []\n",
    "f1s = []\n",
    "\n",
    "temp_lof_results = dict()\n",
    "unique_lof_ks = list(set(lof_krange)) \n",
    "for k in unique_lof_ks:\n",
    "#     print(k)\n",
    "    lof_scores = run_lof(X, y, k=k)\n",
    "    temp_lof_results[k] = lof_scores\n",
    "for i in range(len(lof_krange)):\n",
    "    lof_predictions, lof_scores, f1 = get_predictions(temp_lof_results[lof_krange[i]], num_outliers=N_range[i], method_name='LOF')\n",
    "    all_results.append(lof_predictions)\n",
    "    all_scores.append(lof_scores)\n",
    "    f1s.append(f1)\n",
    "# best_lof_f1 = 0\n",
    "# for i in np.sort(unique_lof_ks):\n",
    "#     temp_f1 = max(np.array(f1s[0:60])[np.where(np.array(lof_krange) == i)[0]])\n",
    "#     print('LOF k = {}, best F-1 = {}'.format(i, temp_f1))\n",
    "#     best_lof_f1 = max(best_lof_f1, temp_f1)\n",
    "# print('Best LOF F-1 = {}'.format(best_lof_f1))\n",
    "\n",
    "temp_knn_results = dict()\n",
    "unique_knn_ks = list(set(knn_krange)) \n",
    "for k in unique_knn_ks:\n",
    "    print(k)\n",
    "    knn_scores = run_knn(X, y, k=k)\n",
    "    temp_knn_results[k] = knn_scores\n",
    "for i in range(len(knn_krange)):\n",
    "    knn_predictions, knn_scores,f1 = get_predictions(temp_knn_results[knn_krange[i]], num_outliers=N_range[i], method_name='KNN')\n",
    "    all_results.append(knn_predictions)\n",
    "    all_scores.append(knn_scores)\n",
    "    f1s.append(f1)\n",
    "# best_knn_f1 = 0\n",
    "# for i in np.sort(unique_knn_ks):\n",
    "#     temp_f1 = max(np.array(f1s[60:120])[np.where(np.array(knn_krange) == i)[0]])\n",
    "#     print('KNN k = {}, best F-1 = {}'.format(i, temp_f1))\n",
    "#     best_knn_f1 = max(best_knn_f1, temp_f1)\n",
    "# print('Best KNN F-1 = {}'.format(best_knn_f1))\n",
    "    \n",
    "temp_if_results = dict()\n",
    "unique_if_features = list(set(if_range)) \n",
    "for k in unique_if_features:\n",
    "    print(k)\n",
    "    if_scores = run_isolation_forest(X, y, max_features=k)\n",
    "    temp_if_results[k] = if_scores\n",
    "for i in range(len(if_range)):\n",
    "    if_predictions, if_scores,f1 = get_predictions(temp_if_results[if_range[i]], num_outliers=N_range[i], method_name='IF')\n",
    "    all_results.append(if_predictions)\n",
    "    all_scores.append(if_scores)\n",
    "    f1s.append(f1)\n",
    "# best_if_f1 = 0\n",
    "# for i in np.sort(unique_if_features):\n",
    "#     temp_f1 = max(np.array(f1s[120:150])[np.where(np.array(if_range) == i)[0]])\n",
    "#     print('IF = {}, best F-1 = {}'.format(i, temp_f1))\n",
    "#     best_if_f1 = max(best_if_f1, temp_f1)\n",
    "# print('Best IF F-1 = {}'.format(best_if_f1))\n",
    "    \n",
    "mahalanobis_scores = run_mahalanobis(X, y)\n",
    "best_mahala_f1 = 0\n",
    "for i in range(len(mahalanobis_N_range)):\n",
    "    mahalanobis_predictions,mahalanobis_scores,f1 = get_predictions(mahalanobis_scores, num_outliers=mahalanobis_N_range[i], method_name='mahala')\n",
    "    all_results.append(mahalanobis_predictions)\n",
    "    all_scores.append(mahalanobis_scores)\n",
    "    best_mahala_f1 = max(best_mahala_f1, f1)\n",
    "    f1s.append(f1)\n",
    "# print('mahalanobis = {}'.format(max(np.array(f1s[150:]))))\n",
    "print('Best Mahala F-1 = {}'.format(best_mahala_f1))\n",
    "L = np.stack(all_results).T\n",
    "scores = np.stack(all_scores).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95156, 156)\n",
      "(95156, 156)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(L))\n",
    "print(np.shape(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Majority Vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 for MV: 0.3913043478260869\n"
     ]
    }
   ],
   "source": [
    "mid = np.shape(L)[1]/2\n",
    "predictions = np.full((len(y)), 0)\n",
    "predictions[np.sum(L, axis = 1) > mid] = 1\n",
    "print('F1 for MV:', metrics.f1_score(y, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snorkel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import LabelModel\n",
    "label_model = LabelModel(cardinality=2, verbose=True)\n",
    "label_model.fit(L, seed=123, lr=0.001, n_epochs=5000, class_balance=[0.9,0.1])\n",
    "probs_train = label_model.predict_proba(L=L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 for snorkel:  0.11267605633802817\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.1\n",
    "predictions = np.full((len(y)), 0)\n",
    "predictions[probs_train[:,1] > threshold] = 1\n",
    "print('F1 for snorkel: ', metrics.f1_score(y, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "snorkel_predictions = predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and load pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "dataset_results = {'L': L, 'scores': scores, 'f1s': f1s}\n",
    "pickle.dump(dataset_results, open(\"aloi.pickle\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "dataset_results = pickle.load(open(\"cover.pickle\", \"rb\"))\n",
    "L = dataset_results['L']\n",
    "scores = dataset_results['scores']\n",
    "f1s = dataset_results['f1s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_prev = L\n",
    "scores_prev = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = L_prev\n",
    "scores = scores_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n",
      "(95156, 156)\n"
     ]
    }
   ],
   "source": [
    "print(max(f1s)) \n",
    "print(np.shape(L))\n",
    "y = y.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_result_list = []\n",
    "classifier_result_list = []\n",
    "prediction_list = []\n",
    "cur_f1_scores = []\n",
    "prediction_high_conf_outliers = np.array([])\n",
    "prediction_high_conf_inliers = np.array([])\n",
    "prediction_classifier_disagree = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_range = np.array([[0, 60], [60, 120], [120, 150], [150, 156]])\n",
    "coef_index_range = np.array([[0, 10], [10, 20], [20, 25], [25, 26]])\n",
    "coef_remain_index = range(156)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(60):\n",
    "#     scores[scores[:,i] > 100,i] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index_range = np.array([[0, 120], [120, 240], [240, 270], [270, 276]])\n",
    "# coef_index_range = np.array([[0, 20], [20, 40], [40, 45], [45, 46]])\n",
    "# coef_remain_index = range(276)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_scores = []\n",
    "# for i in range(np.shape(scores)[1]):\n",
    "#     new_scores.append(np.argsort(np.argsort(scores[:,i]))/len(scores[:,i]))\n",
    "# scores = np.stack(new_scores).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 120, 121, 122, 123, 124, 150]\n",
      "(95156, 26)\n"
     ]
    }
   ],
   "source": [
    "scores_for_training_indexes = []\n",
    "for i in range(len(index_range)):\n",
    "    start=index_range[i][0]\n",
    "    temp_range = coef_index_range[i][1]-coef_index_range[i][0]\n",
    "    scores_for_training_indexes  = scores_for_training_indexes + list(range(start, start+temp_range))\n",
    "print(scores_for_training_indexes) \n",
    "scores_for_training = scores[:, np.array(scores_for_training_indexes)]\n",
    "print(np.shape(scores_for_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_coef_proba(coef):\n",
    "    # weights --> ranking\n",
    "    pos_ranking = np.argsort(np.argsort(coef)) + 1\n",
    "#     print(pos_ranking)\n",
    "\n",
    "    # ranking --> u_k\n",
    "    num_coefs = len(coef)\n",
    "    normalize_factor = np.sum([max(0, np.log(num_coefs + 1) - np.log(j)) for j in range(1, num_coefs + 1)])\n",
    "    u_k = [max(0, np.log(num_coefs + 1) - np.log(j))/normalize_factor - 1/num_coefs for j in pos_ranking]\n",
    "    \n",
    "    # normalize u_k\n",
    "    normalized_u_k = (u_k-min(u_k))/(max(u_k)-min(u_k))\n",
    "#     print(normalized_u_k)\n",
    "\n",
    "    # u_k to probabilities\n",
    "    import scipy.stats\n",
    "    pdfs = np.array([scipy.stats.norm(0, np.std(normalized_u_k)).pdf(j) for j in normalized_u_k])\n",
    "#     preservation_rate = 0.8\n",
    "#     probabilities = pdfs * preservation_rate * num_coefs/np.sum(pdfs)\n",
    "    probabilities = (pdfs-min(pdfs))/(max(pdfs)-min(pdfs))\n",
    "#     probabilities = pdfs\n",
    "    print(probabilities)\n",
    "    return probabilities\n",
    "\n",
    "def filter_update_list(R, value_list):\n",
    "    P = 2\n",
    "    prediction = 0\n",
    "    for new_value in value_list:\n",
    "        K = P / (P + R)\n",
    "        prediction = prediction + K * (new_value - prediction)\n",
    "        P = (1 - K) * P\n",
    "    return prediction\n",
    "\n",
    "def get_kf_results(proba_list):\n",
    "    results = []\n",
    "    for i in range(np.shape(proba_list)[1]):\n",
    "        results.append(filter_update_list(0.1, proba_list[:, i]))\n",
    "    return np.array(results)\n",
    "\n",
    "def generate_decision_on_proba(probabilities):\n",
    "    return np.array([np.random.binomial(n=1, p = min(1, proba)) for proba in probabilities])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative train LR and classifier(SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n",
      "Iteration = 0\n",
      "F1 for training data: 0.3913043478260869\n",
      "accuracy for training data: 0.9997057463533566\n",
      "[[95119     7]\n",
      " [   21     9]]\n",
      "F-1 score from SVM: 0.3913043478260869\n",
      "Number of outliers by SVM: 16\n",
      "F-1 score from SVM: 0.36666666666666664\n",
      "Number of outliers by SVM: 30\n",
      "AUROC from SVM: 0.7507800180812816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Result: Loss 0.00132846\tPrec @ 1 99.985%\n",
      "Number of inliers to change:  123\n",
      "Number of outliers to change:  1\n",
      "Final Training Result: Loss 0.00075954\tPrec @ 1 99.989%\n",
      "F-1 score from NN: 0.0\n",
      "Number of outliers by NN: 6\n",
      "Number of points to remove:  124\n",
      "Number of new points with confidence > 0.99 0\n",
      "[]\n",
      "[]\n",
      "Number of points with confidence < 0.01 0\n",
      "[]\n",
      "[]\n",
      "##################################################################\n",
      "Iteration = 1\n",
      "F1 for training data: 0.5294117647058824\n",
      "accuracy for training data: 0.9998316356595673\n",
      "[[95007     6]\n",
      " [   10     9]]\n",
      "F-1 score from SVM: 0.588235294117647\n",
      "Number of outliers by SVM: 21\n",
      "F-1 score from SVM: 0.6666666666666666\n",
      "Number of outliers by SVM: 30\n",
      "AUROC from SVM: 0.8223202208999993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Result: Loss 0.00020365\tPrec @ 1 99.992%\n",
      "Number of inliers to change:  2904\n",
      "Number of outliers to change:  4\n",
      "Final Training Result: Loss 0.00004440\tPrec @ 1 100.000%\n",
      "F-1 score from NN: 0.6557377049180327\n",
      "Number of outliers by NN: 31\n",
      "Number of points to remove:  2908\n",
      "Number of new points with confidence > 0.99 19\n",
      "[0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 0 0]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0.]\n",
      "F-1 before:  0.4210526315789474\n",
      "F-1 after:  0.8823529411764706\n",
      "Number of points with confidence < 0.01 1\n",
      "[1]\n",
      "[0.]\n",
      "##################################################################\n",
      "Iteration = 2\n",
      "F1 for training data: 0.6666666666666666\n",
      "accuracy for training data: 0.9997829484285466\n",
      "[[92104    10]\n",
      " [   10    20]]\n",
      "F-1 score from SVM: 0.6557377049180327\n",
      "Number of outliers by SVM: 31\n",
      "F-1 score from SVM: 0.6440677966101694\n",
      "Number of outliers by SVM: 29\n",
      "AUROC from SVM: 0.8295448492876116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Result: Loss 0.00007579\tPrec @ 1 100.000%\n",
      "Number of inliers to change:  3664\n",
      "Number of outliers to change:  7\n",
      "Final Training Result: Loss 0.00003287\tPrec @ 1 100.000%\n",
      "F-1 score from NN: 0.625\n",
      "Number of outliers by NN: 34\n",
      "Number of points to remove:  3671\n",
      "Number of new points with confidence > 0.99 10\n",
      "[1 1 1 0 1 1 0 1 1 0]\n",
      "[1. 1. 1. 0. 1. 1. 0. 1. 1. 0.]\n",
      "F-1 before:  1.0\n",
      "F-1 after:  0.8235294117647058\n",
      "Number of points with confidence < 0.01 26\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "##################################################################\n",
      "Iteration = 3\n",
      "F1 for training data: 0.6349206349206349\n",
      "accuracy for training data: 0.9997401394208498\n",
      "[[88466    13]\n",
      " [   10    20]]\n",
      "F-1 score from SVM: 0.625\n",
      "Number of outliers by SVM: 34\n",
      "F-1 score from SVM: 0.6\n",
      "Number of outliers by SVM: 30\n",
      "AUROC from SVM: 0.8404601966514588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Result: Loss 0.00007320\tPrec @ 1 100.000%\n",
      "Number of inliers to change:  12413\n",
      "Number of outliers to change:  3\n",
      "Final Training Result: Loss 0.00003500\tPrec @ 1 100.000%\n",
      "F-1 score from NN: 0.5263157894736841\n",
      "Number of outliers by NN: 46\n",
      "Number of points to remove:  12416\n",
      "Number of new points with confidence > 0.99 4\n",
      "[1 1 1 0]\n",
      "[1. 0. 0. 0.]\n",
      "F-1 before:  0.5\n",
      "F-1 after:  0.4\n",
      "Number of points with confidence < 0.01 391\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0.]\n",
      "##################################################################\n",
      "Iteration = 4\n",
      "F1 for training data: 0.625\n",
      "accuracy for training data: 0.9996862252902416\n",
      "[[76444    14]\n",
      " [   10    20]]\n",
      "F-1 score from SVM: 0.625\n",
      "Number of outliers by SVM: 34\n",
      "F-1 score from SVM: 0.5666666666666667\n",
      "Number of outliers by SVM: 30\n",
      "AUROC from SVM: 0.8159008402890201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Result: Loss 0.00009203\tPrec @ 1 100.000%\n",
      "Number of inliers to change:  8303\n",
      "Number of outliers to change:  4\n",
      "Final Training Result: Loss 0.00003866\tPrec @ 1 100.000%\n",
      "F-1 score from NN: 0.547945205479452\n",
      "Number of outliers by NN: 43\n",
      "Number of points to remove:  8307\n",
      "Number of new points with confidence > 0.99 4\n",
      "[1 1 1 1]\n",
      "[1. 0. 0. 0.]\n",
      "F-1 before:  0.4\n",
      "F-1 after:  0.4\n",
      "Number of points with confidence < 0.01 2\n",
      "[0 0]\n",
      "[0. 0.]\n",
      "##################################################################\n",
      "Iteration = 5\n",
      "F1 for training data: 0.625\n",
      "accuracy for training data: 0.999648026749967\n",
      "[[68143    14]\n",
      " [   10    20]]\n",
      "F-1 score from SVM: 0.625\n",
      "Number of outliers by SVM: 34\n",
      "F-1 score from SVM: 0.5666666666666667\n",
      "Number of outliers by SVM: 30\n",
      "AUROC from SVM: 0.8132480429465481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Result: Loss 0.00009700\tPrec @ 1 100.000%\n",
      "Number of inliers to change:  12052\n",
      "Number of outliers to change:  4\n",
      "Final Training Result: Loss 0.00004080\tPrec @ 1 100.000%\n",
      "F-1 score from NN: 0.25806451612903225\n",
      "Number of outliers by NN: 125\n",
      "Number of points to remove:  12056\n",
      "Number of new points with confidence > 0.99 5\n",
      "[1 1 1 0 1]\n",
      "[1. 0. 0. 0. 0.]\n",
      "F-1 before:  0.4\n",
      "F-1 after:  0.33333333333333337\n",
      "Number of points with confidence < 0.01 5\n",
      "[0 0 0 0 0]\n",
      "[0. 0. 0. 0. 0.]\n",
      "##################################################################\n",
      "Iteration = 6\n",
      "F1 for training data: 0.6153846153846153\n",
      "accuracy for training data: 0.9995546926488662\n",
      "[[56096    15]\n",
      " [   10    20]]\n",
      "F-1 score from SVM: 0.5970149253731343\n",
      "Number of outliers by SVM: 37\n",
      "F-1 score from SVM: 0.5666666666666667\n",
      "Number of outliers by SVM: 30\n",
      "AUROC from SVM: 0.8046715233830218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Result: Loss 0.00011564\tPrec @ 1 100.000%\n",
      "Number of inliers to change:  8220\n",
      "Number of outliers to change:  4\n",
      "Final Training Result: Loss 0.00003776\tPrec @ 1 100.000%\n",
      "F-1 score from NN: 0.23668639053254437\n",
      "Number of outliers by NN: 139\n",
      "Number of points to remove:  8224\n",
      "Number of new points with confidence > 0.99 6\n",
      "[1 1 0 0 1 1]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "F-1 before:  0.0\n",
      "F-1 after:  0.0\n",
      "Number of points with confidence < 0.01 0\n",
      "[]\n",
      "[]\n",
      "##################################################################\n",
      "Iteration = 7\n",
      "F1 for training data: 0.5970149253731343\n",
      "accuracy for training data: 0.9994365962064145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[47876    17]\n",
      " [   10    20]]\n",
      "F-1 score from SVM: 0.5970149253731343\n",
      "Number of outliers by SVM: 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-1 score from SVM: 0.0\n",
      "Number of outliers by SVM: 0\n",
      "AUROC from SVM: 0.7861203386385776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Result: Loss 0.00014222\tPrec @ 1 100.000%\n",
      "Number of inliers to change:  7251\n",
      "Number of outliers to change:  3\n",
      "Final Training Result: Loss 0.00005331\tPrec @ 1 100.000%\n",
      "F-1 score from NN: 0.2380952380952381\n",
      "Number of outliers by NN: 138\n",
      "Number of points to remove:  7254\n",
      "Number of new points with confidence > 0.99 3\n",
      "[1 1 1]\n",
      "[0. 0. 0.]\n",
      "F-1 before:  0.0\n",
      "F-1 after:  0.0\n",
      "Number of points with confidence < 0.01 0\n",
      "[]\n",
      "[]\n",
      "##################################################################\n",
      "Iteration = 8\n",
      "F1 for training data: 0.5970149253731343\n",
      "accuracy for training data: 0.99933615263572\n",
      "[[40625    17]\n",
      " [   10    20]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-1 score from SVM: 0.5970149253731343\n",
      "Number of outliers by SVM: 37\n",
      "F-1 score from SVM: 0.5666666666666667\n",
      "Number of outliers by SVM: 30\n",
      "AUROC from SVM: 0.762537932146136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Result: Loss 0.00015739\tPrec @ 1 100.000%\n",
      "Number of inliers to change:  7551\n",
      "Number of outliers to change:  3\n",
      "Final Training Result: Loss 0.00005030\tPrec @ 1 100.000%\n",
      "F-1 score from NN: 0.0761904761904762\n",
      "Number of outliers by NN: 495\n",
      "Number of points to remove:  7554\n",
      "Number of new points with confidence > 0.99 9\n",
      "[0 0 0 0 0 0 1 1 1]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "F-1 before:  0.0\n",
      "F-1 after:  0.0\n",
      "Number of points with confidence < 0.01 0\n",
      "[]\n",
      "[]\n",
      "##################################################################\n",
      "Iteration = 9\n",
      "F1 for training data: 0.547945205479452\n",
      "accuracy for training data: 0.9990038337307936\n",
      "[[33074    23]\n",
      " [   10    20]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-1 score from SVM: 0.2484472049689441\n",
      "Number of outliers by SVM: 131\n",
      "F-1 score from SVM: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers by SVM: 0\n",
      "AUROC from SVM: 0.7423667907126688\n",
      "Final Training Result: Loss 0.00025426\tPrec @ 1 100.000%\n",
      "Number of inliers to change:  1321\n",
      "Number of outliers to change:  6\n",
      "Final Training Result: Loss 0.00006715\tPrec @ 1 100.000%\n",
      "F-1 score from NN: 0.01360081604896294\n",
      "Number of outliers by NN: 2911\n",
      "Number of points to remove:  1327\n",
      "Number of new points with confidence > 0.99 101\n",
      "[0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0.]\n",
      "F-1 before:  0.0\n",
      "F-1 after:  0.0\n",
      "Number of points with confidence < 0.01 6\n",
      "[0 0 0 0 0 0]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "##################################################################\n",
      "Iteration = 10\n",
      "F1 for training data: 0.2380952380952381\n",
      "accuracy for training data: 0.9959883411163695\n",
      "[[31759   118]\n",
      " [   10    20]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-1 score from SVM: 0.2380952380952381\n",
      "Number of outliers by SVM: 138\n",
      "F-1 score from SVM: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers by SVM: 0\n",
      "AUROC from SVM: 0.7336746350454484\n",
      "Final Training Result: Loss 0.00019589\tPrec @ 1 100.000%\n",
      "Number of inliers to change:  4531\n",
      "Number of outliers to change:  17\n",
      "Final Training Result: Loss 0.00005647\tPrec @ 1 100.000%\n",
      "F-1 score from NN: 0.012202562538133009\n",
      "Number of outliers by NN: 3248\n",
      "Number of points to remove:  4548\n",
      "Number of new points with confidence > 0.99 3018\n",
      "[0 0 0 ... 0 0 0]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "F-1 before:  0.0\n",
      "F-1 after:  0.0\n",
      "Number of points with confidence < 0.01 6\n",
      "[0 0 0 0 0 0]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "##################################################################\n",
      "Iteration = 11\n",
      "F1 for training data: 0.012622278321236984\n",
      "accuracy for training data: 0.8970147780008557\n",
      "[[27234  3119]\n",
      " [   10    20]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-1 score from SVM: 0.012330456226880395\n",
      "Number of outliers by SVM: 3214\n",
      "F-1 score from SVM: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers by SVM: 0\n",
      "AUROC from SVM: 0.7118257188711113\n",
      "Final Training Result: Loss 0.00020104\tPrec @ 1 100.000%\n",
      "Number of inliers to change:  4628\n",
      "Number of outliers to change:  178\n",
      "Final Training Result: Loss 0.00005899\tPrec @ 1 100.000%\n",
      "F-1 score from NN: 0.011383039271485488\n",
      "Number of outliers by NN: 3484\n",
      "Number of points to remove:  4806\n",
      "Number of new points with confidence > 0.99 391\n",
      "[1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 1 1 0 0 1 1\n",
      " 1 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 0\n",
      " 0 1 0 0 0 0 0 1 1 1 1 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1\n",
      " 0 0 0 0 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 0 0 1 1 1 0 1 0 0 1 1 1 1\n",
      " 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0.]\n",
      "F-1 before:  0.0\n",
      "F-1 after:  0.0\n",
      "Number of points with confidence < 0.01 0\n",
      "[]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-1 score from SVM: 0.012330456226880395\n",
      "Number of outliers by SVM: 3214\n",
      "F-1 score from SVM: 0.0\n",
      "Number of outliers by SVM: 0\n",
      "AUROC from SVM: 0.7118257188711113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Working solution: Prune points using NN, add good prediction results back\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "last_training_data_indexes = []\n",
    "counter = 0\n",
    "\n",
    "ratio_to_remove = 0.05\n",
    "dim = np.shape(X)[1]\n",
    "remain_points = np.array(range(len(y)))\n",
    "\n",
    "mid = np.shape(L)[1]/2\n",
    "label_of_point = np.full((len(y)), 0)\n",
    "label_of_point[np.sum(L, axis = 1) > mid] = 1\n",
    "# label_of_point = snorkel_predictions.copy()\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "transformer = RobustScaler().fit(X)\n",
    "X_transformed = transformer.transform(X)\n",
    "\n",
    "prev_loss = 10000\n",
    "early_loss_list = []\n",
    "converge_loss_list = []\n",
    "f1_score_list = []\n",
    "\n",
    "for i_range in range(0, 50):\n",
    "    print(\"##################################################################\")\n",
    "    print('Iteration = {}'.format(i_range))\n",
    "    print('F1 for training data:', metrics.f1_score(y[remain_points], label_of_point[remain_points]))\n",
    "    print('accuracy for training data:', metrics.accuracy_score(y[remain_points], label_of_point[remain_points]))\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    print(confusion_matrix(y[remain_points], label_of_point[remain_points]))\n",
    "\n",
    "    if((i_range  + 1) % 1 == 0):\n",
    "        from sklearn.svm import SVC\n",
    "        clf_X = SVC(gamma='auto', probability=True, random_state=0)\n",
    "        clf_X.fit(X_transformed[remain_points], label_of_point[remain_points])\n",
    "        clf_predict_proba_X = clf_X.predict_proba(X_transformed)[:,1]\n",
    "\n",
    "        SVM_threshold = 0.5\n",
    "        print(\"F-1 score from SVM:\",metrics.f1_score(y, np.array([int(i) for i in clf_predict_proba_X > SVM_threshold])))\n",
    "        print('Number of outliers by SVM:', sum(np.array([int(i) for i in clf_predict_proba_X > SVM_threshold])))\n",
    "        SVM_threshold = np.sort(clf_predict_proba_X)[::-1][int(np.sum(y))]\n",
    "        print(\"F-1 score from SVM:\",metrics.f1_score(y, np.array([int(i) for i in clf_predict_proba_X > SVM_threshold])))\n",
    "        f1_score_list.append(metrics.f1_score(y, np.array([int(i) for i in clf_predict_proba_X > SVM_threshold])))\n",
    "        print('Number of outliers by SVM:', sum(np.array([int(i) for i in clf_predict_proba_X > SVM_threshold])))\n",
    "        print('AUROC from SVM:', roc_auc_score(y, clf_predict_proba_X))\n",
    "\n",
    "    counter = 0\n",
    "#     while(counter < 5):\n",
    "    temp_remain_points = remain_points.copy()\n",
    "    # start pruning points\n",
    "    counter = counter + 1\n",
    "    loss_list, model, avg_train_loss = run_NN(X_transformed[temp_remain_points],label_of_point[temp_remain_points], 3, dim = dim)\n",
    "#     loss_threshold = np.sort(loss_list)[::-1][int(ratio_to_remove * len(loss_list))]\n",
    "#     print(min(loss_list), max(loss_list), loss_threshold, np.mean(loss_list)+ np.std(loss_list))\n",
    "#     loss_threshold = np.mean(loss_list)+ np.std(loss_list)\n",
    "#     print('Number of points to change: ', sum((loss_list > loss_threshold)))\n",
    "#     points_to_remove = temp_remain_points[(loss_list > loss_threshold)]\n",
    "\n",
    "    inlier_labels = np.where(label_of_point[temp_remain_points] == 0)[0]\n",
    "    loss_threshold = np.sort(loss_list[inlier_labels])[::-1][int(ratio_to_remove * len(loss_list[inlier_labels]))]\n",
    "    loss_threshold = np.mean(loss_list[inlier_labels])+ np.std(loss_list[inlier_labels])\n",
    "    print('Number of inliers to change: ', sum(loss_list[inlier_labels] > loss_threshold))\n",
    "    points_to_remove = temp_remain_points[inlier_labels][(loss_list[inlier_labels] > loss_threshold)]\n",
    "\n",
    "    outlier_labels = np.where(label_of_point[temp_remain_points] == 1)[0]\n",
    "    loss_threshold = np.sort(loss_list[outlier_labels])[::-1][int(ratio_to_remove * len(loss_list[outlier_labels]))]\n",
    "    loss_threshold = np.mean(loss_list[outlier_labels])+ np.std(loss_list[outlier_labels])\n",
    "    print('Number of outliers to change: ', sum(loss_list[outlier_labels] > loss_threshold))\n",
    "    points_to_remove = np.append(points_to_remove, temp_remain_points[outlier_labels][(loss_list[outlier_labels] > loss_threshold)])\n",
    "\n",
    "    loss_list, model, avg_train_loss = run_NN(X_transformed[temp_remain_points],label_of_point[temp_remain_points],10, dim = dim)\n",
    "    converge_loss_list.append(avg_train_loss)\n",
    "    predict_proba = inference_NN(model, X_transformed, y)[:,1]\n",
    "    print(\"F-1 score from NN:\",metrics.f1_score(y, np.array([int(i) for i in predict_proba > 0.5])))\n",
    "    print('Number of outliers by NN:', sum(np.array([int(i) for i in predict_proba > 0.5])))\n",
    "\n",
    "    print('Number of points to remove: ', len(points_to_remove))\n",
    "    temp_remain_points = np.setdiff1d(np.array(temp_remain_points), points_to_remove)\n",
    "\n",
    "    predict_outlier_indexes = np.where(predict_proba > 0.9)[0]\n",
    "    new_outlier_indexes = np.setdiff1d(predict_outlier_indexes, temp_remain_points)\n",
    "#     new_outlier_indexes = new_outlier_indexes[label_of_point[new_outlier_indexes]==0]\n",
    "    print('Number of new points with confidence > 0.99', len(new_outlier_indexes))\n",
    "    print(label_of_point[new_outlier_indexes])\n",
    "    print(y[new_outlier_indexes])\n",
    "    if(len(new_outlier_indexes) > 0):\n",
    "        print('F-1 before: ', metrics.f1_score(y[new_outlier_indexes], label_of_point[new_outlier_indexes]))\n",
    "        label_of_point[new_outlier_indexes] = 1\n",
    "        print('F-1 after: ', metrics.f1_score(y[new_outlier_indexes], label_of_point[new_outlier_indexes]))\n",
    "        temp_remain_points = np.union1d(temp_remain_points, predict_outlier_indexes)\n",
    "\n",
    "    predict_inlier_indexes = np.where(predict_proba < 0.00001)[0]\n",
    "    new_inlier_indexes = np.setdiff1d(predict_inlier_indexes, temp_remain_points)\n",
    "#     new_inlier_indexes = new_inlier_indexes[label_of_point[new_inlier_indexes]==1]\n",
    "    print('Number of points with confidence < 0.01', len(new_inlier_indexes))\n",
    "    print(label_of_point[new_inlier_indexes])\n",
    "    print(y[new_inlier_indexes])\n",
    "    if(len(new_inlier_indexes) > 0):\n",
    "        label_of_point[new_inlier_indexes] = 0\n",
    "        temp_remain_points = np.union1d(temp_remain_points, predict_inlier_indexes)\n",
    "#         if(len(new_outlier_indexes) <= len(points_to_remove)):\n",
    "    \n",
    "    if(len(new_outlier_indexes) + len(new_inlier_indexes) > len(points_to_remove) or len(remain_points) < np.shape(L)[0]/3):  # \n",
    "        from sklearn.svm import SVC\n",
    "        clf_X = SVC(gamma='auto', probability=True, random_state=0)\n",
    "        clf_X.fit(X_transformed[remain_points], label_of_point[remain_points])\n",
    "        clf_predict_proba_X = clf_X.predict_proba(X_transformed)[:,1]\n",
    "\n",
    "        SVM_threshold = 0.5\n",
    "        print(\"F-1 score from SVM:\",metrics.f1_score(y, np.array([int(i) for i in clf_predict_proba_X > SVM_threshold])))\n",
    "        print('Number of outliers by SVM:', sum(np.array([int(i) for i in clf_predict_proba_X > SVM_threshold])))\n",
    "        SVM_threshold = np.sort(clf_predict_proba_X)[::-1][int(np.sum(y))]\n",
    "        print(\"F-1 score from SVM:\",metrics.f1_score(y, np.array([int(i) for i in clf_predict_proba_X > SVM_threshold])))\n",
    "        f1_score_list.append(metrics.f1_score(y, np.array([int(i) for i in clf_predict_proba_X > SVM_threshold])))\n",
    "        print('Number of outliers by SVM:', sum(np.array([int(i) for i in clf_predict_proba_X > SVM_threshold])))\n",
    "        print('AUROC from SVM:', roc_auc_score(y, clf_predict_proba_X))\n",
    "        break\n",
    "    else:\n",
    "        remain_points = temp_remain_points\n",
    "    \n",
    "#             break\n",
    "#     if(counter == 5):\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n",
      "Iteration = 0\n",
      "F1 for training data: 0.43640124095139604\n",
      "accuracy for training data: 0.9004202448382971\n",
      "[[4717  196]\n",
      " [ 349  211]]\n",
      "F-1 score from SVM: 0.45901639344262296\n",
      "Number of outliers by SVM: 416\n",
      "F-1 score from SVM: 0.5155172413793103\n",
      "Number of outliers by SVM: 600\n",
      "AUROC from SVM: 0.8983416809630427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Result: Loss 0.1014\tPrec @ 1 96.327%\n",
      "Number of points to change:  273\n",
      "Final Training Result: Loss 0.0473\tPrec @ 1 98.228%\n",
      "F-1 score from NN: 0.43227091633466136\n",
      "Number of outliers by NN: 444\n",
      "Number of points to remove:  273\n",
      "Number of new points with confidence > 0.99 0\n",
      "[]\n",
      "[]\n",
      "Number of points with confidence < 0.01 0\n",
      "[]\n",
      "[]\n",
      "##################################################################\n",
      "Iteration = 1\n",
      "F1 for training data: 0.4262820512820512\n",
      "accuracy for training data: 0.9311538461538461\n",
      "[[4709   22]\n",
      " [ 336  133]]\n",
      "F-1 score from SVM: 0.44907407407407407\n",
      "Number of outliers by SVM: 304\n",
      "F-1 score from SVM: 0.5362068965517242\n",
      "Number of outliers by SVM: 600\n",
      "AUROC from SVM: 0.9139555043470674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Result: Loss 0.0177\tPrec @ 1 99.346%\n",
      "Number of points to change:  260\n",
      "Final Training Result: Loss 0.0002\tPrec @ 1 100.000%\n",
      "F-1 score from NN: 0.4339869281045752\n",
      "Number of outliers by NN: 205\n",
      "Number of points to remove:  260\n",
      "Number of new points with confidence > 0.99 0\n",
      "[]\n",
      "[]\n",
      "Number of points with confidence < 0.01 37\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      "##################################################################\n",
      "Iteration = 2\n",
      "F1 for training data: 0.5283018867924528\n",
      "accuracy for training data: 0.9547920433996383\n",
      "[[4626   20]\n",
      " [ 205  126]]\n",
      "F-1 score from SVM: 0.5758157389635317\n",
      "Number of outliers by SVM: 482\n",
      "F-1 score from SVM: 0.5913793103448277\n",
      "Number of outliers by SVM: 600\n",
      "AUROC from SVM: 0.9226271044750078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Result: Loss 0.0001\tPrec @ 1 100.000%\n",
      "Number of points to change:  248\n",
      "Final Training Result: Loss 0.0001\tPrec @ 1 100.000%\n",
      "F-1 score from NN: 0.5249406175771971\n",
      "Number of outliers by NN: 282\n",
      "Number of points to remove:  248\n",
      "Number of new points with confidence > 0.99 5\n",
      "[0 0 0 0 0]\n",
      "[1 1 1 1 1]\n",
      "F-1 before:  0.0\n",
      "F-1 after:  1.0\n",
      "Number of points with confidence < 0.01 3\n",
      "[1 1 1]\n",
      "[0 0 0]\n",
      "##################################################################\n",
      "Iteration = 3\n",
      "F1 for training data: 0.6341463414634146\n",
      "accuracy for training data: 0.9652923853596971\n",
      "[[4446   18]\n",
      " [ 147  143]]\n",
      "F-1 score from SVM: 0.62882096069869\n",
      "Number of outliers by SVM: 585\n",
      "F-1 score from SVM: 0.6293103448275862\n",
      "Number of outliers by SVM: 600\n",
      "AUROC from SVM: 0.9220419223052543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Result: Loss 0.0001\tPrec @ 1 100.000%\n",
      "Number of points to change:  237\n",
      "Final Training Result: Loss 0.0001\tPrec @ 1 100.000%\n",
      "F-1 score from NN: 0.5127582017010935\n",
      "Number of outliers by NN: 263\n",
      "Number of points to remove:  237\n",
      "Number of new points with confidence > 0.99 0\n",
      "[]\n",
      "[]\n",
      "Number of points with confidence < 0.01 0\n",
      "[]\n",
      "[]\n",
      "##################################################################\n",
      "Iteration = 4\n",
      "F1 for training data: 0.7017543859649124\n",
      "accuracy for training data: 0.9736550808058446\n",
      "[[4258   18]\n",
      " [ 101  140]]\n",
      "F-1 score from SVM: 0.6384039900249376\n",
      "Number of outliers by SVM: 643\n",
      "F-1 score from SVM: 0.639655172413793\n",
      "Number of outliers by SVM: 600\n",
      "AUROC from SVM: 0.9301901296850922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Result: Loss 0.0005\tPrec @ 1 100.000%\n",
      "Number of points to change:  225\n",
      "Final Training Result: Loss 0.0001\tPrec @ 1 100.000%\n",
      "F-1 score from NN: 0.5603644646924829\n",
      "Number of outliers by NN: 318\n",
      "Number of points to remove:  225\n",
      "Number of new points with confidence > 0.99 7\n",
      "[0 0 0 0 0 0 0]\n",
      "[1 1 1 0 0 1 0]\n",
      "F-1 before:  0.0\n",
      "F-1 after:  0.7272727272727273\n",
      "Number of points with confidence < 0.01 6\n",
      "[1 1 1 1 1 1]\n",
      "[0 0 0 0 0 0]\n",
      "##################################################################\n",
      "Iteration = 5\n",
      "F1 for training data: 0.7608695652173912\n",
      "accuracy for training data: 0.979676674364896\n",
      "[[4102   16]\n",
      " [  72  140]]\n",
      "F-1 score from SVM: 0.6017964071856288\n",
      "Number of outliers by SVM: 776\n",
      "F-1 score from SVM: 0.6206896551724138\n",
      "Number of outliers by SVM: 600\n",
      "AUROC from SVM: 0.9280167776453141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Result: Loss 0.0003\tPrec @ 1 100.000%\n",
      "Number of points to change:  216\n",
      "Final Training Result: Loss 0.0001\tPrec @ 1 100.000%\n",
      "F-1 score from NN: 0.578021978021978\n",
      "Number of outliers by NN: 350\n",
      "Number of points to remove:  216\n",
      "Number of new points with confidence > 0.99 12\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 1 1 1 1 1 1 1 0 0 1 0]\n",
      "F-1 before:  0.0\n",
      "F-1 after:  0.8\n",
      "Number of points with confidence < 0.01 16\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "##################################################################\n",
      "Iteration = 6\n",
      "F1 for training data: 0.8342541436464088\n",
      "accuracy for training data: 0.9856149604411412\n",
      "[[3960   16]\n",
      " [  44  151]]\n",
      "F-1 score from SVM: 0.6242299794661191\n",
      "Number of outliers by SVM: 901\n",
      "F-1 score from SVM: 0.6574154379878577\n",
      "Number of outliers by SVM: 593\n",
      "AUROC from SVM: 0.9383179829025036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Result: Loss 0.0003\tPrec @ 1 100.000%\n",
      "Number of points to change:  208\n",
      "Final Training Result: Loss 0.0002\tPrec @ 1 100.000%\n",
      "F-1 score from NN: 0.6466466466466466\n",
      "Number of outliers by NN: 439\n",
      "Number of points to remove:  207\n",
      "Number of new points with confidence > 0.99 18\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 1 1 0 0 1 1 1 1 0 1 0 1 1 1 1 0]\n",
      "F-1 before:  0.0\n",
      "F-1 after:  0.7586206896551725\n",
      "Number of points with confidence < 0.01 5\n",
      "[1 1 1 1 1]\n",
      "[0 0 0 0 0]\n",
      "##################################################################\n",
      "Iteration = 7\n",
      "F1 for training data: 0.856396866840731\n",
      "accuracy for training data: 0.9862671660424469\n",
      "[[3786   24]\n",
      " [  31  164]]\n",
      "F-1 score from SVM: 0.5761067313523348\n",
      "Number of outliers by SVM: 1089\n",
      "F-1 score from SVM: 0.6465517241379309\n",
      "Number of outliers by SVM: 600\n",
      "AUROC from SVM: 0.9420015411008694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Result: Loss 0.0001\tPrec @ 1 100.000%\n",
      "Number of points to change:  200\n",
      "Final Training Result: Loss 0.0001\tPrec @ 1 100.000%\n",
      "F-1 score from NN: 0.5717439293598234\n",
      "Number of outliers by NN: 346\n",
      "Number of points to remove:  200\n",
      "Number of new points with confidence > 0.99 6\n",
      "[0 0 0 0 0 0]\n",
      "[1 1 0 1 1 1]\n",
      "F-1 before:  0.0\n",
      "F-1 after:  0.9090909090909091\n",
      "Number of points with confidence < 0.01 0\n",
      "[]\n",
      "[]\n",
      "##################################################################\n",
      "Iteration = 8\n",
      "F1 for training data: 0.8390804597701148\n",
      "accuracy for training data: 0.985378590078329\n",
      "[[3628   25]\n",
      " [  31  146]]\n",
      "F-1 score from SVM: 0.573134328358209\n",
      "Number of outliers by SVM: 1115\n",
      "F-1 score from SVM: 0.6448275862068964\n",
      "Number of outliers by SVM: 600\n",
      "AUROC from SVM: 0.9397142420982233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Result: Loss 0.0001\tPrec @ 1 100.000%\n",
      "Number of points to change:  191\n",
      "Final Training Result: Loss 0.0000\tPrec @ 1 100.000%\n",
      "F-1 score from NN: 0.6107977437550363\n",
      "Number of outliers by NN: 681\n",
      "Number of points to remove:  191\n",
      "Number of new points with confidence > 0.99 39\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0]\n",
      "[1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
      " 1 1]\n",
      "F-1 before:  0.0\n",
      "F-1 after:  0.7812500000000001\n",
      "Number of points with confidence < 0.01 43\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1]\n",
      "[1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0\n",
      " 0 1 1 1 1 1]\n",
      "##################################################################\n",
      "Iteration = 9\n",
      "F1 for training data: 0.7556390977443609\n",
      "accuracy for training data: 0.9663822084303078\n",
      "[[3536   69]\n",
      " [  61  201]]\n",
      "F-1 score from SVM: 0.5683544303797468\n",
      "Number of outliers by SVM: 1020\n",
      "F-1 score from SVM: 0.6448275862068964\n",
      "Number of outliers by SVM: 600\n",
      "AUROC from SVM: 0.9321979224215639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Result: Loss 0.0000\tPrec @ 1 100.000%\n",
      "Number of points to change:  193\n",
      "Final Training Result: Loss 0.0000\tPrec @ 1 100.000%\n",
      "F-1 score from NN: 0.5769585253456221\n",
      "Number of outliers by NN: 525\n",
      "Number of points to remove:  193\n",
      "Number of new points with confidence > 0.99 2\n",
      "[0 0]\n",
      "[1 1]\n",
      "F-1 before:  0.0\n",
      "F-1 after:  1.0\n",
      "Number of points with confidence < 0.01 4\n",
      "[1 1 1 1]\n",
      "[1 0 1 1]\n",
      "##################################################################\n",
      "Iteration = 10\n",
      "F1 for training data: 0.7441016333938293\n",
      "accuracy for training data: 0.9629044988161011\n",
      "[[3455   74]\n",
      " [  67  205]]\n",
      "F-1 score from SVM: 0.5937291527685123\n",
      "Number of outliers by SVM: 939\n",
      "F-1 score from SVM: 0.6258620689655171\n",
      "Number of outliers by SVM: 600\n",
      "AUROC from SVM: 0.9313286906458085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Result: Loss 0.0000\tPrec @ 1 100.000%\n",
      "Number of points to change:  190\n",
      "Final Training Result: Loss 0.0000\tPrec @ 1 100.000%\n",
      "F-1 score from NN: 0.5783132530120482\n",
      "Number of outliers by NN: 602\n",
      "Number of points to remove:  188\n",
      "Number of new points with confidence > 0.99 11\n",
      "[0 0 0 0 0 0 0 0 0 0 0]\n",
      "[1 0 0 1 1 1 1 1 1 0 0]\n",
      "F-1 before:  0.0\n",
      "F-1 after:  0.7777777777777778\n",
      "Number of points with confidence < 0.01 3\n",
      "[1 1 1]\n",
      "[0 0 0]\n",
      "##################################################################\n",
      "Iteration = 11\n",
      "F1 for training data: 0.7281879194630873\n",
      "accuracy for training data: 0.9561332250203087\n",
      "[[3314   84]\n",
      " [  78  217]]\n",
      "F-1 score from SVM: 0.5900962861072903\n",
      "Number of outliers by SVM: 894\n",
      "F-1 score from SVM: 0.6206896551724138\n",
      "Number of outliers by SVM: 600\n",
      "AUROC from SVM: 0.9239201389898519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Result: Loss 0.0000\tPrec @ 1 100.000%\n",
      "Number of points to change:  184\n",
      "Final Training Result: Loss 0.0002\tPrec @ 1 100.000%\n",
      "F-1 score from NN: 0.5779036827195467\n",
      "Number of outliers by NN: 852\n",
      "Number of points to remove:  183\n",
      "Number of new points with confidence > 0.99 89\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 0 0 0 0 1 1 1 1 1 1\n",
      " 1 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 1 1 0 0 1 1 0 0 0\n",
      " 0 1 1 1 0 0 0 0 0 0 0 0 0 1 1]\n",
      "F-1 before:  0.0\n",
      "F-1 after:  0.6515151515151515\n",
      "Number of points with confidence < 0.01 0\n",
      "[]\n",
      "[]\n",
      "##################################################################\n",
      "Iteration = 12\n",
      "F1 for training data: 0.7245590230664858\n",
      "accuracy for training data: 0.9438295517432208\n",
      "[[3144  138]\n",
      " [  65  267]]\n",
      "F-1 score from SVM: 0.5731147540983607\n",
      "Number of outliers by SVM: 965\n",
      "F-1 score from SVM: 0.6155172413793103\n",
      "Number of outliers by SVM: 600\n",
      "AUROC from SVM: 0.9086143540461168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Result: Loss 0.0003\tPrec @ 1 100.000%\n",
      "Number of points to change:  180\n",
      "Final Training Result: Loss 0.0001\tPrec @ 1 100.000%\n",
      "F-1 score from NN: 0.560500695410292\n",
      "Number of outliers by NN: 878\n",
      "Number of points to remove:  180\n",
      "Number of new points with confidence > 0.99 2\n",
      "[0 0]\n",
      "[0 0]\n",
      "F-1 before:  0.0\n",
      "F-1 after:  0.0\n",
      "Number of points with confidence < 0.01 0\n",
      "[]\n",
      "[]\n",
      "##################################################################\n",
      "Iteration = 13\n",
      "F1 for training data: 0.7388888888888889\n",
      "accuracy for training data: 0.9454439930354034\n",
      "[[2992  149]\n",
      " [  39  266]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/sklearn/metrics/_classification.py:1511: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-1 score from SVM: 0.5580246913580248\n",
      "Number of outliers by SVM: 1060\n",
      "F-1 score from SVM: 0.6172413793103448\n",
      "Number of outliers by SVM: 600\n",
      "AUROC from SVM: 0.9110463129888634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Result: Loss 0.0000\tPrec @ 1 100.000%\n",
      "Number of points to change:  172\n",
      "Final Training Result: Loss 0.0000\tPrec @ 1 100.000%\n",
      "F-1 score from NN: 0.5435494433529798\n",
      "Number of outliers by NN: 967\n",
      "Number of points to remove:  172\n",
      "Number of new points with confidence > 0.99 49\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 1 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0\n",
      " 1 1 1 0 1 1 0 0 0 1 0 1]\n",
      "F-1 before:  0.0\n",
      "F-1 after:  0.7435897435897436\n",
      "Number of points with confidence < 0.01 0\n",
      "[]\n",
      "[]\n",
      "##################################################################\n",
      "Iteration = 14\n",
      "F1 for training data: 0.7305389221556885\n",
      "accuracy for training data: 0.9328558639212176\n",
      "[[2821  187]\n",
      " [  38  305]]\n",
      "F-1 score from SVM: 0.5556223692122669\n",
      "Number of outliers by SVM: 1103\n",
      "F-1 score from SVM: 0.6212251941328731\n",
      "Number of outliers by SVM: 599\n",
      "AUROC from SVM: 0.9117639789479807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Result: Loss 0.0000\tPrec @ 1 100.000%\n",
      "Number of points to change:  167\n",
      "Final Training Result: Loss 0.0000\tPrec @ 1 100.000%\n",
      "F-1 score from NN: 0.5529411764705882\n",
      "Number of outliers by NN: 970\n",
      "Number of points to remove:  167\n",
      "Number of new points with confidence > 0.99 99\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 1 0 0 0 0 1 1 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0\n",
      " 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 1 0 1 1 0 1 1 0 1]\n",
      "F-1 before:  0.0\n",
      "F-1 after:  0.4651162790697675\n",
      "Number of points with confidence < 0.01 0\n",
      "[]\n",
      "[]\n",
      "##################################################################\n",
      "Iteration = 15\n",
      "F1 for training data: 0.6962809917355373\n",
      "accuracy for training data: 0.9105295191722459\n",
      "[[2655  257]\n",
      " [  37  337]]\n",
      "F-1 score from SVM: 0.5506216696269982\n",
      "Number of outliers by SVM: 1129\n",
      "F-1 score from SVM: 0.5810344827586207\n",
      "Number of outliers by SVM: 600\n",
      "AUROC from SVM: 0.9031481710331191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Result: Loss 0.0045\tPrec @ 1 99.878%\n",
      "Number of points to change:  164\n",
      "Final Training Result: Loss 0.0000\tPrec @ 1 100.000%\n",
      "F-1 score from NN: 0.5029115934356803\n",
      "Number of outliers by NN: 1329\n",
      "Number of points to remove:  164\n",
      "Number of new points with confidence > 0.99 396\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 1 1 0 1 1 0 0\n",
      " 1 0 1 0 1 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 1 1 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1]\n",
      "F-1 before:  0.0\n",
      "F-1 after:  0.32558139534883723\n",
      "Number of points with confidence < 0.01 0\n",
      "[]\n",
      "[]\n",
      "##################################################################\n",
      "Iteration = 16\n",
      "F1 for training data: 0.5863453815261044\n",
      "accuracy for training data: 0.8259154929577465\n",
      "[[2494  584]\n",
      " [  34  438]]\n",
      "F-1 score from SVM: 0.5\n",
      "Number of outliers by SVM: 1388\n",
      "F-1 score from SVM: 0.2810344827586207\n",
      "Number of outliers by SVM: 600\n",
      "AUROC from SVM: 0.863205126341194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Result: Loss 0.0001\tPrec @ 1 100.000%\n",
      "Number of points to change:  177\n",
      "Final Training Result: Loss 0.0001\tPrec @ 1 100.000%\n",
      "F-1 score from NN: 0.4952178533475027\n",
      "Number of outliers by NN: 1322\n",
      "Number of points to remove:  177\n",
      "Number of new points with confidence > 0.99 3\n",
      "[0 0 0]\n",
      "[0 0 0]\n",
      "F-1 before:  0.0\n",
      "F-1 after:  0.0\n",
      "Number of points with confidence < 0.01 0\n",
      "[]\n",
      "[]\n",
      "##################################################################\n",
      "Iteration = 17\n",
      "F1 for training data: 0.5850066934404284\n",
      "accuracy for training data: 0.816350710900474\n",
      "[[2319  587]\n",
      " [  33  437]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/sklearn/metrics/_classification.py:1511: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-1 score from SVM: 0.49541284403669733\n",
      "Number of outliers by SVM: 1402\n",
      "F-1 score from SVM: 0.2741379310344828\n",
      "Number of outliers by SVM: 600\n",
      "AUROC from SVM: 0.8621301721380592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Result: Loss 0.0377\tPrec @ 1 99.733%\n",
      "Number of points to change:  168\n",
      "Final Training Result: Loss 0.0000\tPrec @ 1 100.000%\n",
      "F-1 score from NN: 0.4792719919110212\n",
      "Number of outliers by NN: 1418\n",
      "Number of points to remove:  168\n",
      "Number of new points with confidence > 0.99 41\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0]\n",
      "[0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0]\n",
      "F-1 before:  0.0\n",
      "F-1 after:  0.17777777777777776\n",
      "Number of points with confidence < 0.01 0\n",
      "[]\n",
      "[]\n",
      "##################################################################\n",
      "Iteration = 18\n",
      "F1 for training data: 0.5753604193971166\n",
      "accuracy for training data: 0.8018954448180985\n",
      "[[2184  615]\n",
      " [  33  439]]\n",
      "F-1 score from SVM: 0.4580223880597015\n",
      "Number of outliers by SVM: 1584\n",
      "F-1 score from SVM: 0.2534482758620689\n",
      "Number of outliers by SVM: 600\n",
      "AUROC from SVM: 0.852723459626065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Result: Loss 0.0000\tPrec @ 1 100.000%\n",
      "Number of points to change:  163\n",
      "Final Training Result: Loss 0.0001\tPrec @ 1 100.000%\n",
      "F-1 score from NN: 0.4767616191904048\n",
      "Number of outliers by NN: 1441\n",
      "Number of points to remove:  162\n",
      "Number of new points with confidence > 0.99 18\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0]\n",
      "F-1 before:  0.0\n",
      "F-1 after:  0.3636363636363636\n",
      "Number of points with confidence < 0.01 0\n",
      "[]\n",
      "[]\n",
      "##################################################################\n",
      "Iteration = 19\n",
      "F1 for training data: 0.5723514211886305\n",
      "accuracy for training data: 0.7882954908858331\n",
      "[[2022  629]\n",
      " [  33  443]]\n",
      "F-1 score from SVM: 0.44145356662180346\n",
      "Number of outliers by SVM: 1669\n",
      "F-1 score from SVM: 0.2517241379310345\n",
      "Number of outliers by SVM: 600\n",
      "AUROC from SVM: 0.8468376537466197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Result: Loss 0.0000\tPrec @ 1 100.000%\n",
      "Number of points to change:  156\n",
      "Final Training Result: Loss 0.0000\tPrec @ 1 100.000%\n",
      "F-1 score from NN: 0.45957446808510644\n",
      "Number of outliers by NN: 1555\n",
      "Number of points to remove:  156\n",
      "Number of new points with confidence > 0.99 99\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      "F-1 before:  0.0\n",
      "F-1 after:  0.24778761061946902\n",
      "Number of points with confidence < 0.01 0\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "last_training_data_indexes = []\n",
    "counter = 0\n",
    "\n",
    "ratio_to_remove = 0.05\n",
    "\n",
    "remain_points = np.array(range(len(y)))\n",
    "\n",
    "mid = np.shape(L)[1]/2\n",
    "label_of_point = np.full((len(y)), 0)\n",
    "label_of_point[np.sum(L, axis = 1) > mid] = 1\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "transformer = RobustScaler().fit(X)\n",
    "X_transformed = transformer.transform(X)\n",
    "\n",
    "for i_range in range(0, 20):\n",
    "    print(\"##################################################################\")\n",
    "    print('Iteration = {}'.format(i_range))\n",
    "    print('F1 for training data:', metrics.f1_score(y[remain_points], label_of_point[remain_points]))\n",
    "    print('accuracy for training data:', metrics.accuracy_score(y[remain_points], label_of_point[remain_points]))\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    print(confusion_matrix(y[remain_points], label_of_point[remain_points]))\n",
    "\n",
    "    from sklearn.svm import SVC\n",
    "    clf_X = SVC(gamma='auto', probability=True, random_state=0)\n",
    "    clf_X.fit(X_transformed[remain_points], label_of_point[remain_points])\n",
    "    clf_predict_proba_X = clf_X.predict_proba(X_transformed)[:,1]\n",
    "\n",
    "    SVM_threshold = 0.1\n",
    "    print(\"F-1 score from SVM:\",metrics.f1_score(y, np.array([int(i) for i in clf_predict_proba_X > SVM_threshold])))\n",
    "    print('Number of outliers by SVM:', sum(np.array([int(i) for i in clf_predict_proba_X > SVM_threshold])))\n",
    "    SVM_threshold = np.sort(clf_predict_proba_X)[::-1][600]\n",
    "    print(\"F-1 score from SVM:\",metrics.f1_score(y, np.array([int(i) for i in clf_predict_proba_X > SVM_threshold])))\n",
    "    print('Number of outliers by SVM:', sum(np.array([int(i) for i in clf_predict_proba_X > SVM_threshold])))\n",
    "    print('AUROC from SVM:', roc_auc_score(y, clf_predict_proba_X))\n",
    "    \n",
    "    # start pruning points\n",
    "    loss_list, model = run_NN(X_transformed[remain_points],label_of_point[remain_points],3, dim = 10)\n",
    "    \n",
    "    loss_threshold = np.sort(loss_list)[::-1][int(ratio_to_remove * len(loss_list))]\n",
    "    print('Number of points to change: ', int(ratio_to_remove * len(loss_list)))\n",
    "    points_to_remove = remain_points[(loss_list > loss_threshold)]\n",
    "    \n",
    "#     inlier_labels = np.where(label_of_point[remain_points] == 0)[0]\n",
    "#     loss_threshold = np.sort(loss_list[inlier_labels])[::-1][int(ratio_to_remove * len(loss_list[inlier_labels]))]\n",
    "#     print('Number of inliers to change: ', int(ratio_to_remove * len(loss_list[inlier_labels])))\n",
    "#     points_to_remove = remain_points[inlier_labels][(loss_list[inlier_labels] > loss_threshold)]\n",
    "    \n",
    "#     outlier_labels = np.where(label_of_point[remain_points] == 1)[0]\n",
    "#     loss_threshold = np.sort(loss_list[outlier_labels])[::-1][int(ratio_to_remove * len(loss_list[outlier_labels]))]\n",
    "#     print('Number of outliers to change: ', int(ratio_to_remove * len(loss_list[outlier_labels])))\n",
    "#     points_to_remove = np.append(points_to_remove, remain_points[outlier_labels][(loss_list[outlier_labels] > loss_threshold)])\n",
    "\n",
    "    \n",
    "    loss_list, model = run_NN(X_transformed[remain_points],label_of_point[remain_points],10, dim = 10)\n",
    "    predict_proba = inference_NN(model, X_transformed, y)[:,1]\n",
    "    print(\"F-1 score from NN:\",metrics.f1_score(y, np.array([int(i) for i in predict_proba > 0.5])))\n",
    "    print('Number of outliers by NN:', sum(np.array([int(i) for i in predict_proba > 0.5])))\n",
    "    \n",
    "    print('Number of points to remove: ', len(points_to_remove))\n",
    "    remain_points = np.setdiff1d(np.array(remain_points), points_to_remove)\n",
    "    \n",
    "    predict_outlier_indexes = np.where(predict_proba > 0.99999)[0]\n",
    "    new_outlier_indexes = np.setdiff1d(predict_outlier_indexes, remain_points)\n",
    "    new_outlier_indexes = new_outlier_indexes[label_of_point[new_outlier_indexes] != 1]\n",
    "    print('Number of new points with confidence > 0.99', len(new_outlier_indexes))\n",
    "    print(label_of_point[new_outlier_indexes])\n",
    "    print(y[new_outlier_indexes])\n",
    "    if(len(new_outlier_indexes) > 0):\n",
    "        print('F-1 before: ', metrics.f1_score(y[new_outlier_indexes], label_of_point[new_outlier_indexes]))\n",
    "        label_of_point[new_outlier_indexes] = 1\n",
    "        print('F-1 after: ', metrics.f1_score(y[new_outlier_indexes], label_of_point[new_outlier_indexes]))\n",
    "        remain_points = np.union1d(remain_points, predict_outlier_indexes)\n",
    "\n",
    "    predict_inlier_indexes = np.where(predict_proba < 0.00001)[0]\n",
    "    new_inlier_indexes = np.setdiff1d(predict_inlier_indexes, remain_points)\n",
    "    new_inlier_indexes = new_inlier_indexes[label_of_point[new_inlier_indexes] != 0]\n",
    "    print('Number of points with confidence < 0.01', len(new_inlier_indexes))\n",
    "    print(label_of_point[new_inlier_indexes])\n",
    "    print(y[new_inlier_indexes])\n",
    "    if(len(new_inlier_indexes) > 0):\n",
    "        label_of_point[new_inlier_indexes] = 0\n",
    "        remain_points = np.union1d(remain_points, predict_inlier_indexes)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n",
      "Iteration = 0\n",
      "F1 for training data: 0.43640124095139604\n",
      "[[4717  196]\n",
      " [ 349  211]]\n",
      "F-1 score from SVM: 0.4546424759871932\n",
      "Number of outliers by SVM: 377\n",
      "AUROC from SVM: 0.8983416809630427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Result: Loss 0.1149\tPrec @ 1 95.286%\n",
      "Number of inliers to change:  101\n",
      "Number of outliers to change:  8\n",
      "Number of points to remove:  109\n",
      "##################################################################\n",
      "Iteration = 1\n",
      "F1 for training data: 0.4385026737967915\n",
      "[[4634  194]\n",
      " [ 331  205]]\n",
      "F-1 score from SVM: 0.4482758620689656\n",
      "Number of outliers by SVM: 426\n",
      "AUROC from SVM: 0.9059877584251694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Result: Loss 0.0687\tPrec @ 1 97.260%\n",
      "Number of inliers to change:  99\n",
      "Number of outliers to change:  7\n",
      "Number of points to remove:  106\n",
      "##################################################################\n",
      "Iteration = 2\n",
      "F1 for training data: 0.4466230936819172\n",
      "[[4545  187]\n",
      " [ 321  205]]\n",
      "F-1 score from SVM: 0.42124212421242124\n",
      "Number of outliers by SVM: 551\n",
      "AUROC from SVM: 0.897724695414498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Result: Loss 0.0457\tPrec @ 1 98.345%\n",
      "Number of inliers to change:  97\n",
      "Number of outliers to change:  7\n",
      "Number of points to remove:  104\n",
      "##################################################################\n",
      "Iteration = 3\n",
      "F1 for training data: 0.45240761478163494\n",
      "[[4463  183]\n",
      " [ 306  202]]\n",
      "F-1 score from SVM: 0.4095157179269329\n",
      "Number of outliers by SVM: 617\n",
      "AUROC from SVM: 0.8687223764938501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Result: Loss 0.0266\tPrec @ 1 99.146%\n",
      "Number of inliers to change:  95\n",
      "Number of outliers to change:  7\n",
      "Number of points to remove:  102\n",
      "##################################################################\n",
      "Iteration = 4\n",
      "F1 for training data: 0.47251461988304094\n",
      "[[4399  176]\n",
      " [ 275  202]]\n",
      "F-1 score from SVM: 0.4236610711430855\n",
      "Number of outliers by SVM: 691\n",
      "AUROC from SVM: 0.8599700866505772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Result: Loss 0.0144\tPrec @ 1 99.565%\n",
      "Number of inliers to change:  93\n",
      "Number of outliers to change:  7\n",
      "Number of points to remove:  100\n",
      "##################################################################\n",
      "Iteration = 5\n",
      "F1 for training data: 0.48969696969696974\n",
      "[[4329  169]\n",
      " [ 252  202]]\n",
      "F-1 score from SVM: 0.4388318863456984\n",
      "Number of outliers by SVM: 707\n",
      "AUROC from SVM: 0.8736464481986566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Result: Loss 0.0074\tPrec @ 1 99.960%\n",
      "Number of inliers to change:  91\n",
      "Number of outliers to change:  7\n",
      "Number of points to remove:  98\n",
      "##################################################################\n",
      "Iteration = 6\n",
      "F1 for training data: 0.5018820577164367\n",
      "[[4257  164]\n",
      " [ 233  200]]\n",
      "F-1 score from SVM: 0.4417177914110429\n",
      "Number of outliers by SVM: 744\n",
      "AUROC from SVM: 0.8735797519699922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Result: Loss 0.0044\tPrec @ 1 100.000%\n",
      "Number of inliers to change:  89\n",
      "Number of outliers to change:  7\n",
      "Number of points to remove:  96\n",
      "##################################################################\n",
      "Iteration = 7\n",
      "F1 for training data: 0.5215123859191656\n",
      "[[4191  157]\n",
      " [ 210  200]]\n",
      "F-1 score from SVM: 0.47022900763358777\n",
      "Number of outliers by SVM: 750\n",
      "AUROC from SVM: 0.8835124015003927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Result: Loss 0.0013\tPrec @ 1 100.000%\n",
      "Number of inliers to change:  88\n",
      "Number of outliers to change:  7\n",
      "Number of points to remove:  95\n",
      "##################################################################\n",
      "Iteration = 8\n",
      "F1 for training data: 0.5278514588859415\n",
      "[[4108  151]\n",
      " [ 205  199]]\n",
      "F-1 score from SVM: 0.4668674698795181\n",
      "Number of outliers by SVM: 768\n",
      "AUROC from SVM: 0.8721711348899421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Result: Loss 0.0013\tPrec @ 1 100.000%\n",
      "Number of inliers to change:  86\n",
      "Number of outliers to change:  7\n",
      "Number of points to remove:  93\n",
      "##################################################################\n",
      "Iteration = 9\n",
      "F1 for training data: 0.5271739130434783\n",
      "[[4028  149]\n",
      " [ 199  194]]\n",
      "F-1 score from SVM: 0.46053584359160027\n",
      "Number of outliers by SVM: 821\n",
      "AUROC from SVM: 0.8820367247244919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Result: Loss 0.0122\tPrec @ 1 99.847%\n",
      "Number of inliers to change:  84\n",
      "Number of outliers to change:  6\n",
      "Number of points to remove:  90\n",
      "##################################################################\n",
      "Iteration = 10\n",
      "F1 for training data: 0.5193370165745858\n",
      "[[3944  149]\n",
      " [ 199  188]]\n",
      "F-1 score from SVM: 0.41230366492146603\n",
      "Number of outliers by SVM: 968\n",
      "AUROC from SVM: 0.8702934634061238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Result: Loss 0.0004\tPrec @ 1 100.000%\n",
      "Number of inliers to change:  82\n",
      "Number of outliers to change:  6\n",
      "Number of points to remove:  88\n",
      "##################################################################\n",
      "Iteration = 11\n",
      "F1 for training data: 0.5318246110325318\n",
      "[[3873  143]\n",
      " [ 188  188]]\n",
      "F-1 score from SVM: 0.4113475177304965\n",
      "Number of outliers by SVM: 991\n",
      "AUROC from SVM: 0.8644861664388939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Result: Loss 0.0005\tPrec @ 1 100.000%\n",
      "Number of inliers to change:  81\n",
      "Number of outliers to change:  6\n",
      "Number of points to remove:  87\n",
      "##################################################################\n",
      "Iteration = 12\n",
      "F1 for training data: 0.5441389290882779\n",
      "[[3802  137]\n",
      " [ 178  188]]\n",
      "F-1 score from SVM: 0.4059222702035781\n",
      "Number of outliers by SVM: 1061\n",
      "AUROC from SVM: 0.8635973074350847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Result: Loss 0.0005\tPrec @ 1 100.000%\n",
      "Number of inliers to change:  79\n",
      "Number of outliers to change:  6\n",
      "Number of points to remove:  85\n",
      "##################################################################\n",
      "Iteration = 13\n",
      "F1 for training data: 0.556390977443609\n",
      "[[3740  134]\n",
      " [ 161  185]]\n",
      "F-1 score from SVM: 0.40483383685800606\n",
      "Number of outliers by SVM: 1095\n",
      "AUROC from SVM: 0.8563265098426913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Result: Loss 0.0012\tPrec @ 1 99.976%\n",
      "Number of inliers to change:  78\n",
      "Number of outliers to change:  6\n",
      "Number of points to remove:  84\n",
      "##################################################################\n",
      "Iteration = 14\n",
      "F1 for training data: 0.5548780487804879\n",
      "[[3662  131]\n",
      " [ 161  182]]\n",
      "F-1 score from SVM: 0.39504716981132076\n",
      "Number of outliers by SVM: 1136\n",
      "AUROC from SVM: 0.8509650053793144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Result: Loss 0.0003\tPrec @ 1 100.000%\n",
      "Number of inliers to change:  76\n",
      "Number of outliers to change:  6\n",
      "Number of points to remove:  82\n",
      "##################################################################\n",
      "Iteration = 15\n",
      "F1 for training data: 0.5510835913312694\n",
      "[[3586  129]\n",
      " [ 161  178]]\n",
      "F-1 score from SVM: 0.39155920281359907\n",
      "Number of outliers by SVM: 1146\n",
      "AUROC from SVM: 0.8462791864150504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Result: Loss 0.0032\tPrec @ 1 99.926%\n",
      "Number of inliers to change:  74\n",
      "Number of outliers to change:  6\n",
      "Number of points to remove:  80\n",
      "##################################################################\n",
      "Iteration = 16\n",
      "F1 for training data: 0.5751211631663974\n",
      "[[3533  123]\n",
      " [ 140  178]]\n",
      "F-1 score from SVM: 0.4089347079037801\n",
      "Number of outliers by SVM: 1186\n",
      "AUROC from SVM: 0.847098986653485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Result: Loss 0.0002\tPrec @ 1 100.000%\n",
      "Number of inliers to change:  73\n",
      "Number of outliers to change:  6\n",
      "Number of points to remove:  79\n",
      "##################################################################\n",
      "Iteration = 17\n",
      "F1 for training data: 0.5700164744645798\n",
      "[[3461  122]\n",
      " [ 139  173]]\n",
      "F-1 score from SVM: 0.4\n",
      "Number of outliers by SVM: 1225\n",
      "AUROC from SVM: 0.8422185310110203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Result: Loss 0.0002\tPrec @ 1 100.000%\n",
      "Number of inliers to change:  72\n",
      "Number of outliers to change:  5\n",
      "Number of points to remove:  77\n",
      "##################################################################\n",
      "Iteration = 18\n",
      "F1 for training data: 0.574750830564784\n",
      "[[3389  117]\n",
      " [ 139  173]]\n",
      "F-1 score from SVM: 0.39688715953307396\n",
      "Number of outliers by SVM: 1239\n",
      "AUROC from SVM: 0.840631996743334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Result: Loss 0.0003\tPrec @ 1 100.000%\n",
      "Number of inliers to change:  70\n",
      "Number of outliers to change:  5\n",
      "Number of points to remove:  75\n",
      "##################################################################\n",
      "Iteration = 19\n",
      "F1 for training data: 0.5864406779661017\n",
      "[[3326  112]\n",
      " [ 132  173]]\n",
      "F-1 score from SVM: 0.39473684210526316\n",
      "Number of outliers by SVM: 1264\n",
      "AUROC from SVM: 0.8380695530807478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Result: Loss 0.0001\tPrec @ 1 100.000%\n",
      "Number of inliers to change:  69\n",
      "Number of outliers to change:  5\n",
      "Number of points to remove:  74\n"
     ]
    }
   ],
   "source": [
    "# Prune inlier and outlier separately\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "last_training_data_indexes = []\n",
    "counter = 0\n",
    "\n",
    "ratio_to_remove = 0.02\n",
    "\n",
    "flipped = np.zeros(len(y))\n",
    "remain_points = np.array(range(len(y)))\n",
    "\n",
    "mid = np.shape(L)[1]/2\n",
    "label_of_point = np.full((len(y)), 0)\n",
    "label_of_point[np.sum(L, axis = 1) > mid] = 1\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "transformer = RobustScaler().fit(X)\n",
    "X_transformed = transformer.transform(X)\n",
    "\n",
    "for i_range in range(0, 20):\n",
    "    print(\"##################################################################\")\n",
    "    print('Iteration = {}'.format(i_range))\n",
    "    print('F1 for training data:', metrics.f1_score(y[remain_points], label_of_point[remain_points]))\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    print(confusion_matrix(y[remain_points], label_of_point[remain_points]))\n",
    "\n",
    "    from sklearn.svm import SVC\n",
    "    clf_X = SVC(gamma='auto', probability=True, random_state=0)\n",
    "    clf_X.fit(X_transformed[remain_points], label_of_point[remain_points])\n",
    "    clf_predict_proba_X = clf_X.predict_proba(X_transformed)[:,1]\n",
    "\n",
    "    SVM_threshold = 0.5\n",
    "    print(\"F-1 score from SVM:\",metrics.f1_score(y, np.array([int(i) for i in clf_predict_proba_X > 0.5])))\n",
    "    print('Number of outliers by SVM:', sum(np.array([int(i) for i in clf_predict_proba_X > SVM_threshold])))\n",
    "    SVM_threshold = 0\n",
    "    print('AUROC from SVM:', roc_auc_score(y, clf_predict_proba_X))\n",
    "    \n",
    "    # start pruning points\n",
    "    loss_list, model = run_NN(X_transformed[remain_points],label_of_point[remain_points],1, dim = 10)\n",
    "    \n",
    "    inlier_labels = np.where(label_of_point[remain_points] == 0)[0]\n",
    "    loss_threshold = np.sort(loss_list[inlier_labels])[::-1][int(ratio_to_remove * len(loss_list[inlier_labels]))]\n",
    "    print('Number of inliers to change: ', int(ratio_to_remove * len(loss_list[inlier_labels])))\n",
    "    points_to_remove = remain_points[inlier_labels][(loss_list[inlier_labels] > loss_threshold)]\n",
    "    \n",
    "    outlier_labels = np.where(label_of_point[remain_points] == 1)[0]\n",
    "    loss_threshold = np.sort(loss_list[outlier_labels])[::-1][int(ratio_to_remove * len(loss_list[outlier_labels]))]\n",
    "    print('Number of outliers to change: ', int(ratio_to_remove * len(loss_list[outlier_labels])))\n",
    "    points_to_remove = np.append(points_to_remove, remain_points[outlier_labels][(loss_list[outlier_labels] > loss_threshold)])\n",
    "    \n",
    "    print('Number of points to remove: ', len(points_to_remove))\n",
    "    remain_points = np.setdiff1d(np.array(remain_points), points_to_remove)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prune inliers only\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "last_training_data_indexes = []\n",
    "counter = 0\n",
    "\n",
    "ratio_to_remove = 0.02\n",
    "\n",
    "flipped = np.zeros(len(y))\n",
    "remain_points = np.array(range(len(y)))\n",
    "\n",
    "mid = np.shape(L)[1]/2\n",
    "label_of_point = np.full((len(y)), 0)\n",
    "label_of_point[np.sum(L, axis = 1) > mid] = 1\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "transformer = RobustScaler().fit(X)\n",
    "X_transformed = transformer.transform(X)\n",
    "\n",
    "for i_range in range(0, 20):\n",
    "    print(\"##################################################################\")\n",
    "    print('Iteration = {}'.format(i_range))\n",
    "    print('F1 for training data:', metrics.f1_score(y[remain_points], label_of_point[remain_points]))\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    print(confusion_matrix(y[remain_points], label_of_point[remain_points]))\n",
    "\n",
    "    from sklearn.svm import SVC\n",
    "    clf_X = SVC(gamma='auto', probability=True, random_state=0)\n",
    "    clf_X.fit(X_transformed[remain_points], label_of_point[remain_points])\n",
    "    clf_predict_proba_X = clf_X.predict_proba(X_transformed)[:,1]\n",
    "\n",
    "    SVM_threshold = 0.5\n",
    "    print(\"F-1 score from SVM:\",metrics.f1_score(y, np.array([int(i) for i in clf_predict_proba_X > SVM_threshold])))\n",
    "    print('AUROC from SVM:', roc_auc_score(y, clf_predict_proba_X))\n",
    "    print('Number of outliers by SVM:', sum(np.array([int(i) for i in clf_predict_proba_X > SVM_threshold])))\n",
    "    \n",
    "    # start pruning points\n",
    "    loss_list = run_NN(X_transformed[remain_points],label_of_point[remain_points],2, dim = 9)\n",
    "    inlier_labels = np.where(label_of_point[remain_points] == 0)[0]\n",
    "    loss_threshold = np.sort(loss_list[inlier_labels])[::-1][int(ratio_to_remove * len(loss_list[inlier_labels]))]\n",
    "    print('Number of points to change: ', int(ratio_to_remove * len(loss_list[inlier_labels])))\n",
    "    \n",
    "    points_to_change = (loss_list[inlier_labels] > loss_threshold)\n",
    "    points_to_remove = []\n",
    "    for i in remain_points[inlier_labels][points_to_change]:\n",
    "#         if(flipped[i] == False):\n",
    "#             label_of_point[i] = (0 if label_of_point[i] == 1 else 1) \n",
    "#             flipped[i] ==  True\n",
    "#         else:\n",
    "            points_to_remove.append(i)\n",
    "    print('Number of points to remove: ', len(points_to_remove))\n",
    "    remain_points = np.setdiff1d(np.array(remain_points), points_to_remove)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOT WORKING: Prune points using LR, add good prediction results back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n",
      "Iteration = 0\n",
      "F1 for training data: 0.43640124095139604\n",
      "[[4717  196]\n",
      " [ 349  211]]\n",
      "F-1 score from SVM: 0.43782117163412126\n",
      "Number of outliers by SVM: 413\n",
      "F-1 score from SVM: 0.5482758620689656\n",
      "Number of outliers by SVM: 600\n",
      "AUROC from SVM: 0.8152416329853741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prune points using LR, add good prediction results back\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "last_training_data_indexes = []\n",
    "counter = 0\n",
    "\n",
    "ratio_to_remove = 0.05\n",
    "\n",
    "remain_points = np.array(range(len(y)))\n",
    "\n",
    "mid = np.shape(L)[1]/2\n",
    "label_of_point = np.full((len(y)), 0)\n",
    "label_of_point[np.sum(L, axis = 1) > mid] = 1\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "transformer = RobustScaler().fit(np.concatenate((X, scores_for_training), axis = 1))\n",
    "X_transformed = transformer.transform(np.concatenate((X, scores_for_training), axis = 1))\n",
    "\n",
    "for i_range in range(0, 20):\n",
    "    print(\"##################################################################\")\n",
    "    print('Iteration = {}'.format(i_range))\n",
    "    print('F1 for training data:', metrics.f1_score(y[remain_points], label_of_point[remain_points]))\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    print(confusion_matrix(y[remain_points], label_of_point[remain_points]))\n",
    "\n",
    "    from sklearn.svm import SVC\n",
    "    clf_X = SVC(gamma='auto', probability=True, random_state=0)\n",
    "    clf_X.fit(X_transformed[remain_points], label_of_point[remain_points])\n",
    "    clf_predict_proba_X = clf_X.predict_proba(X_transformed)[:,1]\n",
    "\n",
    "    SVM_threshold = 0.1\n",
    "    print(\"F-1 score from SVM:\",metrics.f1_score(y, np.array([int(i) for i in clf_predict_proba_X > SVM_threshold])))\n",
    "    print('Number of outliers by SVM:', sum(np.array([int(i) for i in clf_predict_proba_X > SVM_threshold])))\n",
    "    SVM_threshold = np.sort(clf_predict_proba_X)[::-1][600]\n",
    "    print(\"F-1 score from SVM:\",metrics.f1_score(y, np.array([int(i) for i in clf_predict_proba_X > SVM_threshold])))\n",
    "    print('Number of outliers by SVM:', sum(np.array([int(i) for i in clf_predict_proba_X > SVM_threshold])))\n",
    "    print('AUROC from SVM:', roc_auc_score(y, clf_predict_proba_X))\n",
    "    \n",
    "    # start pruning points\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    clf = LogisticRegression(random_state=0, penalty='l2', max_iter=5).fit(X_transformed[remain_points], label_of_point[remain_points]) \n",
    "    clf_predict_proba = clf.predict_proba(X_transformed[remain_points])\n",
    "    from sklearn.metrics import log_loss\n",
    "    loss_list = []\n",
    "    for y_true, y_pred in zip(label_of_point[remain_points], clf_predict_proba):\n",
    "        loss_list.append(log_loss(np.array([y_true]), np.array(y_pred.reshape(1,2)), labels =np.array([0,1])))\n",
    "    \n",
    "    loss_threshold = np.sort(loss_list)[::-1][int(ratio_to_remove * len(loss_list))]\n",
    "    print('Number of points to change: ', int(ratio_to_remove * len(loss_list)))\n",
    "    points_to_remove = remain_points[(loss_list > loss_threshold)]\n",
    "    \n",
    "#     inlier_labels = np.where(label_of_point[remain_points] == 0)[0]\n",
    "#     loss_threshold = np.sort(loss_list[inlier_labels])[::-1][int(ratio_to_remove * len(loss_list[inlier_labels]))]\n",
    "#     print('Number of inliers to change: ', int(ratio_to_remove * len(loss_list[inlier_labels])))\n",
    "#     points_to_remove = remain_points[inlier_labels][(loss_list[inlier_labels] > loss_threshold)]\n",
    "    \n",
    "#     outlier_labels = np.where(label_of_point[remain_points] == 1)[0]\n",
    "#     loss_threshold = np.sort(loss_list[outlier_labels])[::-1][int(ratio_to_remove * len(loss_list[outlier_labels]))]\n",
    "#     print('Number of outliers to change: ', int(ratio_to_remove * len(loss_list[outlier_labels])))\n",
    "#     points_to_remove = np.append(points_to_remove, remain_points[outlier_labels][(loss_list[outlier_labels] > loss_threshold)])\n",
    "    \n",
    "    print('Number of points to remove: ', len(points_to_remove))\n",
    "    remain_points = np.setdiff1d(np.array(remain_points), points_to_remove)\n",
    "    \n",
    "    predict_proba = clf_predict_proba_X\n",
    "   \n",
    "    print('Number of points with confidence > 0.99', sum(predict_proba > 0.999))\n",
    "    predict_outlier_indexes = np.where(predict_proba > 0.999)[0]\n",
    "    label_of_point[np.setdiff1d(predict_outlier_indexes, remain_points)] = 1\n",
    "    remain_points = np.union1d(remain_points, predict_outlier_indexes)\n",
    "    \n",
    "    print('Number of points with confidence < 0.01', sum(predict_proba < 0.001))\n",
    "    predict_inlier_indexes = np.where(predict_proba < 0.001)[0]\n",
    "    label_of_point[np.setdiff1d(predict_inlier_indexes, remain_points)] = 0\n",
    "    remain_points = np.union1d(remain_points, predict_inlier_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n",
      "Iteration = 0, L shape = (5473, 156)\n",
      "num of inliers = 3788\n",
      "num of outliers = 55\n",
      "num of outliers = 55\n",
      "Training data shape:  (3843, 26)\n",
      "Training data F-1 0.48\n",
      "Training data accuracy: 0.9797033567525371\n",
      "Training data outlier precision: 0.6545454545454545\n",
      "[[3729   19]\n",
      " [  59   36]]\n",
      "(3843, 10)\n",
      "(3843,)\n",
      "F-1 score from LR: 0.38451612903225807\n",
      "AUROC from LR: 0.8899748480707163\n",
      "Number of outliers by LR:  215\n",
      "F-1 score from SVM: 0.48945147679324896\n",
      "AUROC from SVM: 0.9172523334593353\n",
      "Number of outliers by SVM: 625\n",
      "[[1.         0.62135705]\n",
      " [0.62135705 1.        ]]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 21 22 23 24\n",
      " 25]\n",
      "[[ 0 10]\n",
      " [10 20]\n",
      " [20 24]\n",
      " [24 25]]\n",
      "##################################################################\n",
      "Iteration = 1, L shape = (5473, 150)\n",
      "num of inliers = 3787\n",
      "num of outliers = 57\n",
      "num of outliers = 57\n",
      "Training data shape:  (3844, 25)\n",
      "Training data F-1 0.48684210526315785\n",
      "Training data accuracy: 0.9797086368366286\n",
      "Training data outlier precision: 0.6491228070175439\n",
      "[[3729   20]\n",
      " [  58   37]]\n",
      "(3844, 10)\n",
      "(3844,)\n",
      "F-1 score from LR: 0.3850129198966408\n",
      "AUROC from LR: 0.8885853130179409\n",
      "Number of outliers by LR:  214\n",
      "F-1 score from SVM: 0.49792531120331945\n",
      "AUROC from SVM: 0.917520208775552\n",
      "Number of outliers by SVM: 645\n",
      "[[1.         0.60403108]\n",
      " [0.60403108 1.        ]]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 21 23 24 25]\n",
      "[[ 0 10]\n",
      " [10 20]\n",
      " [20 23]\n",
      " [23 24]]\n",
      "##################################################################\n",
      "Iteration = 2, L shape = (5473, 144)\n",
      "num of inliers = 3787\n",
      "num of outliers = 57\n",
      "num of outliers = 57\n",
      "Training data shape:  (3844, 24)\n",
      "Training data F-1 0.48684210526315785\n",
      "Training data accuracy: 0.9797086368366286\n",
      "Training data outlier precision: 0.6491228070175439\n",
      "[[3729   20]\n",
      " [  58   37]]\n",
      "(3844, 10)\n",
      "(3844,)\n",
      "F-1 score from LR: 0.3850129198966408\n",
      "AUROC from LR: 0.8870294917274869\n",
      "Number of outliers by LR:  214\n",
      "F-1 score from SVM: 0.49792531120331945\n",
      "AUROC from SVM: 0.917520208775552\n",
      "Number of outliers by SVM: 645\n",
      "[[1.        0.6027313]\n",
      " [0.6027313 1.       ]]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 23 24 25]\n",
      "[[ 0 10]\n",
      " [10 20]\n",
      " [20 22]\n",
      " [22 23]]\n",
      "##################################################################\n",
      "Iteration = 3, L shape = (5473, 138)\n",
      "num of inliers = 3787\n",
      "num of outliers = 57\n",
      "num of outliers = 57\n",
      "Training data shape:  (3844, 23)\n",
      "Training data F-1 0.48684210526315785\n",
      "Training data accuracy: 0.9797086368366286\n",
      "Training data outlier precision: 0.6491228070175439\n",
      "[[3729   20]\n",
      " [  58   37]]\n",
      "(3844, 10)\n",
      "(3844,)\n",
      "F-1 score from LR: 0.3850129198966408\n",
      "AUROC from LR: 0.8849826262685004\n",
      "Number of outliers by LR:  214\n",
      "F-1 score from SVM: 0.49792531120331945\n",
      "AUROC from SVM: 0.917520208775552\n",
      "Number of outliers by SVM: 645\n",
      "[[1.       0.601347]\n",
      " [0.601347 1.      ]]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 23 24 25]\n",
      "[[ 0 10]\n",
      " [10 19]\n",
      " [19 21]\n",
      " [21 22]]\n",
      "##################################################################\n",
      "Iteration = 4, L shape = (5473, 132)\n",
      "num of inliers = 3790\n",
      "num of outliers = 58\n",
      "num of outliers = 58\n",
      "Training data shape:  (3848, 22)\n",
      "Training data F-1 0.4840764331210191\n",
      "Training data accuracy: 0.978950103950104\n",
      "Training data outlier precision: 0.6551724137931034\n",
      "[[3729   20]\n",
      " [  61   38]]\n",
      "(3848, 10)\n",
      "(3848,)\n",
      "F-1 score from LR: 0.3896774193548387\n",
      "AUROC from LR: 0.8850177008519671\n",
      "Number of outliers by LR:  215\n",
      "F-1 score from SVM: 0.49413735343383586\n",
      "AUROC from SVM: 0.9168770535896019\n",
      "Number of outliers by SVM: 634\n",
      "[[1.         0.60703647]\n",
      " [0.60703647 1.        ]]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 23 25]\n",
      "[[ 0 10]\n",
      " [10 19]\n",
      " [19 20]\n",
      " [20 21]]\n",
      "##################################################################\n",
      "Iteration = 5, L shape = (5473, 126)\n",
      "num of inliers = 3790\n",
      "num of outliers = 58\n",
      "num of outliers = 58\n",
      "Training data shape:  (3848, 21)\n",
      "Training data F-1 0.4840764331210191\n",
      "Training data accuracy: 0.978950103950104\n",
      "Training data outlier precision: 0.6551724137931034\n",
      "[[3729   20]\n",
      " [  61   38]]\n",
      "(3848, 10)\n",
      "(3848,)\n",
      "F-1 score from LR: 0.39180537772087065\n",
      "AUROC from LR: 0.883011543717833\n",
      "Number of outliers by LR:  221\n",
      "F-1 score from SVM: 0.49413735343383586\n",
      "AUROC from SVM: 0.9168770535896019\n",
      "Number of outliers by SVM: 634\n",
      "[[1.        0.6052713]\n",
      " [0.6052713 1.       ]]\n",
      "[ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 23 25]\n",
      "[[ 0  9]\n",
      " [ 9 18]\n",
      " [18 19]\n",
      " [19 20]]\n",
      "##################################################################\n",
      "Iteration = 6, L shape = (5473, 120)\n",
      "num of inliers = 3793\n",
      "num of outliers = 58\n",
      "num of outliers = 58\n",
      "Training data shape:  (3851, 20)\n",
      "Training data F-1 0.4840764331210191\n",
      "Training data accuracy: 0.9789665022072189\n",
      "Training data outlier precision: 0.6551724137931034\n",
      "[[3732   20]\n",
      " [  61   38]]\n",
      "(3851, 10)\n",
      "(3851,)\n",
      "F-1 score from LR: 0.3767741935483871\n",
      "AUROC from LR: 0.8822798842720478\n",
      "Number of outliers by LR:  215\n",
      "F-1 score from SVM: 0.49792531120331945\n",
      "AUROC from SVM: 0.9168746910528918\n",
      "Number of outliers by SVM: 645\n",
      "[[1.         0.59553339]\n",
      " [0.59553339 1.        ]]\n",
      "[ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 23 25]\n",
      "[[ 0  9]\n",
      " [ 9 15]\n",
      " [15 16]\n",
      " [16 17]]\n",
      "##################################################################\n",
      "Iteration = 7, L shape = (5473, 102)\n",
      "num of inliers = 3801\n",
      "num of outliers = 61\n",
      "num of outliers = 61\n",
      "Training data shape:  (3862, 17)\n",
      "Training data F-1 0.4727272727272727\n",
      "Training data accuracy: 0.9774728120145003\n",
      "Training data outlier precision: 0.639344262295082\n",
      "[[3736   22]\n",
      " [  65   39]]\n",
      "(3862, 10)\n",
      "(3862,)\n",
      "F-1 score from LR: 0.38193548387096776\n",
      "AUROC from LR: 0.8852995696548516\n",
      "Number of outliers by LR:  215\n",
      "F-1 score from SVM: 0.4970906068162926\n",
      "AUROC from SVM: 0.9158464423831816\n",
      "Number of outliers by SVM: 643\n",
      "[[1.         0.59712275]\n",
      " [0.59712275 1.        ]]\n",
      "[ 0  1  3  4  5  6  7  8  9 10 11 12 13 23 25]\n",
      "[[ 0  9]\n",
      " [ 9 13]\n",
      " [13 14]\n",
      " [14 15]]\n",
      "##################################################################\n",
      "Iteration = 8, L shape = (5473, 90)\n",
      "num of inliers = 3802\n",
      "num of outliers = 63\n",
      "num of outliers = 63\n",
      "Training data shape:  (3865, 15)\n",
      "Training data F-1 0.46706586826347307\n",
      "Training data accuracy: 0.9769728331177232\n",
      "Training data outlier precision: 0.6190476190476191\n",
      "[[3737   24]\n",
      " [  65   39]]\n",
      "(3865, 10)\n",
      "(3865,)\n",
      "F-1 score from LR: 0.39748427672955977\n",
      "AUROC from LR: 0.889536143176994\n",
      "Number of outliers by LR:  235\n",
      "F-1 score from SVM: 0.49792531120331945\n",
      "AUROC from SVM: 0.9153170524265071\n",
      "Number of outliers by SVM: 645\n",
      "[[1.         0.60155432]\n",
      " [0.60155432 1.        ]]\n",
      "[ 0  1  3  4  5  6  7  8  9 10 11 12 13 25]\n",
      "[[ 0  9]\n",
      " [ 9 13]\n",
      " [13 13]\n",
      " [13 14]]\n",
      "##################################################################\n",
      "Iteration = 9, L shape = (5473, 84)\n",
      "num of inliers = 3802\n",
      "num of outliers = 73\n",
      "num of outliers = 73\n",
      "Training data shape:  (3875, 14)\n",
      "Training data F-1 0.47513812154696133\n",
      "Training data accuracy: 0.9754838709677419\n",
      "Training data outlier precision: 0.589041095890411\n",
      "[[3737   30]\n",
      " [  65   43]]\n",
      "(3875, 10)\n",
      "(3875,)\n",
      "F-1 score from LR: 0.4127764127764128\n",
      "AUROC from LR: 0.8893567721206129\n",
      "Number of outliers by LR:  254\n",
      "F-1 score from SVM: 0.4962531223980017\n",
      "AUROC from SVM: 0.9152107382745486\n",
      "Number of outliers by SVM: 641\n",
      "[[1.         0.61789705]\n",
      " [0.61789705 1.        ]]\n",
      "[ 0  1  3  4  5  6  7  8  9 10 11 25]\n",
      "[[ 0  9]\n",
      " [ 9 11]\n",
      " [11 11]\n",
      " [11 12]]\n",
      "##################################################################\n",
      "Iteration = 10, L shape = (5473, 72)\n",
      "num of inliers = 3805\n",
      "num of outliers = 77\n",
      "num of outliers = 77\n",
      "Training data shape:  (3882, 12)\n",
      "Training data F-1 0.48936170212765956\n",
      "Training data accuracy: 0.9752704791344667\n",
      "Training data outlier precision: 0.5974025974025974\n",
      "[[3740   31]\n",
      " [  65   46]]\n",
      "(3882, 10)\n",
      "(3882,)\n",
      "F-1 score from LR: 0.4182041820418204\n",
      "AUROC from LR: 0.8924342487860196\n",
      "Number of outliers by LR:  253\n",
      "F-1 score from SVM: 0.49710024855012425\n",
      "AUROC from SVM: 0.9152521735337734\n",
      "Number of outliers by SVM: 647\n",
      "[[1.         0.60867361]\n",
      " [0.60867361 1.        ]]\n",
      "[ 1  3  4  5  6  7  8  9 10 11 25]\n",
      "[[ 0  8]\n",
      " [ 8 10]\n",
      " [10 10]\n",
      " [10 11]]\n",
      "##################################################################\n",
      "Iteration = 11, L shape = (5473, 66)\n",
      "num of inliers = 3949\n",
      "num of outliers = 98\n",
      "num of outliers = 98\n",
      "Training data shape:  (4047, 11)\n",
      "Training data F-1 0.5281385281385281\n",
      "Training data accuracy: 0.9730664689893749\n",
      "Training data outlier precision: 0.6224489795918368\n",
      "[[3877   37]\n",
      " [  72   61]]\n",
      "(4047, 10)\n",
      "(4047,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-1 score from LR: 0.42372881355932207\n",
      "AUROC from LR: 0.8943600796720071\n",
      "Number of outliers by LR:  266\n",
      "F-1 score from SVM: 0.49581239530988275\n",
      "AUROC from SVM: 0.913093905382222\n",
      "Number of outliers by SVM: 634\n",
      "[[1.        0.6317891]\n",
      " [0.6317891 1.       ]]\n",
      "[ 1  3  4  5  6  7  8  9 10 25]\n",
      "[[ 0  8]\n",
      " [ 8  9]\n",
      " [ 9  9]\n",
      " [ 9 10]]\n",
      "##################################################################\n",
      "Iteration = 12, L shape = (5473, 60)\n",
      "num of inliers = 3958\n",
      "num of outliers = 108\n",
      "num of outliers = 108\n",
      "Training data shape:  (4066, 10)\n",
      "Training data F-1 0.5224489795918367\n",
      "Training data accuracy: 0.971224790949336\n",
      "Training data outlier precision: 0.5925925925925926\n",
      "[[3885   44]\n",
      " [  73   64]]\n",
      "(4066, 10)\n",
      "(4066,)\n",
      "F-1 score from LR: 0.4359281437125749\n",
      "AUROC from LR: 0.8961843214794568\n",
      "Number of outliers by LR:  275\n",
      "F-1 score from SVM: 0.5008403361344538\n",
      "AUROC from SVM: 0.9118677488296356\n",
      "Number of outliers by SVM: 630\n",
      "[[1.       0.630547]\n",
      " [0.630547 1.      ]]\n",
      "[ 3  4  5  6  7  8  9 25]\n",
      "[[0 7]\n",
      " [7 7]\n",
      " [7 7]\n",
      " [7 8]]\n",
      "##################################################################\n",
      "Iteration = 13, L shape = (5473, 48)\n",
      "num of inliers = 4167\n",
      "num of outliers = 130\n",
      "num of outliers = 130\n",
      "Training data shape:  (4297, 8)\n",
      "Training data F-1 0.5\n",
      "Training data accuracy: 0.9646264835932046\n",
      "Training data outlier precision: 0.5846153846153846\n",
      "[[4069   54]\n",
      " [  98   76]]\n",
      "(4297, 10)\n",
      "(4297,)\n",
      "F-1 score from LR: 0.45948945615982245\n",
      "AUROC from LR: 0.9011209328021866\n",
      "Number of outliers by LR:  341\n",
      "F-1 score from SVM: 0.5131690739167374\n",
      "AUROC from SVM: 0.9114670262568696\n",
      "Number of outliers by SVM: 617\n",
      "[[1.        0.6811657]\n",
      " [0.6811657 1.       ]]\n",
      "[ 4  5  6  7  8  9 25]\n",
      "[[0 6]\n",
      " [6 6]\n",
      " [6 6]\n",
      " [6 7]]\n",
      "##################################################################\n",
      "Iteration = 14, L shape = (5473, 42)\n",
      "num of inliers = 4219\n",
      "num of outliers = 156\n",
      "num of outliers = 156\n",
      "Training data shape:  (4375, 7)\n",
      "Training data F-1 0.5561797752808988\n",
      "Training data accuracy: 0.9638857142857142\n",
      "Training data outlier precision: 0.6346153846153846\n",
      "[[4118   57]\n",
      " [ 101   99]]\n",
      "(4375, 10)\n",
      "(4375,)\n",
      "F-1 score from LR: 0.45810055865921784\n",
      "AUROC from LR: 0.8999233084237155\n",
      "Number of outliers by LR:  335\n",
      "F-1 score from SVM: 0.5307557117750439\n",
      "AUROC from SVM: 0.9138966226629059\n",
      "Number of outliers by SVM: 578\n",
      "[[1.         0.71299123]\n",
      " [0.71299123 1.        ]]\n",
      "[ 4  5  6  7  8 25]\n",
      "[[0 5]\n",
      " [5 5]\n",
      " [5 5]\n",
      " [5 6]]\n",
      "##################################################################\n",
      "Iteration = 15, L shape = (5473, 36)\n",
      "num of inliers = 4237\n",
      "num of outliers = 165\n",
      "num of outliers = 165\n",
      "Training data shape:  (4402, 6)\n",
      "Training data F-1 0.5744680851063829\n",
      "Training data accuracy: 0.963652885052249\n",
      "Training data outlier precision: 0.6545454545454545\n",
      "[[4134   57]\n",
      " [ 103  108]]\n",
      "(4402, 10)\n",
      "(4402,)\n",
      "F-1 score from LR: 0.46718576195773076\n",
      "AUROC from LR: 0.9026929283824255\n",
      "Number of outliers by LR:  339\n",
      "F-1 score from SVM: 0.5268630849220104\n",
      "AUROC from SVM: 0.9177842676863133\n",
      "Number of outliers by SVM: 594\n",
      "[[1.         0.70410616]\n",
      " [0.70410616 1.        ]]\n",
      "[ 4  5  6  7 25]\n",
      "[[0 4]\n",
      " [4 4]\n",
      " [4 4]\n",
      " [4 5]]\n",
      "##################################################################\n",
      "Iteration = 16, L shape = (5473, 30)\n",
      "num of inliers = 4252\n",
      "num of outliers = 202\n",
      "num of outliers = 202\n",
      "Training data shape:  (4454, 5)\n",
      "Training data F-1 0.6339285714285714\n",
      "Training data accuracy: 0.9631791647956892\n",
      "Training data outlier precision: 0.7029702970297029\n",
      "[[4148   60]\n",
      " [ 104  142]]\n",
      "(4454, 10)\n",
      "(4454,)\n",
      "F-1 score from LR: 0.47577092511013214\n",
      "AUROC from LR: 0.9036750167194907\n",
      "Number of outliers by LR:  348\n",
      "F-1 score from SVM: 0.5267702936096719\n",
      "AUROC from SVM: 0.9189010206158588\n",
      "Number of outliers by SVM: 598\n",
      "[[1.         0.70428806]\n",
      " [0.70428806 1.        ]]\n",
      "[ 4  5  6 25]\n",
      "[[0 3]\n",
      " [3 3]\n",
      " [3 3]\n",
      " [3 4]]\n",
      "##################################################################\n",
      "Iteration = 17, L shape = (5473, 24)\n",
      "num of inliers = 4266\n",
      "num of outliers = 214\n",
      "num of outliers = 214\n",
      "Training data shape:  (4480, 4)\n",
      "Training data F-1 0.649789029535865\n",
      "Training data accuracy: 0.9629464285714285\n",
      "Training data outlier precision: 0.719626168224299\n",
      "[[4160   60]\n",
      " [ 106  154]]\n",
      "(4480, 10)\n",
      "(4480,)\n",
      "F-1 score from LR: 0.4718853362734289\n",
      "AUROC from LR: 0.906325419441133\n",
      "Number of outliers by LR:  347\n",
      "F-1 score from SVM: 0.5237683664649956\n",
      "AUROC from SVM: 0.9192842967636882\n",
      "Number of outliers by SVM: 597\n",
      "[[1.         0.70947187]\n",
      " [0.70947187 1.        ]]\n",
      "[ 4  5 25]\n",
      "[[0 2]\n",
      " [2 2]\n",
      " [2 2]\n",
      " [2 3]]\n",
      "##################################################################\n",
      "Iteration = 18, L shape = (5473, 18)\n",
      "num of inliers = 4284\n",
      "num of outliers = 224\n",
      "num of outliers = 224\n",
      "Training data shape:  (4508, 3)\n",
      "Training data F-1 0.6544715447154471\n",
      "Training data accuracy: 0.9622892635314996\n",
      "Training data outlier precision: 0.71875\n",
      "[[4177   63]\n",
      " [ 107  161]]\n",
      "(4508, 10)\n",
      "(4508,)\n",
      "F-1 score from LR: 0.4707182320441989\n",
      "AUROC from LR: 0.9089000392544562\n",
      "Number of outliers by LR:  345\n",
      "F-1 score from SVM: 0.5258620689655171\n",
      "AUROC from SVM: 0.9184415980925242\n",
      "Number of outliers by SVM: 600\n",
      "[[1.         0.71882749]\n",
      " [0.71882749 1.        ]]\n",
      "[ 5 25]\n",
      "[[0 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 2]]\n",
      "##################################################################\n",
      "Iteration = 19, L shape = (5473, 12)\n",
      "num of inliers = 4307\n",
      "num of outliers = 226\n",
      "num of outliers = 226\n",
      "Training data shape:  (4533, 2)\n",
      "Training data F-1 0.6519114688128773\n",
      "Training data accuracy: 0.9618354290756673\n",
      "Training data outlier precision: 0.7168141592920354\n",
      "[[4198   64]\n",
      " [ 109  162]]\n",
      "(4533, 10)\n",
      "(4533,)\n",
      "F-1 score from LR: 0.4720087815587267\n",
      "AUROC from LR: 0.9158675234799802\n",
      "Number of outliers by LR:  351\n",
      "F-1 score from SVM: 0.5242214532871972\n",
      "AUROC from SVM: 0.9193522651275043\n",
      "Number of outliers by SVM: 596\n",
      "[[1.         0.74277284]\n",
      " [0.74277284 1.        ]]\n",
      "[5]\n",
      "[[0 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]]\n",
      "##################################################################\n",
      "Iteration = 20, L shape = (5473, 6)\n",
      "num of inliers = 4370\n",
      "num of outliers = 327\n",
      "num of outliers = 288\n",
      "Training data shape:  (4658, 1)\n",
      "Training data F-1 0.6414473684210525\n",
      "Training data accuracy: 0.9531987977672821\n",
      "Training data outlier precision: 0.6770833333333334\n",
      "[[4245   93]\n",
      " [ 125  195]]\n",
      "(4658, 10)\n",
      "(4658,)\n",
      "F-1 score from LR: 0.5\n",
      "AUROC from LR: 0.8182805821290454\n",
      "Number of outliers by LR:  520\n",
      "F-1 score from SVM: 0.5664280031821798\n",
      "AUROC from SVM: 0.9075217716844522\n",
      "Number of outliers by SVM: 697\n",
      "[[1.         0.70849454]\n",
      " [0.70849454 1.        ]]\n",
      "##################################################################\n",
      "Iteration = 21, L shape = (5473, 6)\n",
      "num of inliers = 4228\n",
      "num of outliers = 339\n",
      "num of outliers = 315\n",
      "Training data shape:  (4543, 1)\n",
      "Training data F-1 0.6816\n",
      "Training data accuracy: 0.9561963460268545\n",
      "Training data outlier precision: 0.6761904761904762\n",
      "[[4131  102]\n",
      " [  97  213]]\n",
      "(4543, 10)\n",
      "(4543,)\n",
      "F-1 score from LR: 0.49085659287776706\n",
      "AUROC from LR: 0.8182829446657555\n",
      "Number of outliers by LR:  479\n",
      "F-1 score from SVM: 0.5751004016064258\n",
      "AUROC from SVM: 0.9090516050710942\n",
      "Number of outliers by SVM: 685\n",
      "[[1.         0.71733511]\n",
      " [0.71733511 1.        ]]\n",
      "##################################################################\n",
      "Iteration = 22, L shape = (5473, 6)\n",
      "num of inliers = 4330\n",
      "num of outliers = 307\n",
      "num of outliers = 286\n",
      "Training data shape:  (4616, 1)\n",
      "Training data F-1 0.6506849315068493\n",
      "Training data accuracy: 0.9558058925476604\n",
      "Training data outlier precision: 0.6643356643356644\n",
      "[[4222   96]\n",
      " [ 108  190]]\n",
      "(4616, 10)\n",
      "(4616,)\n",
      "F-1 score from LR: 0.5030800821355237\n",
      "AUROC from LR: 0.8182982102875752\n",
      "Number of outliers by LR:  414\n",
      "F-1 score from SVM: 0.5965463108320251\n",
      "AUROC from SVM: 0.9037984138291996\n",
      "Number of outliers by SVM: 714\n",
      "[[1.         0.69406158]\n",
      " [0.69406158 1.        ]]\n",
      "##################################################################\n",
      "Iteration = 23, L shape = (5473, 6)\n",
      "num of inliers = 4343\n",
      "num of outliers = 300\n",
      "num of outliers = 280\n",
      "Training data shape:  (4623, 1)\n",
      "Training data F-1 0.6527777777777778\n",
      "Training data accuracy: 0.9567380488860048\n",
      "Training data outlier precision: 0.6714285714285714\n",
      "[[4235   92]\n",
      " [ 108  188]]\n",
      "(4623, 10)\n",
      "(4623,)\n",
      "F-1 score from LR: 0.5015608740894901\n",
      "AUROC from LR: 0.8182973016196098\n",
      "Number of outliers by LR:  401\n",
      "F-1 score from SVM: 0.6066350710900474\n",
      "AUROC from SVM: 0.9065813003402052\n",
      "Number of outliers by SVM: 706\n",
      "[[1.         0.69622627]\n",
      " [0.69622627 1.        ]]\n",
      "##################################################################\n",
      "Iteration = 24, L shape = (5473, 6)\n",
      "num of inliers = 4352\n",
      "num of outliers = 300\n",
      "num of outliers = 281\n",
      "Training data shape:  (4633, 1)\n",
      "Training data F-1 0.6551126516464472\n",
      "Training data accuracy: 0.9570472695877401\n",
      "Training data outlier precision: 0.6725978647686833\n",
      "[[4245   92]\n",
      " [ 107  189]]\n",
      "(4633, 10)\n",
      "(4633,)\n",
      "F-1 score from LR: 0.5015608740894901\n",
      "AUROC from LR: 0.8182973016196098\n",
      "Number of outliers by LR:  401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-1 score from SVM: 0.6141607000795545\n",
      "AUROC from SVM: 0.912348434183362\n",
      "Number of outliers by SVM: 697\n",
      "[[1.         0.70308214]\n",
      " [0.70308214 1.        ]]\n",
      "##################################################################\n",
      "Iteration = 25, L shape = (5473, 6)\n",
      "num of inliers = 4363\n",
      "num of outliers = 300\n",
      "num of outliers = 281\n",
      "Training data shape:  (4644, 1)\n",
      "Training data F-1 0.65625\n",
      "Training data accuracy: 0.9573643410852714\n",
      "Training data outlier precision: 0.6725978647686833\n",
      "[[4257   92]\n",
      " [ 106  189]]\n",
      "(4644, 10)\n",
      "(4644,)\n",
      "F-1 score from LR: 0.5015608740894901\n",
      "AUROC from LR: 0.8182976650867959\n",
      "Number of outliers by LR:  401\n",
      "F-1 score from SVM: 0.6195826645264847\n",
      "AUROC from SVM: 0.9122154051932192\n",
      "Number of outliers by SVM: 686\n",
      "[[1.         0.71113561]\n",
      " [0.71113561 1.        ]]\n",
      "##################################################################\n",
      "Iteration = 26, L shape = (5473, 6)\n",
      "num of inliers = 4386\n",
      "num of outliers = 300\n",
      "num of outliers = 281\n",
      "Training data shape:  (4667, 1)\n",
      "Training data F-1 0.65625\n",
      "Training data accuracy: 0.9575744589672166\n",
      "Training data outlier precision: 0.6725978647686833\n",
      "[[4280   92]\n",
      " [ 106  189]]\n",
      "(4667, 10)\n",
      "(4667,)\n",
      "F-1 score from LR: 0.5015608740894901\n",
      "AUROC from LR: 0.8182976650867959\n",
      "Number of outliers by LR:  401\n",
      "F-1 score from SVM: 0.6251012145748988\n",
      "AUROC from SVM: 0.9125428891279697\n",
      "Number of outliers by SVM: 675\n",
      "[[1.         0.71779601]\n",
      " [0.71779601 1.        ]]\n",
      "##################################################################\n",
      "Iteration = 27, L shape = (5473, 6)\n",
      "num of inliers = 4395\n",
      "num of outliers = 300\n",
      "num of outliers = 281\n",
      "Training data shape:  (4676, 1)\n",
      "Training data F-1 0.65625\n",
      "Training data accuracy: 0.957656116338751\n",
      "Training data outlier precision: 0.6725978647686833\n",
      "[[4289   92]\n",
      " [ 106  189]]\n",
      "(4676, 10)\n",
      "(4676,)\n",
      "F-1 score from LR: 0.5015608740894901\n",
      "AUROC from LR: 0.8182976650867959\n",
      "Number of outliers by LR:  401\n",
      "F-1 score from SVM: 0.6266233766233766\n",
      "AUROC from SVM: 0.9126819153266843\n",
      "Number of outliers by SVM: 672\n",
      "[[1.         0.71986916]\n",
      " [0.71986916 1.        ]]\n",
      "##################################################################\n",
      "Iteration = 28, L shape = (5473, 6)\n",
      "num of inliers = 4401\n",
      "num of outliers = 300\n",
      "num of outliers = 281\n",
      "Training data shape:  (4682, 1)\n",
      "Training data F-1 0.65625\n",
      "Training data accuracy: 0.9577103801794105\n",
      "Training data outlier precision: 0.6725978647686833\n",
      "[[4295   92]\n",
      " [ 106  189]]\n",
      "(4682, 10)\n",
      "(4682,)\n",
      "F-1 score from LR: 0.5015608740894901\n",
      "AUROC from LR: 0.8182976650867959\n",
      "Number of outliers by LR:  401\n",
      "F-1 score from SVM: 0.6261151662611517\n",
      "AUROC from SVM: 0.9133795905905614\n",
      "Number of outliers by SVM: 673\n",
      "[[1.         0.71977019]\n",
      " [0.71977019 1.        ]]\n",
      "##################################################################\n",
      "Iteration = 29, L shape = (5473, 6)\n",
      "num of inliers = 4410\n",
      "num of outliers = 300\n",
      "num of outliers = 281\n",
      "Training data shape:  (4691, 1)\n",
      "Training data F-1 0.65625\n",
      "Training data accuracy: 0.957791515668301\n",
      "Training data outlier precision: 0.6725978647686833\n",
      "[[4304   92]\n",
      " [ 106  189]]\n",
      "(4691, 10)\n",
      "(4691,)\n",
      "F-1 score from LR: 0.5015608740894901\n",
      "AUROC from LR: 0.8182976650867959\n",
      "Number of outliers by LR:  401\n",
      "F-1 score from SVM: 0.6370967741935485\n",
      "AUROC from SVM: 0.9137245209502486\n",
      "Number of outliers by SVM: 680\n",
      "[[1.         0.71876022]\n",
      " [0.71876022 1.        ]]\n",
      "##################################################################\n",
      "Iteration = 30, L shape = (5473, 6)\n",
      "num of inliers = 4411\n",
      "num of outliers = 300\n",
      "num of outliers = 281\n",
      "Training data shape:  (4692, 1)\n",
      "Training data F-1 0.65625\n",
      "Training data accuracy: 0.9578005115089514\n",
      "Training data outlier precision: 0.6725978647686833\n",
      "[[4305   92]\n",
      " [ 106  189]]\n",
      "(4692, 10)\n",
      "(4692,)\n",
      "F-1 score from LR: 0.5015608740894901\n",
      "AUROC from LR: 0.8182976650867959\n",
      "Number of outliers by LR:  401\n",
      "F-1 score from SVM: 0.6291768541157294\n",
      "AUROC from SVM: 0.9140229275101043\n",
      "Number of outliers by SVM: 667\n",
      "[[1.         0.72265495]\n",
      " [0.72265495 1.        ]]\n",
      "##################################################################\n",
      "Iteration = 31, L shape = (5473, 6)\n",
      "num of inliers = 4415\n",
      "num of outliers = 300\n",
      "num of outliers = 281\n",
      "Training data shape:  (4696, 1)\n",
      "Training data F-1 0.65625\n",
      "Training data accuracy: 0.9578364565587735\n",
      "Training data outlier precision: 0.6725978647686833\n",
      "[[4309   92]\n",
      " [ 106  189]]\n",
      "(4696, 10)\n",
      "(4696,)\n",
      "F-1 score from LR: 0.5015608740894901\n",
      "AUROC from LR: 0.8182976650867959\n",
      "Number of outliers by LR:  401\n",
      "F-1 score from SVM: 0.6227347611202635\n",
      "AUROC from SVM: 0.912520172428833\n",
      "Number of outliers by SVM: 654\n",
      "[[1.         0.72715558]\n",
      " [0.72715558 1.        ]]\n",
      "##################################################################\n",
      "Iteration = 32, L shape = (5473, 6)\n",
      "num of inliers = 4416\n",
      "num of outliers = 300\n",
      "num of outliers = 281\n",
      "Training data shape:  (4697, 1)\n",
      "Training data F-1 0.65625\n",
      "Training data accuracy: 0.9578454332552693\n",
      "Training data outlier precision: 0.6725978647686833\n",
      "[[4310   92]\n",
      " [ 106  189]]\n",
      "(4697, 10)\n",
      "(4697,)\n",
      "F-1 score from LR: 0.5015608740894901\n",
      "AUROC from LR: 0.8182976650867959\n",
      "Number of outliers by LR:  401\n",
      "F-1 score from SVM: 0.6244864420706655\n",
      "AUROC from SVM: 0.9125605172864992\n",
      "Number of outliers by SVM: 657\n",
      "[[1.         0.72571889]\n",
      " [0.72571889 1.        ]]\n",
      "##################################################################\n",
      "Iteration = 33, L shape = (5473, 6)\n",
      "num of inliers = 4416\n",
      "num of outliers = 300\n",
      "num of outliers = 281\n",
      "Training data shape:  (4697, 1)\n",
      "Training data F-1 0.65625\n",
      "Training data accuracy: 0.9578454332552693\n",
      "Training data outlier precision: 0.6725978647686833\n",
      "[[4310   92]\n",
      " [ 106  189]]\n",
      "(4697, 10)\n",
      "(4697,)\n",
      "F-1 score from LR: 0.5015608740894901\n",
      "AUROC from LR: 0.8182976650867959\n",
      "Number of outliers by LR:  401\n",
      "F-1 score from SVM: 0.6244864420706655\n",
      "AUROC from SVM: 0.9125605172864992\n",
      "Number of outliers by SVM: 657\n",
      "[[1.         0.72571889]\n",
      " [0.72571889 1.        ]]\n",
      "##################################################################\n",
      "Iteration = 34, L shape = (5473, 6)\n",
      "num of inliers = 4416\n",
      "num of outliers = 300\n",
      "num of outliers = 281\n",
      "Training data shape:  (4697, 1)\n",
      "Training data F-1 0.65625\n",
      "Training data accuracy: 0.9578454332552693\n",
      "Training data outlier precision: 0.6725978647686833\n",
      "[[4310   92]\n",
      " [ 106  189]]\n",
      "(4697, 10)\n",
      "(4697,)\n",
      "F-1 score from LR: 0.5015608740894901\n",
      "AUROC from LR: 0.8182976650867959\n",
      "Number of outliers by LR:  401\n",
      "F-1 score from SVM: 0.6244864420706655\n",
      "AUROC from SVM: 0.9125605172864992\n",
      "Number of outliers by SVM: 657\n",
      "[[1.         0.72571889]\n",
      " [0.72571889 1.        ]]\n",
      "##################################################################\n",
      "Iteration = 35, L shape = (5473, 6)\n",
      "num of inliers = 4416\n",
      "num of outliers = 300\n",
      "num of outliers = 281\n",
      "Training data shape:  (4697, 1)\n",
      "Training data F-1 0.65625\n",
      "Training data accuracy: 0.9578454332552693\n",
      "Training data outlier precision: 0.6725978647686833\n",
      "[[4310   92]\n",
      " [ 106  189]]\n",
      "(4697, 10)\n",
      "(4697,)\n",
      "F-1 score from LR: 0.5015608740894901\n",
      "AUROC from LR: 0.8182976650867959\n",
      "Number of outliers by LR:  401\n",
      "F-1 score from SVM: 0.6244864420706655\n",
      "AUROC from SVM: 0.9125605172864992\n",
      "Number of outliers by SVM: 657\n",
      "[[1.         0.72571889]\n",
      " [0.72571889 1.        ]]\n",
      "##################################################################\n",
      "Iteration = 36, L shape = (5473, 6)\n",
      "num of inliers = 4416\n",
      "num of outliers = 300\n",
      "num of outliers = 281\n",
      "Training data shape:  (4697, 1)\n",
      "Training data F-1 0.65625\n",
      "Training data accuracy: 0.9578454332552693\n",
      "Training data outlier precision: 0.6725978647686833\n",
      "[[4310   92]\n",
      " [ 106  189]]\n",
      "(4697, 10)\n",
      "(4697,)\n",
      "F-1 score from LR: 0.5015608740894901\n",
      "AUROC from LR: 0.8182976650867959\n",
      "Number of outliers by LR:  401\n",
      "F-1 score from SVM: 0.6244864420706655\n",
      "AUROC from SVM: 0.9125605172864992\n",
      "Number of outliers by SVM: 657\n",
      "[[1.         0.72571889]\n",
      " [0.72571889 1.        ]]\n",
      "##################################################################\n",
      "Iteration = 37, L shape = (5473, 6)\n",
      "num of inliers = 4416\n",
      "num of outliers = 300\n",
      "num of outliers = 281\n",
      "Training data shape:  (4697, 1)\n",
      "Training data F-1 0.65625\n",
      "Training data accuracy: 0.9578454332552693\n",
      "Training data outlier precision: 0.6725978647686833\n",
      "[[4310   92]\n",
      " [ 106  189]]\n",
      "(4697, 10)\n",
      "(4697,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Result: Loss 0.0055\tPrec @ 1 99.979%\n",
      "Number of points to remove:  234\n",
      "Training data F-1 0.7213740458015269\n",
      "Training data accuracy: 0.9673888764797856\n",
      "Training data outlier precision: 0.6725978647686833\n",
      "[[4142   92]\n",
      " [  54  189]]\n",
      "Final Training Result: Loss 0.0011\tPrec @ 1 100.000%\n",
      "Number of points to remove:  223\n",
      "Training data F-1 0.7411764705882352\n",
      "Training data accuracy: 0.9690721649484536\n",
      "Training data outlier precision: 0.6725978647686833\n",
      "[[3947   92]\n",
      " [  40  189]]\n",
      "Final Training Result: Loss 0.0008\tPrec @ 1 100.000%\n",
      "Number of points to remove:  213\n",
      "Training data F-1 0.7514910536779325\n",
      "Training data accuracy: 0.9692799213565987\n",
      "Training data outlier precision: 0.6725978647686833\n",
      "[[3755   92]\n",
      " [  33  189]]\n",
      "F-1 score from SVM: 0.6357702349869452\n",
      "AUROC from SVM: 0.9367617981448635\n",
      "Number of outliers by SVM: 972\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "# stable version - V1: Use LR and SVM. finally use NN\n",
    "high_confidence_threshold = 0.99\n",
    "low_confidence_threshold = 0.01\n",
    "LR_threshold = 0.5\n",
    "max_iter = 100\n",
    "union_inliers = False\n",
    "remain_params_tracking = np.array(range(0,np.max(coef_index_range)))\n",
    "training_data_F1 = []\n",
    "two_prediction_corr = []\n",
    "\n",
    "min_max_diff = []\n",
    "N_size = 6\n",
    "\n",
    "last_training_data_indexes = []\n",
    "counter = 0\n",
    "\n",
    "start_training_cleaning=False\n",
    "ratio_to_remove = 0.05\n",
    "times_to_remove = 3\n",
    "\n",
    "\n",
    "for i_range in range(0, 50):\n",
    "    print(\"##################################################################\")\n",
    "    print('Iteration = {}, L shape = {}'.format(i_range, np.shape(L)))\n",
    "    num_methods = np.shape(L)[1]\n",
    "    \n",
    "    agree_outlier_indexes = np.sum(L,axis=1)==np.shape(L)[1]\n",
    "#     print('All agree, Number of outliers = {}'.format(sum(agree_outlier_indexes)))\n",
    "    agree_inlier_indexes = np.sum(L,axis=1)==0\n",
    "#     print('All agree, Number of inliers = {}'.format(sum(agree_inlier_indexes)))\n",
    "\n",
    "    disagree_indexes = np.where(np.logical_or(np.sum(L,axis = 1)==0, np.sum(L,axis = 1)==num_methods)==0)[0]\n",
    "    \n",
    "#     all_inlier_indexes = np.where(agree_inlier_indexes)[0]\n",
    "    all_inlier_indexes = np.setdiff1d(np.where(agree_inlier_indexes)[0], prediction_high_conf_outliers)\n",
    "    if len(prediction_high_conf_inliers) >0:\n",
    "        if union_inliers:\n",
    "            all_inlier_indexes = np.union1d(np.setdiff1d(np.where(agree_inlier_indexes)[0], prediction_high_conf_outliers), prediction_high_conf_inliers)\n",
    "        else:\n",
    "            all_inlier_indexes = np.intersect1d(np.setdiff1d(np.where(agree_inlier_indexes)[0], prediction_high_conf_outliers), prediction_high_conf_inliers)\n",
    "    print('num of inliers = {}'.format(np.shape(all_inlier_indexes)[0]))\n",
    "#     all_outlier_indexes = np.union1d(np.where(agree_outlier_indexes)[0], self_agree_index_list)\n",
    "\n",
    "#     if(len(np.intersect1d(np.where(agree_outlier_indexes)[0], prediction_high_conf_outliers)) > 0 and\n",
    "#       (len(np.union1d(np.where(agree_outlier_indexes)[0], prediction_high_conf_outliers)) > 2000)):\n",
    "#         all_outlier_indexes = np.intersect1d(np.where(agree_outlier_indexes)[0], prediction_high_conf_outliers)\n",
    "#     else:\n",
    "    all_outlier_indexes = np.union1d(np.where(agree_outlier_indexes)[0], prediction_high_conf_outliers)\n",
    "#     if(len(all_outlier_indexes) > 1000):\n",
    "#         all_outlier_indexes = np.random.RandomState(1).permutation(all_outlier_indexes)[:1000]\n",
    "        \n",
    "#     all_outlier_indexes = np.union1d(np.where(agree_outlier_indexes)[0], prediction_high_conf_outliers)\n",
    "    print('num of outliers = {}'.format(np.shape(all_outlier_indexes)[0]))\n",
    "    all_inlier_indexes = np.setdiff1d(all_inlier_indexes, prediction_classifier_disagree)\n",
    "    \n",
    "    self_agree_index_list = []\n",
    "    if((len(all_outlier_indexes) == 0) or (len(all_inlier_indexes)/ len(all_outlier_indexes) > 1000)):\n",
    "        for i in range(0, len(index_range)):\n",
    "            if(index_range[i,1]-index_range[i,0] <= 6):\n",
    "                continue\n",
    "            temp_index = disagree_indexes[np.where(np.sum(L[disagree_indexes][:,index_range[i,0]: index_range[i,1]], axis = 1)==(index_range[i,1]-index_range[i,0]))[0]]\n",
    "            self_agree_index_list = np.union1d(self_agree_index_list, temp_index)\n",
    "        self_agree_index_list = [int(i) for i in self_agree_index_list]\n",
    "#     self_agree_index_list = np.random.RandomState(1).permutation(self_agree_index_list)[:500]\n",
    "    all_outlier_indexes = np.union1d(all_outlier_indexes, self_agree_index_list)\n",
    "    all_outlier_indexes = np.setdiff1d(all_outlier_indexes, prediction_classifier_disagree)\n",
    "    print('num of outliers = {}'.format(np.shape(all_outlier_indexes)[0]))\n",
    "    \n",
    "    \n",
    "    from sklearn.preprocessing import RobustScaler\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    data_indexes = np.concatenate((all_inlier_indexes, all_outlier_indexes), axis = 0)\n",
    "    data_indexes = np.array([int(i) for i in data_indexes])\n",
    "    labels = np.concatenate((np.zeros(len(all_inlier_indexes)), np.ones(len(all_outlier_indexes))), axis = 0)\n",
    "    transformer = RobustScaler().fit(scores_for_training)\n",
    "    scores_transformed = transformer.transform(scores_for_training)\n",
    "    training_data = scores_transformed[data_indexes]\n",
    "    print('Training data shape: ', np.shape(training_data))\n",
    "    training_data_F1.append(metrics.f1_score(y[data_indexes], labels))\n",
    "    print('Training data F-1', metrics.f1_score(y[data_indexes], labels))\n",
    "    accurate_training = (np.array(y[data_indexes]) == np.array(labels))\n",
    "    print('Training data accuracy:', sum(accurate_training)/len(labels))\n",
    "    print('Training data outlier precision:', sum(y[data_indexes][-int(sum(labels)):])/sum(labels))\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    print(confusion_matrix(y[data_indexes], labels))\n",
    "    \n",
    "    transformer = RobustScaler().fit(X)\n",
    "    X_transformed = transformer.transform(X)\n",
    "    X_training_data = X_transformed[data_indexes]\n",
    "    print(np.shape(X_training_data))\n",
    "    print(np.shape(labels))\n",
    "    \n",
    "    if(start_training_cleaning):\n",
    "        for i in range(times_to_remove):\n",
    "            loss_list = run_NN(X_training_data,labels)\n",
    "            loss_threshold = np.sort(loss_list[:-int(sum(labels))])[::-1][int(ratio_to_remove * len(loss_list[:-int(sum(labels))]))]\n",
    "            print('Number of points to remove: ', int(ratio_to_remove * len(loss_list)))\n",
    "    #         print('Number of outliers to remove: ', sum((loss_list > loss_threshold)[-int(sum(labels)):]))\n",
    "    #         print('Number of inliers to remove: ', sum((loss_list > loss_threshold)[:-int(sum(labels))]))\n",
    "            points_left = (loss_list <= loss_threshold)\n",
    "            points_left[-int(sum(labels)):] = True\n",
    "            data_indexes = data_indexes[points_left]\n",
    "            labels = labels[points_left]\n",
    "            training_data_F1.append(metrics.f1_score(y[data_indexes], labels))\n",
    "            print('Training data F-1', metrics.f1_score(y[data_indexes], labels))\n",
    "            \n",
    "            accurate_training = (np.array(y[data_indexes]) == np.array(labels))\n",
    "            print('Training data accuracy:', sum(accurate_training)/len(labels))\n",
    "            print('Training data outlier precision:', sum(y[data_indexes][-int(sum(labels)):])/sum(labels))\n",
    "            from sklearn.metrics import confusion_matrix\n",
    "            print(confusion_matrix(y[data_indexes], labels))\n",
    "            X_training_data = X_transformed[data_indexes]\n",
    "        from sklearn.svm import SVC\n",
    "        clf_X = SVC(gamma='auto', probability=True, random_state=0)\n",
    "        clf_X.fit(X_training_data, labels)\n",
    "        clf_predictions_X = clf_X.predict(X_transformed)\n",
    "        clf_predict_proba_X = clf_X.predict_proba(X_transformed)[:,1]\n",
    "        \n",
    "        SVM_threshold = 0.5\n",
    "        print(\"F-1 score from SVM:\",metrics.f1_score(y, np.array([int(i) for i in clf_predict_proba_X > SVM_threshold])))\n",
    "        cur_f1_scores.append(metrics.f1_score(y, np.array([int(i) for i in clf_predict_proba_X > SVM_threshold])))\n",
    "        print('AUROC from SVM:', roc_auc_score(y, clf_predict_proba_X))\n",
    "        print('Number of outliers by SVM:', sum(np.array([int(i) for i in clf_predict_proba_X > SVM_threshold])))\n",
    "        break\n",
    "        \n",
    "    training_data = scores_transformed[data_indexes]\n",
    "#     from sklearn import svm\n",
    "#     from sklearn.kernel_approximation import Nystroem\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    clf = LogisticRegression(random_state=0, penalty='l2', max_iter=max_iter).fit(training_data, labels) \n",
    "    clf_predictions = clf.predict(scores_transformed)\n",
    "    clf_predict_proba = clf.predict_proba(scores_transformed)[:,1]\n",
    "#     LR_threshold = np.array(np.sort(clf_predict_proba)[::-1])[int(sum(clf_predictions_X))]\n",
    "    print(\"F-1 score from LR:\",metrics.f1_score(y, np.array([int(i) for i in clf_predict_proba > LR_threshold])))\n",
    "    print('AUROC from LR:', roc_auc_score(y, clf_predict_proba))\n",
    "    print('Number of outliers by LR: ', sum(np.array([int(i) for i in clf_predict_proba > LR_threshold])))\n",
    "    \n",
    "#     from sklearn.calibration import CalibratedClassifierCV\n",
    "#     feature_map_nystroem = Nystroem(random_state=0)\n",
    "#     X_nystroem_transformed = feature_map_nystroem.fit_transform(X_transformed)\n",
    "#     X_training_data_transformed = X_nystroem_transformed[data_indexes]\n",
    "#     clf_predict_proba_X = LogisticRegression(random_state=0, penalty='l2', max_iter=max_iter).fit(X_training_data_transformed, labels).predict_proba(X_nystroem_transformed)[:,1]\n",
    "#     clf_X = svm.LinearSVC(random_state = 0)#.fit(X_training_data_transformed, labels)\n",
    "#     clf_XX = CalibratedClassifierCV(clf_X) \n",
    "#     clf_XX.fit(X_training_data_transformed, labels)\n",
    "#     clf_predict_proba_X = clf_XX.predict_proba(X_nystroem_transformed)[:,1]\n",
    "#     from sklearn.neural_network import MLPClassifier\n",
    "#     clf_X = MLPClassifier(random_state = 0, hidden_layer_sizes = (500,100))\n",
    "#     clf_X.fit(X_training_data, labels)\n",
    "#     clf_predict_proba_X = clf_X.predict_proba(X_transformed)[:,1]\n",
    "    from sklearn.svm import SVC\n",
    "    clf_X = SVC(gamma='auto', probability=True, random_state=0)\n",
    "    clf_X.fit(X_training_data, labels)\n",
    "    clf_predictions_X = clf_X.predict(X_transformed)\n",
    "    clf_predict_proba_X = clf_X.predict_proba(X_transformed)[:,1]\n",
    "    \n",
    "    SVM_threshold = 0.5\n",
    "    print(\"F-1 score from SVM:\",metrics.f1_score(y, np.array([int(i) for i in clf_predict_proba_X > SVM_threshold])))\n",
    "    print('AUROC from SVM:', roc_auc_score(y, clf_predict_proba_X))\n",
    "    cur_f1_scores.append(metrics.f1_score(y, np.array([int(i) for i in clf_predict_proba_X > SVM_threshold])))\n",
    "    print('Number of outliers by SVM:', sum(np.array([int(i) for i in clf_predict_proba_X > SVM_threshold])))\n",
    "    \n",
    "    agreed_outlier_indexes = np.where(np.sum(L,axis=1)==np.shape(L)[1])[0]\n",
    "    agreed_inlier_indexes = np.where(np.sum(L,axis=1)==0)[0]\n",
    "        \n",
    "    prediction_result_list.append(clf_predict_proba)\n",
    "    classifier_result_list.append(clf_predict_proba_X)\n",
    "    \n",
    "    prediction_list.append(np.array([int(i) for i in clf_predictions]))\n",
    "    \n",
    "    prediction_high_conf_outliers = np.intersect1d(np.where(prediction_result_list[-1] > high_confidence_threshold)[0],\n",
    "                                                   np.where(classifier_result_list[-1] > high_confidence_threshold)[0])\n",
    "#     print('length of prediction_high_conf_outliers:' , len(prediction_high_conf_outliers))\n",
    "    prediction_high_conf_inliers = np.intersect1d(np.where(prediction_result_list[-1] < low_confidence_threshold)[0],\n",
    "                                                   np.where(classifier_result_list[-1] < low_confidence_threshold)[0])\n",
    "#     print('length of prediction high conf inliers: ', len(prediction_high_conf_inliers))\n",
    "    \n",
    "    temp_prediction = np.array([int(i) for i in prediction_result_list[-1] > LR_threshold])\n",
    "    temp_classifier = np.array([int(i) for i in classifier_result_list[-1] > SVM_threshold])\n",
    "    prediction_classifier_disagree = np.where(temp_prediction != temp_classifier)[0]\n",
    "#     print('length of prediction-classifier disagree: {}'.format(len(prediction_classifier_disagree)))\n",
    "#     print('length of prediction-classifier disagree in training: {}'.format(len(np.where(temp_prediction[data_indexes] != temp_classifier[data_indexes])[0])))\n",
    "    print(np.corrcoef(clf_predict_proba,clf_predict_proba_X))\n",
    "    two_prediction_corr.append(np.corrcoef(clf_predict_proba,clf_predict_proba_X)[0,1])\n",
    "\n",
    "    if np.max(coef_index_range) >= 2:\n",
    "        if(len(prediction_high_conf_outliers) > 0 and len(prediction_high_conf_inliers) > 0):\n",
    "            new_data_indexes = np.concatenate((prediction_high_conf_outliers, prediction_high_conf_inliers), axis = 0)\n",
    "            new_data_indexes = np.array([int(i) for i in new_data_indexes])\n",
    "            new_labels = np.concatenate((np.ones(len(prediction_high_conf_outliers)), np.zeros(len(prediction_high_conf_inliers))), axis = 0)\n",
    "            clf_prune_2 = LogisticRegression(random_state=0, penalty='l2', max_iter=max_iter).fit(scores_transformed[new_data_indexes], new_labels) \n",
    "#             print(\"F-1 score from both LR and SVM:\",metrics.f1_score(y, np.array([int(i) for i in clf_prune_2.predict_proba(scores_transformed)[:,1] > 0.5])))\n",
    "#             print('Coef from both LR and SVM: ', clf_prune_2.coef_[0])\n",
    "            combined_coef = clf_prune_2.coef_[0]  \n",
    "        else:\n",
    "#             print('Coef from normal training: ', clf.coef_[0])\n",
    "            combined_coef = clf.coef_[0]\n",
    "#             print('Combined Coef: ',  combined_coef)\n",
    "\n",
    "        if(np.max(coef_index_range) >= 2 or \n",
    "           ((np.max(combined_coef)/np.min(combined_coef) >= 1.1) and np.max(coef_index_range) >= 2)):\n",
    "            if(len(set(combined_coef)) > 1):\n",
    "                cur_clf_coef = combined_coef \n",
    "                cutoff = max(max(0, np.mean(combined_coef)-np.std(combined_coef)),min(combined_coef))\n",
    "#                 print(cutoff)\n",
    "\n",
    "                remain_indexes_after_cond = (cur_clf_coef > cutoff) #np.logical_and(cur_clf_coef > cutoff, abs(cur_clf_coef) > 0.01) # # \n",
    "                remain_params_tracking = remain_params_tracking[remain_indexes_after_cond]\n",
    "                print(remain_params_tracking)\n",
    "                remain_indexes_after_cond_expanded = []\n",
    "                for i in range(0, len(coef_index_range)): #\n",
    "                    s_e_range = coef_index_range[i,1]-coef_index_range[i,0]\n",
    "                    s1, e1 = coef_index_range[i,0], coef_index_range[i,1]\n",
    "                    s2, e2 = index_range[i,0], index_range[i,1]\n",
    "                    saved_indexes = np.where(cur_clf_coef[s1:e1] > cutoff)[0]\n",
    "                    for j in range(N_size):\n",
    "                        remain_indexes_after_cond_expanded.extend(np.array(saved_indexes) + j * s_e_range + s2)\n",
    "\n",
    "                new_coef_index_range_seq = []\n",
    "                for i in range(0, len(coef_index_range)): #\n",
    "                    s, e = coef_index_range[i,0], coef_index_range[i,1]\n",
    "                    new_coef_index_range_seq.append(sum((remain_indexes_after_cond)[s:e]))\n",
    "\n",
    "                coef_index_range = []\n",
    "                index_range = []\n",
    "                cur_sum = 0\n",
    "                for i in range(0, len(new_coef_index_range_seq)):\n",
    "                    coef_index_range.append([cur_sum, cur_sum + new_coef_index_range_seq[i]])\n",
    "                    index_range.append([cur_sum * 6, 6 * (cur_sum + new_coef_index_range_seq[i])])\n",
    "                    cur_sum += new_coef_index_range_seq[i]\n",
    "\n",
    "                coef_index_range = np.array(coef_index_range)\n",
    "                index_range = np.array(index_range)\n",
    "                print(coef_index_range)\n",
    "#                 print(index_range)\n",
    "\n",
    "                L=L[:,remain_indexes_after_cond_expanded]\n",
    "                scores_for_training = scores_for_training[:, remain_indexes_after_cond]\n",
    "    if((len(last_training_data_indexes) == len(data_indexes)) and \n",
    "       (sum(last_training_data_indexes == data_indexes) == len(data_indexes)) and \n",
    "       (np.max(coef_index_range) < 2)):\n",
    "        counter =  counter + 1\n",
    "    else:\n",
    "        counter = 0\n",
    "    if(counter > 3):\n",
    "        start_training_cleaning = True\n",
    "    last_training_data_indexes = data_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 120, 121, 122, 123, 124, 150]\n",
      "(5473, 26)\n",
      "Training data shape:  (3844, 26)\n",
      "Training data F-1 0.48684210526315785\n",
      "(3844, 10)\n",
      "(3844,)\n",
      "F-1 score from SVM: 0.5163934426229508\n",
      "F-1 score from LR: 0.38451612903225807\n",
      "Number of outliers by LR:  215\n",
      "length of prediction_high_conf_outliers: 0\n",
      "length of prediction high conf inliers:  4451\n",
      "[[1.         0.59827539]\n",
      " [0.59827539 1.        ]]\n",
      "Coef from normal training:  [0.12980633 0.14992153 0.12103794 0.12403997 0.12121649 0.12758978\n",
      " 0.12355261 0.1211296  0.12026376 0.12217489 0.1416179  0.1210424\n",
      " 0.10718062 0.10467596 0.0998384  0.09805611 0.09470999 0.09329764\n",
      " 0.09198757 0.09022005 0.07473925 0.07813622 0.07733447 0.08746685\n",
      " 0.08085779 0.22716684]\n",
      "Combined Coef:  [0.12980633 0.14992153 0.12103794 0.12403997 0.12121649 0.12758978\n",
      " 0.12355261 0.1211296  0.12026376 0.12217489 0.1416179  0.1210424\n",
      " 0.10718062 0.10467596 0.0998384  0.09805611 0.09470999 0.09329764\n",
      " 0.09198757 0.09022005 0.07473925 0.07813622 0.07733447 0.08746685\n",
      " 0.08085779 0.22716684]\n",
      "0.0821723481090583\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 23 25]\n",
      "[[ 0 10]\n",
      " [10 20]\n",
      " [20 21]\n",
      " [21 22]]\n",
      "[[  0  60]\n",
      " [ 60 120]\n",
      " [120 126]\n",
      " [126 132]]\n"
     ]
    }
   ],
   "source": [
    "index_range = np.array([[0, 60], [60, 120], [120, 150], [150, 156]])\n",
    "coef_index_range = np.array([[0, 10], [10, 20], [20, 25], [25, 26]])\n",
    "coef_remain_index = range(156)\n",
    "\n",
    "scores_for_training_indexes = []\n",
    "for i in range(len(index_range)):\n",
    "    start=index_range[i][0]\n",
    "    temp_range = coef_index_range[i][1]-coef_index_range[i][0]\n",
    "    scores_for_training_indexes  = scores_for_training_indexes + list(range(start, start+temp_range))\n",
    "print(scores_for_training_indexes) \n",
    "scores_for_training = scores[:, np.array(scores_for_training_indexes)]\n",
    "print(np.shape(scores_for_training))\n",
    "\n",
    "transformer = RobustScaler().fit(scores_for_training)\n",
    "scores_transformed = transformer.transform(scores_for_training)\n",
    "training_data = scores_transformed[data_indexes]\n",
    "print('Training data shape: ', np.shape(training_data))\n",
    "training_data_F1.append(metrics.f1_score(y[data_indexes], labels))\n",
    "print('Training data F-1', metrics.f1_score(y[data_indexes], labels))\n",
    "\n",
    "transformer = RobustScaler().fit(X)\n",
    "X_transformed = transformer.transform(X)\n",
    "X_training_data = X_transformed[data_indexes]\n",
    "print(np.shape(X_training_data))\n",
    "print(np.shape(labels))\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "feature_map_nystroem = Nystroem(random_state=0)\n",
    "X_nystroem_transformed = feature_map_nystroem.fit_transform(X_transformed)\n",
    "X_training_data_transformed = X_nystroem_transformed[data_indexes]\n",
    "clf_predict_proba_X = LogisticRegression(random_state=0, penalty='l2', max_iter=max_iter).fit(X_training_data_transformed, labels).predict_proba(X_nystroem_transformed)[:,1]\n",
    "clf_X = svm.LinearSVC(random_state = 0)#.fit(X_training_data_transformed, labels)\n",
    "clf_XX = CalibratedClassifierCV(clf_X) \n",
    "clf_XX.fit(X_training_data_transformed, labels)\n",
    "clf_predict_proba_X = clf_XX.predict_proba(X_nystroem_transformed)[:,1]\n",
    "\n",
    "#     clf_X = SVC(gamma='auto', probability=True, random_state=0)\n",
    "#     clf_X.fit(X_training_data, labels)\n",
    "#     clf_predictions_X = clf_X.predict(X_transformed)\n",
    "#     clf_predict_proba_X = clf_X.predict_proba(X_transformed)[:,1]\n",
    "print(\"F-1 score from SVM:\",metrics.f1_score(y, np.array([int(i) for i in clf_predict_proba_X > 0.5])))\n",
    "cur_f1_scores.append(metrics.f1_score(y, np.array([int(i) for i in clf_predict_proba_X > 0.5])))\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "clf = LogisticRegression(random_state=0, penalty='l2', max_iter=max_iter).fit(training_data, labels) \n",
    "clf_predictions = clf.predict(scores_transformed)\n",
    "clf_predict_proba = clf.predict_proba(scores_transformed)[:,1]\n",
    "# LR_threshold = np.array(np.sort(clf_predict_proba)[::-1])[int(sum(clf_predictions_X))]\n",
    "print(\"F-1 score from LR:\",metrics.f1_score(y, np.array([int(i) for i in clf_predict_proba > LR_threshold])))\n",
    "print('Number of outliers by LR: ', sum(np.array([int(i) for i in clf_predict_proba > LR_threshold])))\n",
    "\n",
    "\n",
    "agreed_outlier_indexes = np.where(np.sum(L,axis=1)==np.shape(L)[1])[0]\n",
    "agreed_inlier_indexes = np.where(np.sum(L,axis=1)==0)[0]\n",
    "\n",
    "prediction_result_list.append(clf_predict_proba)\n",
    "classifier_result_list.append(clf_predict_proba_X)\n",
    "\n",
    "prediction_list.append(np.array([int(i) for i in clf_predictions]))\n",
    "\n",
    "prediction_high_conf_outliers = np.intersect1d(np.where(prediction_result_list[-1] > high_confidence_threshold)[0],\n",
    "                                               np.where(classifier_result_list[-1] > high_confidence_threshold)[0])\n",
    "print('length of prediction_high_conf_outliers:' , len(prediction_high_conf_outliers))\n",
    "prediction_high_conf_inliers = np.intersect1d(np.where(prediction_result_list[-1] < low_confidence_threshold)[0],\n",
    "                                               np.where(classifier_result_list[-1] < low_confidence_threshold)[0])\n",
    "print('length of prediction high conf inliers: ', len(prediction_high_conf_inliers))\n",
    "\n",
    "temp_prediction = np.array([int(i) for i in prediction_result_list[-1] > LR_threshold])\n",
    "temp_classifier = np.array([int(i) for i in classifier_result_list[-1] > 0.5])\n",
    "prediction_classifier_disagree = np.where(temp_prediction != temp_classifier)[0]\n",
    "print(np.corrcoef(clf_predict_proba,clf_predict_proba_X))\n",
    "\n",
    "L = L_prev\n",
    "remain_params_tracking = np.array(range(0,np.max(coef_index_range)))\n",
    "\n",
    "if np.max(coef_index_range) >= 2:\n",
    "    if(len(prediction_high_conf_outliers) > 0 and len(prediction_high_conf_inliers) > 0):\n",
    "        new_data_indexes = np.concatenate((prediction_high_conf_outliers, prediction_high_conf_inliers), axis = 0)\n",
    "        new_data_indexes = np.array([int(i) for i in new_data_indexes])\n",
    "        new_labels = np.concatenate((np.ones(len(prediction_high_conf_outliers)), np.zeros(len(prediction_high_conf_inliers))), axis = 0)\n",
    "        clf_prune_2 = LogisticRegression(random_state=0, penalty='l2', max_iter=max_iter).fit(scores_transformed[new_data_indexes], new_labels) \n",
    "        print(\"F-1 score from both LR and SVM:\",metrics.f1_score(y, np.array([int(i) for i in clf_prune_2.predict_proba(scores_transformed)[:,1] > 0.5])))\n",
    "        print('Coef from both LR and SVM: ', clf_prune_2.coef_[0])\n",
    "        combined_coef = clf_prune_2.coef_[0]  \n",
    "    else:\n",
    "        print('Coef from normal training: ', clf.coef_[0])\n",
    "        combined_coef = clf.coef_[0]\n",
    "        print('Combined Coef: ',  combined_coef)\n",
    "\n",
    "    if(np.max(coef_index_range) > 2 or \n",
    "       ((np.max(combined_coef)/np.min(combined_coef) >= 1.1) and np.max(coef_index_range) >= 2)):\n",
    "        if(len(set(combined_coef)) > 1):\n",
    "            cur_clf_coef = combined_coef \n",
    "            cutoff = max(max(0, np.mean(combined_coef)-np.std(combined_coef)),min(combined_coef))\n",
    "            print(cutoff)\n",
    "\n",
    "            remain_indexes_after_cond = (cur_clf_coef > cutoff) #np.logical_and(cur_clf_coef > cutoff, abs(cur_clf_coef) > 0.01) # # \n",
    "            remain_params_tracking = remain_params_tracking[remain_indexes_after_cond]\n",
    "            print(remain_params_tracking)\n",
    "            remain_indexes_after_cond_expanded = []\n",
    "            for i in range(0, len(coef_index_range)): #\n",
    "                s_e_range = coef_index_range[i,1]-coef_index_range[i,0]\n",
    "                s1, e1 = coef_index_range[i,0], coef_index_range[i,1]\n",
    "                s2, e2 = index_range[i,0], index_range[i,1]\n",
    "                saved_indexes = np.where(cur_clf_coef[s1:e1] > cutoff)[0]\n",
    "                for j in range(N_size):\n",
    "                    remain_indexes_after_cond_expanded.extend(np.array(saved_indexes) + j * s_e_range + s2)\n",
    "\n",
    "            new_coef_index_range_seq = []\n",
    "            for i in range(0, len(coef_index_range)): #\n",
    "                s, e = coef_index_range[i,0], coef_index_range[i,1]\n",
    "                new_coef_index_range_seq.append(sum((remain_indexes_after_cond)[s:e]))\n",
    "\n",
    "            coef_index_range = []\n",
    "            index_range = []\n",
    "            cur_sum = 0\n",
    "            for i in range(0, len(new_coef_index_range_seq)):\n",
    "                coef_index_range.append([cur_sum, cur_sum + new_coef_index_range_seq[i]])\n",
    "                index_range.append([cur_sum * 6, 6 * (cur_sum + new_coef_index_range_seq[i])])\n",
    "                cur_sum += new_coef_index_range_seq[i]\n",
    "\n",
    "            coef_index_range = np.array(coef_index_range)\n",
    "            index_range = np.array(index_range)\n",
    "            print(coef_index_range)\n",
    "            print(index_range)\n",
    "\n",
    "            L=L[:,remain_indexes_after_cond_expanded]\n",
    "            scores_for_training = scores_for_training[:, remain_indexes_after_cond]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n",
      "Iteration = 0, L shape = (5473, 132)\n",
      "(3736, 10)\n",
      "(3736,)\n",
      "F-1 score from SVM: 0.5404624277456647\n",
      "F-1 score from LR: 0.49128630705394194\n",
      "Number of outliers by LR:  645\n",
      "[[1.         0.53562677]\n",
      " [0.53562677 1.        ]]\n",
      "[ 0  1  2  3  4  5  9 11 12 13 14 15 16 17 18 19 23 25]\n",
      "[[ 0  7]\n",
      " [ 7 16]\n",
      " [16 17]\n",
      " [17 18]]\n",
      "##################################################################\n",
      "Iteration = 1, L shape = (5473, 108)\n",
      "(3752, 10)\n",
      "(3752,)\n",
      "F-1 score from SVM: 0.5436046511627907\n",
      "F-1 score from LR: 0.4896265560165975\n",
      "Number of outliers by LR:  645\n",
      "[[1.         0.53377743]\n",
      " [0.53377743 1.        ]]\n",
      "[ 0  1  2  3  4  5  9 11 12 13 14 15 16 17 25]\n",
      "[[ 0  7]\n",
      " [ 7 14]\n",
      " [14 14]\n",
      " [14 15]]\n",
      "##################################################################\n",
      "Iteration = 2, L shape = (5473, 90)\n",
      "(3766, 10)\n",
      "(3766,)\n",
      "F-1 score from SVM: 0.5428156748911466\n",
      "F-1 score from LR: 0.4962655601659751\n",
      "Number of outliers by LR:  645\n",
      "[[1.         0.53398074]\n",
      " [0.53398074 1.        ]]\n",
      "[ 0  1  2  3  4  5  9 11 12 13 14 25]\n",
      "[[ 0  7]\n",
      " [ 7 11]\n",
      " [11 11]\n",
      " [11 12]]\n",
      "##################################################################\n",
      "Iteration = 3, L shape = (5473, 72)\n",
      "(3776, 10)\n",
      "(3776,)\n",
      "F-1 score from SVM: 0.5430847212165097\n",
      "F-1 score from LR: 0.5012448132780083\n",
      "Number of outliers by LR:  645\n",
      "[[1.         0.54842166]\n",
      " [0.54842166 1.        ]]\n",
      "[ 0  1  2  3  4  5  9 11 25]\n",
      "[[0 7]\n",
      " [7 8]\n",
      " [8 8]\n",
      " [8 9]]\n",
      "##################################################################\n",
      "Iteration = 4, L shape = (5473, 54)\n",
      "(3785, 10)\n",
      "(3785,)\n",
      "F-1 score from SVM: 0.5426917510853836\n",
      "F-1 score from LR: 0.5029045643153527\n",
      "Number of outliers by LR:  645\n",
      "[[1.         0.55087703]\n",
      " [0.55087703 1.        ]]\n",
      "[ 0  1  3  4  5  9 11 25]\n",
      "[[0 6]\n",
      " [6 7]\n",
      " [7 7]\n",
      " [7 8]]\n",
      "##################################################################\n",
      "Iteration = 5, L shape = (5473, 48)\n",
      "(3788, 10)\n",
      "(3788,)\n",
      "F-1 score from SVM: 0.5426917510853836\n",
      "F-1 score from LR: 0.5128630705394192\n",
      "Number of outliers by LR:  645\n",
      "[[1.         0.54733171]\n",
      " [0.54733171 1.        ]]\n",
      "[ 0  1  3  4  5  9 25]\n",
      "[[0 6]\n",
      " [6 6]\n",
      " [6 6]\n",
      " [6 7]]\n",
      "##################################################################\n",
      "Iteration = 6, L shape = (5473, 42)\n",
      "(3822, 10)\n",
      "(3822,)\n",
      "F-1 score from SVM: 0.541907514450867\n",
      "F-1 score from LR: 0.5095435684647304\n",
      "Number of outliers by LR:  645\n",
      "[[1.         0.54582164]\n",
      " [0.54582164 1.        ]]\n",
      "[ 0  1  4  5  9 25]\n",
      "[[0 5]\n",
      " [5 5]\n",
      " [5 5]\n",
      " [5 6]]\n",
      "##################################################################\n",
      "Iteration = 7, L shape = (5473, 36)\n",
      "(3831, 10)\n",
      "(3831,)\n",
      "F-1 score from SVM: 0.5422993492407809\n",
      "F-1 score from LR: 0.5195020746887967\n",
      "Number of outliers by LR:  645\n",
      "[[1.         0.54500211]\n",
      " [0.54500211 1.        ]]\n",
      "[ 1  5  9 25]\n",
      "[[0 3]\n",
      " [3 3]\n",
      " [3 3]\n",
      " [3 4]]\n",
      "##################################################################\n",
      "Iteration = 8, L shape = (5473, 24)\n",
      "(3993, 10)\n",
      "(3993,)\n",
      "F-1 score from SVM: 0.5466284074605452\n",
      "F-1 score from LR: 0.5510373443983403\n",
      "Number of outliers by LR:  645\n",
      "[[1.         0.58346745]\n",
      " [0.58346745 1.        ]]\n",
      "[ 5  9 25]\n",
      "[[0 2]\n",
      " [2 2]\n",
      " [2 2]\n",
      " [2 3]]\n",
      "##################################################################\n",
      "Iteration = 9, L shape = (5473, 18)\n",
      "(4176, 10)\n",
      "(4176,)\n",
      "F-1 score from SVM: 0.5464868701206529\n",
      "F-1 score from LR: 0.5261410788381743\n",
      "Number of outliers by LR:  645\n",
      "[[1.        0.6048111]\n",
      " [0.6048111 1.       ]]\n",
      "[ 5 25]\n",
      "[[0 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 2]]\n",
      "##################################################################\n",
      "Iteration = 10, L shape = (5473, 12)\n",
      "(4245, 10)\n",
      "(4245,)\n",
      "F-1 score from SVM: 0.5445473984319317\n",
      "F-1 score from LR: 0.529460580912863\n",
      "Number of outliers by LR:  645\n",
      "[[1.        0.6263748]\n",
      " [0.6263748 1.       ]]\n",
      "[25]\n",
      "[[0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 1]]\n",
      "##################################################################\n",
      "Iteration = 11, L shape = (5473, 6)\n",
      "(4512, 10)\n",
      "(4512,)\n",
      "F-1 score from SVM: 0.5407665505226481\n",
      "F-1 score from LR: 0.529460580912863\n",
      "Number of outliers by LR:  645\n",
      "[[1.         0.72968835]\n",
      " [0.72968835 1.        ]]\n",
      "##################################################################\n",
      "Iteration = 12, L shape = (5473, 6)\n",
      "(4512, 10)\n",
      "(4512,)\n",
      "F-1 score from SVM: 0.5400139178844816\n",
      "F-1 score from LR: 0.529460580912863\n",
      "Number of outliers by LR:  645\n",
      "[[1.         0.72688861]\n",
      " [0.72688861 1.        ]]\n",
      "##################################################################\n",
      "Iteration = 13, L shape = (5473, 6)\n",
      "(4502, 10)\n",
      "(4502,)\n",
      "F-1 score from SVM: 0.5415512465373961\n",
      "F-1 score from LR: 0.529460580912863\n",
      "Number of outliers by LR:  645\n",
      "[[1.         0.72655023]\n",
      " [0.72655023 1.        ]]\n",
      "##################################################################\n",
      "Iteration = 14, L shape = (5473, 6)\n",
      "(4491, 10)\n",
      "(4491,)\n",
      "F-1 score from SVM: 0.5397923875432526\n",
      "F-1 score from LR: 0.529460580912863\n",
      "Number of outliers by LR:  645\n",
      "[[1.         0.72642709]\n",
      " [0.72642709 1.        ]]\n",
      "##################################################################\n",
      "Iteration = 15, L shape = (5473, 6)\n",
      "(4483, 10)\n",
      "(4483,)\n",
      "F-1 score from SVM: 0.5401662049861495\n",
      "F-1 score from LR: 0.529460580912863\n",
      "Number of outliers by LR:  645\n",
      "[[1.        0.7266844]\n",
      " [0.7266844 1.       ]]\n",
      "##################################################################\n",
      "Iteration = 16, L shape = (5473, 6)\n",
      "(4478, 10)\n",
      "(4478,)\n",
      "F-1 score from SVM: 0.5401662049861495\n",
      "F-1 score from LR: 0.529460580912863\n",
      "Number of outliers by LR:  645\n",
      "[[1.         0.72636695]\n",
      " [0.72636695 1.        ]]\n",
      "##################################################################\n",
      "Iteration = 17, L shape = (5473, 6)\n",
      "(4475, 10)\n",
      "(4475,)\n",
      "F-1 score from SVM: 0.5401662049861495\n",
      "F-1 score from LR: 0.529460580912863\n",
      "Number of outliers by LR:  645\n",
      "[[1.         0.72641617]\n",
      " [0.72641617 1.        ]]\n",
      "##################################################################\n",
      "Iteration = 18, L shape = (5473, 6)\n",
      "(4471, 10)\n",
      "(4471,)\n",
      "F-1 score from SVM: 0.5401662049861495\n",
      "F-1 score from LR: 0.529460580912863\n",
      "Number of outliers by LR:  645\n",
      "[[1.         0.72617044]\n",
      " [0.72617044 1.        ]]\n",
      "##################################################################\n",
      "Iteration = 19, L shape = (5473, 6)\n",
      "(4471, 10)\n",
      "(4471,)\n",
      "F-1 score from SVM: 0.5397923875432526\n",
      "F-1 score from LR: 0.529460580912863\n",
      "Number of outliers by LR:  645\n",
      "[[1.         0.72599174]\n",
      " [0.72599174 1.        ]]\n",
      "##################################################################\n",
      "Iteration = 20, L shape = (5473, 6)\n",
      "(4471, 10)\n",
      "(4471,)\n",
      "F-1 score from SVM: 0.5397923875432526\n",
      "F-1 score from LR: 0.529460580912863\n",
      "Number of outliers by LR:  645\n",
      "[[1.         0.72599174]\n",
      " [0.72599174 1.        ]]\n",
      "##################################################################\n",
      "Iteration = 21, L shape = (5473, 6)\n",
      "(4471, 10)\n",
      "(4471,)\n",
      "F-1 score from SVM: 0.5397923875432526\n",
      "F-1 score from LR: 0.529460580912863\n",
      "Number of outliers by LR:  645\n",
      "[[1.         0.72599174]\n",
      " [0.72599174 1.        ]]\n",
      "##################################################################\n",
      "Iteration = 22, L shape = (5473, 6)\n",
      "(4471, 10)\n",
      "(4471,)\n",
      "F-1 score from SVM: 0.5397923875432526\n",
      "F-1 score from LR: 0.529460580912863\n",
      "Number of outliers by LR:  645\n",
      "[[1.         0.72599174]\n",
      " [0.72599174 1.        ]]\n",
      "##################################################################\n",
      "Iteration = 23, L shape = (5473, 6)\n",
      "(4471, 10)\n",
      "(4471,)\n",
      "F-1 score from SVM: 0.5397923875432526\n",
      "F-1 score from LR: 0.529460580912863\n",
      "Number of outliers by LR:  645\n",
      "[[1.         0.72599174]\n",
      " [0.72599174 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "last_training_data_indexes = []\n",
    "counter = 0\n",
    "\n",
    "for i_range in range(0, 50):\n",
    "    print(\"##################################################################\")\n",
    "    print('Iteration = {}, L shape = {}'.format(i_range, np.shape(L)))\n",
    "    num_methods = np.shape(L)[1]\n",
    "\n",
    "#     agree_outlier_indexes = (np.sum(L,axis=1)==np.shape(L)[1])\n",
    "#     print('All agree, Number of outliers = {}'.format(sum(agree_outlier_indexes)))\n",
    "#     agree_inlier_indexes = (np.sum(L,axis=1)==0)\n",
    "#     print('All agree, Number of inliers = {}'.format(sum(agree_inlier_indexes)))\n",
    "\n",
    "#     all_inlier_indexes = np.union1d(np.setdiff1d(np.where(agree_inlier_indexes)[0], prediction_high_conf_outliers), prediction_high_conf_inliers)\n",
    "#     print('num of inliers = {}'.format(np.shape(all_inlier_indexes)[0]))\n",
    "#     all_outlier_indexes = np.union1d(np.where(agree_outlier_indexes)[0], prediction_high_conf_outliers)\n",
    "#     print('num of outliers = {}'.format(np.shape(all_outlier_indexes)[0]))\n",
    "\n",
    "#     disagree_indexes = np.where(np.logical_or(np.sum(L,axis = 1)==0, np.sum(L,axis = 1)==num_methods)==0)[0]\n",
    "\n",
    "    ########################################################################\n",
    "\n",
    "    agree_outlier_indexes = np.sum(L,axis=1)==np.shape(L)[1]\n",
    "#     print('All agree, Number of outliers = {}'.format(sum(agree_outlier_indexes)))\n",
    "    agree_inlier_indexes = np.sum(L,axis=1)==0\n",
    "#     print('All agree, Number of inliers = {}'.format(sum(agree_inlier_indexes)))\n",
    "\n",
    "    disagree_indexes = np.where(np.logical_or(np.sum(L,axis = 1)==0, np.sum(L,axis = 1)==num_methods)==0)[0]\n",
    "    # print('Number of disagreed points = {}'.format(len(disagree_indexes)))\n",
    "    # print('Number of disagreed points (true outliers) = {}'.format(sum(y[disagree_indexes] == 1)))\n",
    "    # print('Number of disagreed points (true inliers) = {}'.format(sum(y[disagree_indexes] == 0)))\n",
    "\n",
    "#     all_inlier_indexes = np.where(agree_inlier_indexes)[0]\n",
    "    all_inlier_indexes = np.setdiff1d(np.where(agree_inlier_indexes)[0], prediction_high_conf_outliers)\n",
    "    if len(prediction_high_conf_inliers) >0:\n",
    "        all_inlier_indexes = np.intersect1d(np.setdiff1d(np.where(agree_inlier_indexes)[0], prediction_high_conf_outliers), prediction_high_conf_inliers)\n",
    "#     print('num of inliers = {}'.format(np.shape(all_inlier_indexes)[0]))\n",
    "#     all_outlier_indexes = np.union1d(np.where(agree_outlier_indexes)[0], self_agree_index_list)\n",
    "\n",
    "#     if(len(np.intersect1d(np.where(agree_outlier_indexes)[0], prediction_high_conf_outliers)) > 0 and\n",
    "#       (len(np.union1d(np.where(agree_outlier_indexes)[0], prediction_high_conf_outliers)) > 2000)):\n",
    "#         all_outlier_indexes = np.intersect1d(np.where(agree_outlier_indexes)[0], prediction_high_conf_outliers)\n",
    "#     else:\n",
    "    all_outlier_indexes = np.union1d(np.where(agree_outlier_indexes)[0], prediction_high_conf_outliers)\n",
    "#     if(len(all_outlier_indexes) > 1000):\n",
    "#         all_outlier_indexes = np.random.RandomState(1).permutation(all_outlier_indexes)[:1000]\n",
    "        \n",
    "#     all_outlier_indexes = np.union1d(np.where(agree_outlier_indexes)[0], prediction_high_conf_outliers)\n",
    "#     print('num of outliers = {}'.format(np.shape(all_outlier_indexes)[0]))\n",
    "    all_inlier_indexes = np.setdiff1d(all_inlier_indexes, prediction_classifier_disagree)\n",
    "    \n",
    "    self_agree_index_list = []\n",
    "    if((len(all_outlier_indexes) == 0) or (len(all_inlier_indexes)/ len(all_outlier_indexes) > 1000)):\n",
    "        for i in range(0, len(index_range)):\n",
    "            if(index_range[i,1]-index_range[i,0] <= 6):\n",
    "                continue\n",
    "            temp_index = disagree_indexes[np.where(np.sum(L[disagree_indexes][:,index_range[i,0]: index_range[i,1]], axis = 1)==(index_range[i,1]-index_range[i,0]))[0]]\n",
    "            self_agree_index_list = np.union1d(self_agree_index_list, temp_index)\n",
    "        self_agree_index_list = [int(i) for i in self_agree_index_list]\n",
    "#     self_agree_index_list = np.random.RandomState(1).permutation(self_agree_index_list)[:500]\n",
    "    all_outlier_indexes = np.union1d(all_outlier_indexes, self_agree_index_list)\n",
    "    all_outlier_indexes = np.setdiff1d(all_outlier_indexes, prediction_classifier_disagree)\n",
    "#     print('num of outliers = {}'.format(np.shape(all_outlier_indexes)[0]))\n",
    "    \n",
    "    \n",
    "    from sklearn.preprocessing import RobustScaler\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    data_indexes = np.concatenate((all_inlier_indexes, all_outlier_indexes), axis = 0)\n",
    "    data_indexes = np.array([int(i) for i in data_indexes])\n",
    "    labels = np.concatenate((np.zeros(len(all_inlier_indexes)), np.ones(len(all_outlier_indexes))), axis = 0)\n",
    "    transformer = RobustScaler().fit(scores_for_training)\n",
    "    scores_transformed = transformer.transform(scores_for_training)\n",
    "    training_data = scores_transformed[data_indexes]\n",
    "#     print('Training data shape: ', np.shape(training_data))\n",
    "    training_data_F1.append(metrics.f1_score(y[data_indexes], labels))\n",
    "#     print('Training data F-1', metrics.f1_score(y[data_indexes], labels))\n",
    "    \n",
    "    transformer = RobustScaler().fit(X)\n",
    "    X_transformed = transformer.transform(X)\n",
    "    X_training_data = X_transformed[data_indexes]\n",
    "    print(np.shape(X_training_data))\n",
    "    print(np.shape(labels))\n",
    "\n",
    "    from sklearn import svm\n",
    "    from sklearn.kernel_approximation import Nystroem\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.calibration import CalibratedClassifierCV\n",
    "    feature_map_nystroem = Nystroem(random_state=0)\n",
    "    X_nystroem_transformed = feature_map_nystroem.fit_transform(X_transformed)\n",
    "    X_training_data_transformed = X_nystroem_transformed[data_indexes]\n",
    "    clf_predict_proba_X = LogisticRegression(random_state=0, penalty='l2', max_iter=max_iter).fit(X_training_data_transformed, labels).predict_proba(X_nystroem_transformed)[:,1]\n",
    "    clf_X = svm.LinearSVC(random_state = 0)#.fit(X_training_data_transformed, labels)\n",
    "    clf_XX = CalibratedClassifierCV(clf_X) \n",
    "    clf_XX.fit(X_training_data_transformed, labels)\n",
    "    clf_predict_proba_X = clf_XX.predict_proba(X_nystroem_transformed)[:,1]\n",
    "    \n",
    "#     clf_X = SVC(gamma='auto', probability=True, random_state=0)\n",
    "#     clf_X.fit(X_training_data, labels)\n",
    "#     clf_predictions_X = clf_X.predict(X_transformed)\n",
    "#     clf_predict_proba_X = clf_X.predict_proba(X_transformed)[:,1]\n",
    "\n",
    "    print(\"F-1 score from SVM:\",metrics.f1_score(y, np.array([int(i) for i in clf_predict_proba_X > 0.5])))\n",
    "    cur_f1_scores.append(metrics.f1_score(y, np.array([int(i) for i in clf_predict_proba_X > 0.5])))\n",
    "    \n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.svm import SVC\n",
    "    clf = LogisticRegression(random_state=0, penalty='l2', max_iter=max_iter).fit(training_data, labels) \n",
    "    clf_predictions = clf.predict(scores_transformed)\n",
    "    clf_predict_proba = clf.predict_proba(scores_transformed)[:,1]\n",
    "    LR_threshold = np.array(np.sort(clf_predict_proba)[::-1])[int(sum(clf_predictions_X))]\n",
    "    print(\"F-1 score from LR:\",metrics.f1_score(y, np.array([int(i) for i in clf_predict_proba > LR_threshold])))\n",
    "    print('Number of outliers by LR: ', sum(np.array([int(i) for i in clf_predict_proba > LR_threshold])))\n",
    "    \n",
    "    \n",
    "    agreed_outlier_indexes = np.where(np.sum(L,axis=1)==np.shape(L)[1])[0]\n",
    "    agreed_inlier_indexes = np.where(np.sum(L,axis=1)==0)[0]\n",
    "        \n",
    "    prediction_result_list.append(clf_predict_proba)\n",
    "    classifier_result_list.append(clf_predict_proba_X)\n",
    "    \n",
    "    prediction_list.append(np.array([int(i) for i in clf_predictions]))\n",
    "    print(np.corrcoef(clf_predict_proba,clf_predict_proba_X))\n",
    "    \n",
    "    prediction_high_conf_outliers = np.intersect1d(np.where(prediction_result_list[-1] > high_confidence_threshold)[0],\n",
    "                                                   np.where(classifier_result_list[-1] > high_confidence_threshold)[0])\n",
    "#     print('length of prediction_high_conf_outliers:' , len(prediction_high_conf_outliers))\n",
    "    prediction_high_conf_inliers = np.intersect1d(np.where(prediction_result_list[-1] < low_confidence_threshold)[0],\n",
    "                                                   np.where(classifier_result_list[-1] < low_confidence_threshold)[0])\n",
    "#     print('length of prediction high conf inliers: ', len(prediction_high_conf_inliers))\n",
    "    \n",
    "    temp_prediction = np.array([int(i) for i in prediction_result_list[-1] > LR_threshold])\n",
    "    temp_classifier = np.array([int(i) for i in classifier_result_list[-1] > 0.5])\n",
    "    prediction_classifier_disagree = np.where(temp_prediction != temp_classifier)[0]\n",
    "    \n",
    "    if np.max(coef_index_range) >= 2:\n",
    "        if(len(prediction_high_conf_outliers) > 0 and len(prediction_high_conf_inliers) > 0):\n",
    "            new_data_indexes = np.concatenate((prediction_high_conf_outliers, prediction_high_conf_inliers), axis = 0)\n",
    "            new_data_indexes = np.array([int(i) for i in new_data_indexes])\n",
    "            new_labels = np.concatenate((np.ones(len(prediction_high_conf_outliers)), np.zeros(len(prediction_high_conf_inliers))), axis = 0)\n",
    "            clf_prune_2 = LogisticRegression(random_state=0, penalty='l2', max_iter=max_iter).fit(scores_transformed[new_data_indexes], new_labels) \n",
    "#             print(\"F-1 score from both LR and SVM:\",metrics.f1_score(y, np.array([int(i) for i in clf_prune_2.predict_proba(scores_transformed)[:,1] > 0.5])))\n",
    "#             print('Coef from both LR and SVM: ', clf_prune_2.coef_[0])\n",
    "            combined_coef = clf_prune_2.coef_[0]  \n",
    "        else:\n",
    "#             print('Coef from normal training: ', clf.coef_[0])\n",
    "            combined_coef = clf.coef_[0]\n",
    "#             print('Combined Coef: ',  combined_coef)\n",
    "\n",
    "        if(np.max(coef_index_range) >= 2 or \n",
    "           ((np.max(combined_coef)/np.min(combined_coef) >= 1.1) and np.max(coef_index_range) >= 2)):\n",
    "            if(len(set(combined_coef)) > 1):\n",
    "                cur_clf_coef = combined_coef \n",
    "                cutoff = max(max(0, np.mean(combined_coef)-np.std(combined_coef)),min(combined_coef))\n",
    "#                 print(cutoff)\n",
    "\n",
    "                remain_indexes_after_cond = (cur_clf_coef > cutoff) #np.logical_and(cur_clf_coef > cutoff, abs(cur_clf_coef) > 0.01) # # \n",
    "                remain_params_tracking = remain_params_tracking[remain_indexes_after_cond]\n",
    "                print(remain_params_tracking)\n",
    "                remain_indexes_after_cond_expanded = []\n",
    "                for i in range(0, len(coef_index_range)): #\n",
    "                    s_e_range = coef_index_range[i,1]-coef_index_range[i,0]\n",
    "                    s1, e1 = coef_index_range[i,0], coef_index_range[i,1]\n",
    "                    s2, e2 = index_range[i,0], index_range[i,1]\n",
    "                    saved_indexes = np.where(cur_clf_coef[s1:e1] > cutoff)[0]\n",
    "                    for j in range(N_size):\n",
    "                        remain_indexes_after_cond_expanded.extend(np.array(saved_indexes) + j * s_e_range + s2)\n",
    "\n",
    "                new_coef_index_range_seq = []\n",
    "                for i in range(0, len(coef_index_range)): #\n",
    "                    s, e = coef_index_range[i,0], coef_index_range[i,1]\n",
    "                    new_coef_index_range_seq.append(sum((remain_indexes_after_cond)[s:e]))\n",
    "\n",
    "                coef_index_range = []\n",
    "                index_range = []\n",
    "                cur_sum = 0\n",
    "                for i in range(0, len(new_coef_index_range_seq)):\n",
    "                    coef_index_range.append([cur_sum, cur_sum + new_coef_index_range_seq[i]])\n",
    "                    index_range.append([cur_sum * 6, 6 * (cur_sum + new_coef_index_range_seq[i])])\n",
    "                    cur_sum += new_coef_index_range_seq[i]\n",
    "\n",
    "                coef_index_range = np.array(coef_index_range)\n",
    "                index_range = np.array(index_range)\n",
    "                print(coef_index_range)\n",
    "#                 print(index_range)\n",
    "\n",
    "                L=L[:,remain_indexes_after_cond_expanded]\n",
    "                scores_for_training = scores_for_training[:, remain_indexes_after_cond]\n",
    "    if((len(last_training_data_indexes) == len(data_indexes)) and \n",
    "       (sum(last_training_data_indexes == data_indexes) == len(data_indexes)) and \n",
    "       (np.max(coef_index_range) < 2)):\n",
    "        counter =  counter + 1\n",
    "    else:\n",
    "        counter = 0\n",
    "    if(counter > 3):\n",
    "        break\n",
    "    last_training_data_indexes = data_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.48945147679324896, 0.49792531120331945, 0.49792531120331945, 0.49792531120331945, 0.49413735343383586, 0.49413735343383586, 0.49792531120331945, 0.4970906068162926, 0.49792531120331945, 0.4962531223980017, 0.49710024855012425, 0.49581239530988275, 0.5008403361344538, 0.5131690739167374, 0.5307557117750439, 0.5268630849220104, 0.5267702936096719, 0.5237683664649956, 0.5258620689655171, 0.5242214532871972, 0.5608380338436744, 0.5795918367346938, 0.5782932891466446, 0.585, 0.5906382978723403, 0.5941780821917808, 0.5958904109589042, 0.6013745704467354, 0.6012058570198104, 0.6015557476231634, 0.6073943661971831, 0.6098418277680141, 0.6156583629893239, 0.6164874551971326, 0.6223021582733813, 0.623985572587917, 0.6256781193490054, 0.6279491833030852, 0.6302367941712205, 0.6308113035551504, 0.6308113035551504, 0.6308113035551504, 0.6308113035551504, 0.6308113035551504, 0.6308113035551504]\n",
      "[0.6213570487954626, 0.6040310780823559, 0.6027312958577775, 0.601347000818649, 0.6070364728106306, 0.6052712961056745, 0.5955333920718894, 0.5971227450485538, 0.6015543237435828, 0.6178970521071834, 0.6086736099796847, 0.631789104010191, 0.6305470026964096, 0.6811656957158055, 0.7129912277163392, 0.7041061638570748, 0.7042880560095617, 0.7094718716656779, 0.7188274916394289, 0.7427728439687519, 0.6981193514741313, 0.7169416453487611, 0.7156619380793847, 0.7211626578611708, 0.7355717273288548, 0.7388572405354323, 0.7387942031358111, 0.7422402814243374, 0.743186780951859, 0.745342843683792, 0.7554116535996018, 0.7568543585491414, 0.7604722818036108, 0.7662312612307807, 0.7675765814625559, 0.7691180876499032, 0.7709489652414798, 0.7719382498364005, 0.7744853188999297, 0.7729611866939153, 0.7713032321155415, 0.7713032321155415, 0.7713032321155415, 0.7713032321155415, 0.7713032321155415]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7febe98a76d8>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(cur_f1_scores)\n",
    "print(two_prediction_corr)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(len(cur_f1_scores)), cur_f1_scores, label='SVM_F1')\n",
    "plt.plot(range(len(two_prediction_corr)), two_prediction_corr, label='Correlation')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All agree, Number of outliers = 55\n",
      "All agree, Number of inliers = 3788\n",
      "num of inliers = 3787\n",
      "num of outliers = 57\n",
      "num of outliers = 57\n",
      "Training data shape:  (3844, 26)\n",
      "Training data F-1 0.48684210526315785\n",
      "F-1 score from LR: 0.42352941176470593\n",
      "(3844, 10)\n",
      "(3844,)\n",
      "F-1 score from SVM: 0.49792531120331945\n",
      "length of prediction_high_conf_outliers: 57\n",
      "length of prediction high conf inliers:  4585\n",
      "length of prediction-classifier disagree: 397\n",
      "length of prediction-classifier disagree in training: 0\n",
      "[[1.         0.60564063]\n",
      " [0.60564063 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "num_methods = np.shape(L)[1]\n",
    "\n",
    "agree_outlier_indexes = np.sum(L,axis=1)==np.shape(L)[1]\n",
    "print('All agree, Number of outliers = {}'.format(sum(agree_outlier_indexes)))\n",
    "agree_inlier_indexes = np.sum(L,axis=1)==0\n",
    "print('All agree, Number of inliers = {}'.format(sum(agree_inlier_indexes)))\n",
    "\n",
    "disagree_indexes = np.where(np.logical_or(np.sum(L,axis = 1)==0, np.sum(L,axis = 1)==num_methods)==0)[0]\n",
    "# print('Number of disagreed points = {}'.format(len(disagree_indexes)))\n",
    "# print('Number of disagreed points (true outliers) = {}'.format(sum(y[disagree_indexes] == 1)))\n",
    "# print('Number of disagreed points (true inliers) = {}'.format(sum(y[disagree_indexes] == 0)))\n",
    "\n",
    "#     all_inlier_indexes = np.where(agree_inlier_indexes)[0]\n",
    "all_inlier_indexes = np.setdiff1d(np.where(agree_inlier_indexes)[0], prediction_high_conf_outliers)\n",
    "if len(prediction_high_conf_inliers) >0:\n",
    "    all_inlier_indexes = np.intersect1d(np.setdiff1d(np.where(agree_inlier_indexes)[0], prediction_high_conf_outliers), prediction_high_conf_inliers)\n",
    "print('num of inliers = {}'.format(np.shape(all_inlier_indexes)[0]))\n",
    "#     all_outlier_indexes = np.union1d(np.where(agree_outlier_indexes)[0], self_agree_index_list)\n",
    "\n",
    "#     if(len(np.intersect1d(np.where(agree_outlier_indexes)[0], prediction_high_conf_outliers)) > 0 and\n",
    "#       (len(np.union1d(np.where(agree_outlier_indexes)[0], prediction_high_conf_outliers)) > 2000)):\n",
    "#         all_outlier_indexes = np.intersect1d(np.where(agree_outlier_indexes)[0], prediction_high_conf_outliers)\n",
    "#     else:\n",
    "all_outlier_indexes = np.union1d(np.where(agree_outlier_indexes)[0], prediction_high_conf_outliers)\n",
    "#     if(len(all_outlier_indexes) > 1000):\n",
    "#         all_outlier_indexes = np.random.RandomState(1).permutation(all_outlier_indexes)[:1000]\n",
    "\n",
    "#     all_outlier_indexes = np.union1d(np.where(agree_outlier_indexes)[0], prediction_high_conf_outliers)\n",
    "print('num of outliers = {}'.format(np.shape(all_outlier_indexes)[0]))\n",
    "all_inlier_indexes = np.setdiff1d(all_inlier_indexes, prediction_classifier_disagree)\n",
    "\n",
    "self_agree_index_list = []\n",
    "if((len(all_outlier_indexes) == 0) or (len(all_inlier_indexes)/ len(all_outlier_indexes) > 1000)):\n",
    "    for i in range(0, len(index_range)):\n",
    "        if(index_range[i,1]-index_range[i,0] <= 6):\n",
    "            continue\n",
    "        temp_index = disagree_indexes[np.where(np.sum(L[disagree_indexes][:,index_range[i,0]: index_range[i,1]], axis = 1)==(index_range[i,1]-index_range[i,0]))[0]]\n",
    "        self_agree_index_list = np.union1d(self_agree_index_list, temp_index)\n",
    "    self_agree_index_list = [int(i) for i in self_agree_index_list]\n",
    "#     self_agree_index_list = np.random.RandomState(1).permutation(self_agree_index_list)[:500]\n",
    "all_outlier_indexes = np.union1d(all_outlier_indexes, self_agree_index_list)\n",
    "all_outlier_indexes = np.setdiff1d(all_outlier_indexes, prediction_classifier_disagree)\n",
    "print('num of outliers = {}'.format(np.shape(all_outlier_indexes)[0]))\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "data_indexes = np.concatenate((all_inlier_indexes, all_outlier_indexes), axis = 0)\n",
    "data_indexes = np.array([int(i) for i in data_indexes])\n",
    "labels = np.concatenate((np.zeros(len(all_inlier_indexes)), np.ones(len(all_outlier_indexes))), axis = 0)\n",
    "transformer = RobustScaler().fit(scores_for_training)\n",
    "scores_transformed = transformer.transform(scores_for_training)\n",
    "training_data = scores_transformed[data_indexes]\n",
    "print('Training data shape: ', np.shape(training_data))\n",
    "print('Training data F-1', metrics.f1_score(y[data_indexes], labels))\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "#     clf = SVC(gamma='auto', probability=True, random_state=0)\n",
    "#     clf.fit(training_data, labels)\n",
    "clf = LogisticRegression(random_state=0, penalty='l2', max_iter=100).fit(training_data, labels) \n",
    "clf_predictions = clf.predict(scores_transformed)\n",
    "clf_predict_proba = clf.predict_proba(scores_transformed)[:,1]\n",
    "print(\"F-1 score from LR:\",metrics.f1_score(y, np.array([int(i) for i in clf_predict_proba > 0.1])))\n",
    "\n",
    "\n",
    "transformer = RobustScaler().fit(X)\n",
    "X_transformed = transformer.transform(X)\n",
    "X_training_data = X_transformed[data_indexes]\n",
    "print(np.shape(X_training_data))\n",
    "print(np.shape(labels))\n",
    "\n",
    "clf_X = SVC(gamma='auto', probability=True, random_state=0)\n",
    "clf_X.fit(X_training_data, labels)\n",
    "clf_predictions_X = clf_X.predict(X_transformed)\n",
    "clf_predict_proba_X = clf_X.predict_proba(X_transformed)[:,1]\n",
    "print(\"F-1 score from SVM:\",metrics.f1_score(y, np.array([int(i) for i in clf_predict_proba_X > 0.5])))\n",
    "cur_f1_scores.append(metrics.f1_score(y, np.array([int(i) for i in clf_predict_proba_X > 0.5])))\n",
    "\n",
    "agreed_outlier_indexes = np.where(np.sum(L,axis=1)==np.shape(L)[1])[0]\n",
    "agreed_inlier_indexes = np.where(np.sum(L,axis=1)==0)[0]\n",
    "\n",
    "prediction_result_list.append(clf_predict_proba)\n",
    "classifier_result_list.append(clf_predict_proba_X)\n",
    "\n",
    "prediction_list.append(np.array([int(i) for i in clf_predictions]))\n",
    "\n",
    "prediction_high_conf_outliers = np.intersect1d(np.where(prediction_result_list[-1] > 0.99)[0],\n",
    "                                               np.where(classifier_result_list[-1] > 0.99)[0])\n",
    "print('length of prediction_high_conf_outliers:' , len(prediction_high_conf_outliers))\n",
    "prediction_high_conf_inliers = np.intersect1d(np.where(prediction_result_list[-1] < 0.01)[0],\n",
    "                                               np.where(classifier_result_list[-1] < 0.01)[0])\n",
    "print('length of prediction high conf inliers: ', len(prediction_high_conf_inliers))\n",
    "\n",
    "temp_prediction = np.array([int(i) for i in prediction_result_list[-1] > 0.1])\n",
    "temp_classifier = np.array([int(i) for i in classifier_result_list[-1] > 0.5])\n",
    "prediction_classifier_disagree = np.where(temp_prediction != temp_classifier)[0]\n",
    "print('length of prediction-classifier disagree: {}'.format(len(prediction_classifier_disagree)))\n",
    "print('length of prediction-classifier disagree in training: {}'.format(len(np.where(temp_prediction[data_indexes] != temp_classifier[data_indexes])[0])))\n",
    "print(np.corrcoef(clf_predict_proba,clf_predict_proba_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.060394   0.09231781 0.1129055  0.12807603 0.14594223 0.13592194\n",
      " 0.13450412 0.1398666  0.13111137 0.13618559 0.16265347 0.18082909\n",
      " 0.20060729 0.20009989 0.21899055 0.21076635 0.21494993 0.22526172\n",
      " 0.23348437 0.23114429 0.31017955 0.30621549 0.30651975 0.29919923\n",
      " 0.29861265 0.28209968]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "transformer = RobustScaler().fit(scores_for_training)\n",
    "scores_transformed = transformer.transform(scores_for_training)\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "clf = SelectKBest(mutual_info_classif, k=26).fit(scores_transformed, clf_predictions_X)\n",
    "print(clf.scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n",
      "L shape (567498, 156)\n",
      "All agree, Number of outliers = 7\n",
      "All agree, Number of inliers = 405653\n",
      "num of inliers = 405653\n",
      "num of outliers = 7\n",
      "num of outliers = 5600\n",
      "Training data shape:  (411253, 26)\n",
      "F-1 score from LR: 0.0009445606403962393\n",
      "(411253, 3)\n",
      "(411253,)\n",
      "F-1 score from SVM: 0.21107398568019095\n",
      "length of prediction_high_conf_outliers: 170\n",
      "length of prediction high conf inliers:  3894\n",
      "Coef from both LR and SVM:  [-8.06574632e-08  2.49365658e-09  1.05287318e-08  7.71208138e-08\n",
      "  4.21006595e-08 -2.21786356e-09 -1.20514981e-09  1.75776514e-09\n",
      "  3.15789174e-09  6.55909865e-09 -6.38151274e-17 -2.63715523e-17\n",
      " -4.90913279e-17  6.09186439e-17  8.92000784e-17  9.54526254e-17\n",
      "  9.69215749e-17  8.49276263e-17  7.38015761e-17  7.04956356e-16\n",
      "  6.93680770e-16  6.93680770e-16  7.15344285e-16  7.15344285e-16\n",
      "  7.15344285e-16 -8.65472102e-17]\n",
      "[0.         1.         1.         1.         1.         0.02067345\n",
      " 0.11845923 1.         1.         1.         0.50828685 0.85027029\n",
      " 0.70177783 0.94577069 1.         1.         1.         1.\n",
      " 0.99208165 1.         1.         1.         1.         1.\n",
      " 1.         0.2974353 ]\n",
      "Combined_proba [0.         0.95238095 0.95238095 0.95238095 0.95238095 0.019689\n",
      " 0.11281831 0.95238095 0.95238095 0.95238095 0.48408271 0.80978122\n",
      " 0.66835984 0.90073399 0.95238095 0.95238095 0.95238095 0.95238095\n",
      " 0.94483967 0.95238095 0.95238095 0.95238095 0.95238095 0.95238095\n",
      " 0.95238095 0.28327172]\n",
      "Remain_indexes:  [False  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True]\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25]\n",
      "[[ 0  9]\n",
      " [ 9 19]\n",
      " [19 24]\n",
      " [24 25]]\n",
      "[[  0  54]\n",
      " [ 54 114]\n",
      " [114 144]\n",
      " [144 150]]\n",
      "##################################################################\n",
      "L shape (567498, 150)\n",
      "All agree, Number of outliers = 7\n",
      "All agree, Number of inliers = 421148\n",
      "num of inliers = 2510\n",
      "num of outliers = 170\n",
      "num of outliers = 170\n",
      "Training data shape:  (2680, 25)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-1 score from LR: 0.0008850168274988532\n",
      "(2680, 3)\n",
      "(2680,)\n",
      "F-1 score from SVM: 0.2969778374748153\n",
      "length of prediction_high_conf_outliers: 171\n",
      "length of prediction high conf inliers:  24792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coef from both LR and SVM:  [-1.41283806e-06  9.07420514e-07  4.54446176e-07  5.94137406e-07\n",
      " -4.37098189e-07  1.47352261e-08  2.36097508e-07 -1.43089145e-07\n",
      "  6.72821556e-08  4.27216259e-13  7.35639956e-13  8.96802554e-13\n",
      "  8.52242339e-13  8.46466377e-13  7.40566833e-13  6.26168497e-13\n",
      "  5.62140431e-13  4.53219622e-13  4.59417795e-13 -9.46106087e-14\n",
      " -9.46106087e-14 -2.24744870e-13 -2.24744870e-13 -2.24744870e-13\n",
      "  1.11621311e-13]\n",
      "[0.         1.         1.         1.         0.02335328 1.\n",
      " 1.         0.13088751 1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         0.9632164  0.87763031 0.5409125  0.32218491 0.73498493\n",
      " 0.99791623]\n",
      "Combined_proba [0.48780488 0.97560976 0.97560976 0.97560976 0.02147645 0.54558987\n",
      " 0.97560976 0.55165244 0.97560976 0.73574968 0.90257087 0.83013553\n",
      " 0.94915644 0.97560976 0.97560976 0.97560976 0.97560976 0.97174715\n",
      " 0.97560976 0.95766654 0.91591722 0.75166464 0.64496825 0.84633411\n",
      " 0.63187879]\n",
      "Remain_indexes:  [ True  True  True  True False  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True]\n",
      "[ 1  2  3  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25]\n",
      "[[ 0  8]\n",
      " [ 8 18]\n",
      " [18 23]\n",
      " [23 24]]\n",
      "[[  0  48]\n",
      " [ 48 108]\n",
      " [108 138]\n",
      " [138 144]]\n",
      "##################################################################\n",
      "L shape (567498, 144)\n",
      "All agree, Number of outliers = 7\n",
      "All agree, Number of inliers = 422523\n",
      "num of inliers = 2503\n",
      "num of outliers = 171\n",
      "num of outliers = 171\n",
      "Training data shape:  (2674, 24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-1 score from LR: 0.0008738691105628011\n",
      "(2674, 3)\n",
      "(2674,)\n",
      "F-1 score from SVM: 0.30063226595961656\n",
      "length of prediction_high_conf_outliers: 1785\n",
      "length of prediction high conf inliers:  33006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coef from both LR and SVM:  [-4.48311181e-02  2.48362930e-02  2.70928732e-02  1.05711529e-02\n",
      " -3.15223091e-02 -1.44477778e-02  7.43777005e-03  2.16708160e-02\n",
      "  1.02697791e-06  1.94611021e-06  2.15731048e-06  2.13600531e-06\n",
      "  2.16615063e-06  2.23752288e-06  2.21048754e-06  2.25465291e-06\n",
      "  2.32332430e-06  2.33847689e-06  9.34996585e-07  9.34996585e-07\n",
      "  9.35768720e-07  9.35768720e-07  9.35768720e-07  4.48330228e-07]\n",
      "[0.         1.         1.         1.         0.02655676 0.14517961\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.76902134 0.57580744 0.97808283 0.90402888 1.         0.34964336]\n",
      "Combined_proba [0.32786885 0.98360656 0.98360656 0.98360656 0.37541508 0.70333758\n",
      " 0.69865164 0.98360656 0.82238913 0.93451485 0.8858288  0.96582646\n",
      " 0.98360656 0.98360656 0.98360656 0.98360656 0.98101038 0.98360656\n",
      " 0.89581565 0.80440582 0.82590011 0.72990616 0.89671637 0.53934259]\n",
      "Remain_indexes:  [False  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True]\n",
      "[ 2  3  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25]\n",
      "[[ 0  7]\n",
      " [ 7 17]\n",
      " [17 22]\n",
      " [22 23]]\n",
      "[[  0  42]\n",
      " [ 42 102]\n",
      " [102 132]\n",
      " [132 138]]\n",
      "##################################################################\n",
      "L shape (567498, 138)\n",
      "All agree, Number of outliers = 13\n",
      "All agree, Number of inliers = 434145\n",
      "num of inliers = 17315\n",
      "num of outliers = 1785\n",
      "num of outliers = 1785\n",
      "Training data shape:  (19100, 23)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-1 score from LR: 0.012787479721347457\n",
      "(19100, 3)\n",
      "(19100,)\n",
      "F-1 score from SVM: 0.30623268698060946\n",
      "length of prediction_high_conf_outliers: 179\n",
      "length of prediction high conf inliers:  0\n",
      "Coef from normal training:  [5.92863570e-09 3.86670629e-07 8.04747936e-07 3.27730791e-07\n",
      " 2.67458034e-07 2.06103209e-07 9.86661573e-09 1.85653852e-07\n",
      " 3.51601044e-07 4.29260551e-07 4.16009472e-07 4.09538877e-07\n",
      " 4.08654048e-07 3.89528106e-07 3.88266152e-07 3.96967484e-07\n",
      " 3.97335516e-07 1.27272777e-07 1.27272777e-07 1.34676841e-07\n",
      " 1.34676841e-07 1.34676841e-07 9.79731337e-08]\n",
      "Combined Coef:  [5.92863570e-09 3.86670629e-07 8.04747936e-07 3.27730791e-07\n",
      " 2.67458034e-07 2.06103209e-07 9.86661573e-09 1.85653852e-07\n",
      " 3.51601044e-07 4.29260551e-07 4.16009472e-07 4.09538877e-07\n",
      " 4.08654048e-07 3.89528106e-07 3.88266152e-07 3.96967484e-07\n",
      " 3.97335516e-07 1.27272777e-07 1.27272777e-07 1.34676841e-07\n",
      " 1.34676841e-07 1.34676841e-07 9.79731337e-08]\n",
      "[0.         1.         1.         1.         1.         1.\n",
      " 0.03009521 1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.61203794\n",
      " 0.37896555 0.98965877 0.80299291 0.92866271 0.16076793]\n",
      "Combined_proba [0.74074074 0.98765432 0.98765432 0.52963358 0.77658756 0.77305864\n",
      " 0.74817166 0.86624367 0.95068402 0.91401922 0.97426437 0.98765432\n",
      " 0.98765432 0.98765432 0.98765432 0.98569917 0.98765432 0.82574708\n",
      " 0.69935884 0.86633435 0.74795227 0.90460436 0.44586736]\n",
      "Remain_indexes:  [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True False]\n",
      "[ 2  3  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]\n",
      "[[ 0  7]\n",
      " [ 7 17]\n",
      " [17 22]\n",
      " [22 22]]\n",
      "[[  0  42]\n",
      " [ 42 102]\n",
      " [102 132]\n",
      " [132 132]]\n",
      "##################################################################\n",
      "L shape (567498, 132)\n",
      "All agree, Number of outliers = 13\n",
      "All agree, Number of inliers = 448984\n",
      "num of inliers = 448984\n",
      "num of outliers = 179\n",
      "num of outliers = 4395\n",
      "Training data shape:  (225489, 22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-1 score from LR: 0.016831351532406634\n",
      "(225489, 3)\n",
      "(225489,)\n",
      "F-1 score from SVM: 0.32176380702903296\n",
      "length of prediction_high_conf_outliers: 6032\n",
      "length of prediction high conf inliers:  0\n",
      "Coef from normal training:  [3.55762821e-06 4.83972658e-02 8.70159830e-02 5.06692922e-02\n",
      " 3.87039282e-02 2.43293307e-02 4.28432352e-02 8.02009184e-02\n",
      " 1.00343311e-01 9.76132870e-02 8.17539207e-02 7.82873626e-02\n",
      " 6.65477176e-02 5.22056857e-02 4.29609159e-02 4.15179503e-02\n",
      " 4.14397067e-02 5.42386362e-02 5.42386362e-02 5.49546140e-02\n",
      " 5.49546140e-02 5.49546140e-02]\n",
      "Combined Coef:  [3.55762821e-06 4.83972658e-02 8.70159830e-02 5.06692922e-02\n",
      " 3.87039282e-02 2.43293307e-02 4.28432352e-02 8.02009184e-02\n",
      " 1.00343311e-01 9.76132870e-02 8.17539207e-02 7.82873626e-02\n",
      " 6.65477176e-02 5.22056857e-02 4.29609159e-02 4.15179503e-02\n",
      " 4.14397067e-02 5.42386362e-02 5.42386362e-02 5.49546140e-02\n",
      " 5.49546140e-02 5.49546140e-02]\n",
      "[0.         0.99725731 1.         1.         0.17871066 0.03433636\n",
      " 0.8370554  1.         1.         1.         1.         1.\n",
      " 1.         1.         0.95113278 0.65042735 0.41140374 1.\n",
      " 1.         1.         1.         1.        ]\n",
      "Combined_proba [0.59405941 0.9895559  0.99009901 0.62277544 0.65819609 0.626777\n",
      " 0.7657724  0.89273007 0.96044956 0.93104511 0.97936053 0.99009901\n",
      " 0.99009901 0.99009901 0.98042233 0.91930871 0.8735453  0.86025261\n",
      " 0.75889174 0.89280279 0.79786271 0.92349458]\n",
      "Remain_indexes:  [False  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True]\n",
      "[ 3  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]\n",
      "[[ 0  6]\n",
      " [ 6 16]\n",
      " [16 21]\n",
      " [21 21]]\n",
      "[[  0  36]\n",
      " [ 36  96]\n",
      " [ 96 126]\n",
      " [126 126]]\n",
      "##################################################################\n",
      "L shape (567498, 126)\n",
      "All agree, Number of outliers = 35\n",
      "All agree, Number of inliers = 458389\n",
      "num of inliers = 458389\n",
      "num of outliers = 6032\n",
      "num of outliers = 6032\n",
      "Training data shape:  (308797, 21)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-1 score from LR: 0.0017988874108114644\n",
      "(308797, 3)\n",
      "(308797,)\n",
      "F-1 score from SVM: 0.24666703854521116\n",
      "length of prediction_high_conf_outliers: 7378\n",
      "length of prediction high conf inliers:  207487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coef from both LR and SVM:  [3.25847559e-05 5.23283145e-02 2.21203545e-01 2.17291461e-01\n",
      " 1.93363364e-01 2.38868074e-01 6.97424732e-01 1.45901883e+00\n",
      " 1.39304400e+00 1.26971629e+00 1.48220825e+00 1.57245305e+00\n",
      " 1.63852804e+00 1.69069396e+00 1.75605540e+00 1.78283137e+00\n",
      " 2.15519529e+00 2.15519529e+00 2.52058481e+00 2.52058481e+00\n",
      " 2.52058481e+00]\n",
      "[0.         0.03948839 0.69087643 0.44732402 0.19950254 0.87063812\n",
      " 0.97059651 1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.        ]\n",
      "Combined_proba [0.82599294 0.83297329 0.6340318  0.6233412  0.55615313 0.78310558\n",
      " 0.90560056 0.96698682 0.94244262 0.98277202 0.99173554 0.99173554\n",
      " 0.99173554 0.98365831 0.93264612 0.8944469  0.88335135 0.79874435\n",
      " 0.91052134 0.83127383 0.93614011]\n",
      "Remain_indexes:  [ True  True  True  True False  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True]\n",
      "[ 3  4  6  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]\n",
      "[[ 0  5]\n",
      " [ 5 15]\n",
      " [15 20]\n",
      " [20 20]]\n",
      "[[  0  30]\n",
      " [ 30  90]\n",
      " [ 90 120]\n",
      " [120 120]]\n",
      "##################################################################\n",
      "L shape (567498, 120)\n",
      "All agree, Number of outliers = 35\n",
      "All agree, Number of inliers = 459590\n",
      "num of inliers = 204177\n",
      "num of outliers = 7378\n",
      "num of outliers = 7378\n",
      "Training data shape:  (211555, 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-1 score from LR: 0.0040447620331670485\n",
      "(211555, 3)\n",
      "(211555,)\n",
      "F-1 score from SVM: 0.22811452153727108\n",
      "length of prediction_high_conf_outliers: 8028\n",
      "length of prediction high conf inliers:  497997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coef from both LR and SVM:  [ 8.46573995e-07  3.45503505e-03 -6.74646797e-04  2.02030832e-04\n",
      "  1.91318739e-04  3.57391533e-03  4.55405582e-03  4.71552308e-03\n",
      "  4.38118144e-03  4.62819123e-03  4.39802664e-03  3.95869282e-03\n",
      "  3.81204281e-03  3.97806564e-03  4.10304515e-03 -7.65294483e-04\n",
      " -7.65294483e-04 -1.09782981e-03 -1.09782981e-03 -1.09782981e-03]\n",
      "[0.98597856 1.         0.90255658 1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         0.73213553 0.48554925 0.04523228\n",
      " 0.22228973 0.        ]\n",
      "Combined_proba [0.84868594 0.85666502 0.67212043 0.67676798 0.81387074 0.91899055\n",
      " 0.97166954 0.95060678 0.9852157  0.9929078  0.9929078  0.9929078\n",
      " 0.98597628 0.94219986 0.90941897 0.8619023  0.75431951 0.7877853\n",
      " 0.74489311 0.80335428]\n",
      "Remain_indexes:  [ True  True False  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True]\n",
      "[ 3  4  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]\n",
      "[[ 0  4]\n",
      " [ 4 14]\n",
      " [14 19]\n",
      " [19 19]]\n",
      "[[  0  24]\n",
      " [ 24  84]\n",
      " [ 84 114]\n",
      " [114 114]]\n",
      "##################################################################\n",
      "L shape (567498, 114)\n",
      "All agree, Number of outliers = 35\n",
      "All agree, Number of inliers = 462213\n",
      "num of inliers = 457215\n",
      "num of outliers = 8028\n",
      "num of outliers = 8028\n",
      "Training data shape:  (465243, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-1 score from LR: 0.0008401373871727258\n",
      "(465243, 3)\n",
      "(465243,)\n",
      "F-1 score from SVM: 0.24007818014007276\n",
      "length of prediction_high_conf_outliers: 322\n",
      "length of prediction high conf inliers:  0\n",
      "Coef from normal training:  [ 9.49041510e-06  5.42302904e-04  3.28644106e-04  3.47990626e-04\n",
      "  3.86500750e-04  5.45228304e-04  5.75178836e-04  5.44620413e-04\n",
      "  5.61962262e-04  5.48165142e-04  5.09072710e-04  4.99524764e-04\n",
      "  5.18833690e-04  5.31032315e-04  2.23246409e-07  2.23246409e-07\n",
      " -2.29106407e-05 -2.29106407e-05 -2.29106407e-05]\n",
      "Combined Coef:  [ 9.49041510e-06  5.42302904e-04  3.28644106e-04  3.47990626e-04\n",
      "  3.86500750e-04  5.45228304e-04  5.75178836e-04  5.44620413e-04\n",
      "  5.61962262e-04  5.48165142e-04  5.09072710e-04  4.99524764e-04\n",
      "  5.18833690e-04  5.31032315e-04  2.23246409e-07  2.23246409e-07\n",
      " -2.29106407e-05 -2.29106407e-05 -2.29106407e-05]\n",
      "[0.93233893 1.         0.99624212 1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         0.77474342 0.5275918  0.05223215 0.24869672\n",
      " 0.        ]\n",
      "Combined_proba [0.85907761 0.87447061 0.71645421 0.83699239 0.92905383 0.97518886\n",
      " 0.95674259 0.98705226 0.99378882 0.99378882 0.99378882 0.98771836\n",
      " 0.94938    0.92067127 0.85107511 0.72615458 0.69641224 0.68325381\n",
      " 0.70355871]\n",
      "Remain_indexes:  [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True False  True]\n",
      "[ 3  4  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 24]\n",
      "[[ 0  4]\n",
      " [ 4 14]\n",
      " [14 18]\n",
      " [18 18]]\n",
      "[[  0  24]\n",
      " [ 24  84]\n",
      " [ 84 108]\n",
      " [108 108]]\n",
      "##################################################################\n",
      "L shape (567498, 108)\n",
      "All agree, Number of outliers = 35\n",
      "All agree, Number of inliers = 462213\n",
      "num of inliers = 462213\n",
      "num of outliers = 322\n",
      "num of outliers = 322\n",
      "Training data shape:  (278059, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-1 score from LR: 0.002272380079437825\n",
      "(278059, 3)\n",
      "(278059,)\n",
      "F-1 score from SVM: 0.3353556802669498\n",
      "length of prediction_high_conf_outliers: 5326\n",
      "length of prediction high conf inliers:  247148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coef from both LR and SVM:  [9.41622928e-08 1.10616131e-03 7.22373455e-04 7.17472744e-04\n",
      " 8.44516003e-04 1.28181092e-03 1.33166609e-03 1.25667566e-03\n",
      " 1.23794321e-03 1.21245551e-03 1.13932101e-03 1.11812621e-03\n",
      " 1.14093140e-03 1.14739478e-03 2.69405932e-04 2.69405932e-04\n",
      " 2.51052552e-04 2.51052552e-04]\n",
      "[0.         1.         1.         0.95873404 1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         0.57372415 0.81807592 0.06090233 0.27950275]\n",
      "Combined_proba [0.76415191 0.88834126 0.74778524 0.85044451 0.93689319 0.97793042\n",
      " 0.96152241 0.98848295 0.99447514 0.99447514 0.99447514 0.98907545\n",
      " 0.95497337 0.92943688 0.82042859 0.73631163 0.62619015 0.6567017 ]\n",
      "Remain_indexes:  [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True False  True]\n",
      "[ 3  4  7  9 10 11 12 13 14 15 16 17 18 19 20 21 24]\n",
      "[[ 0  4]\n",
      " [ 4 14]\n",
      " [14 17]\n",
      " [17 17]]\n",
      "[[  0  24]\n",
      " [ 24  84]\n",
      " [ 84 102]\n",
      " [102 102]]\n",
      "##################################################################\n",
      "L shape (567498, 102)\n",
      "All agree, Number of outliers = 35\n",
      "All agree, Number of inliers = 462213\n",
      "num of inliers = 243884\n",
      "num of outliers = 5326\n",
      "num of outliers = 5326\n",
      "Training data shape:  (249210, 17)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-1 score from LR: 0.005112123034624967\n",
      "(249210, 3)\n",
      "(249210,)\n",
      "F-1 score from SVM: 0.305428926647327\n",
      "length of prediction_high_conf_outliers: 6114\n",
      "length of prediction high conf inliers:  511995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coef from both LR and SVM:  [-4.86021480e-10  3.58247349e-03 -7.35883222e-04  5.47791371e-04\n",
      "  5.13221053e-03  5.85153858e-03  5.85445158e-03  5.20299321e-03\n",
      "  5.52589321e-03  4.97113578e-03  4.15324009e-03  3.78470773e-03\n",
      "  3.97079112e-03  4.14307102e-03 -3.14590731e-03 -3.14590731e-03\n",
      " -4.11200408e-03]\n",
      "[0.86033891 1.         0.6222358  0.98002696 1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         0.07068095 0.31338818 0.        ]\n",
      "Combined_proba [0.77372276 0.89945158 0.73529276 0.86333828 0.94317247 0.9801264\n",
      " 0.96535103 0.98962892 0.99502488 0.99502488 0.99502488 0.99016247\n",
      " 0.95945363 0.93645808 0.74582684 0.6942297  0.59135825]\n",
      "Remain_indexes:  [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True False]\n",
      "[ 3  4  7  9 10 11 12 13 14 15 16 17 18 19 20 21]\n",
      "[[ 0  4]\n",
      " [ 4 14]\n",
      " [14 16]\n",
      " [16 16]]\n",
      "[[ 0 24]\n",
      " [24 84]\n",
      " [84 96]\n",
      " [96 96]]\n",
      "##################################################################\n",
      "L shape (567498, 96)\n",
      "All agree, Number of outliers = 35\n",
      "All agree, Number of inliers = 462232\n",
      "num of inliers = 460770\n",
      "num of outliers = 6114\n",
      "num of outliers = 6114\n",
      "Training data shape:  (466884, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-1 score from LR: 0.0008647250319730264\n",
      "(466884, 3)\n",
      "(466884,)\n",
      "F-1 score from SVM: 0.317946505608283\n",
      "length of prediction_high_conf_outliers: 264\n",
      "length of prediction high conf inliers:  0\n",
      "Coef from normal training:  [ 1.50681723e-07  5.80646629e-04  3.58190003e-04  3.60391697e-04\n",
      "  4.35422136e-04  5.91793885e-04  6.13336051e-04  5.71583795e-04\n",
      "  5.84418739e-04  5.60818608e-04  5.10541038e-04  4.92837686e-04\n",
      "  5.10089323e-04  5.21441199e-04 -4.25340341e-05 -4.25340341e-05]\n",
      "Combined Coef:  [ 1.50681723e-07  5.80646629e-04  3.58190003e-04  3.60391697e-04\n",
      "  4.35422136e-04  5.91793885e-04  6.13336051e-04  5.71583795e-04\n",
      "  5.84418739e-04  5.60818608e-04  5.10541038e-04  4.92837686e-04\n",
      "  5.10089323e-04  5.21441199e-04 -4.25340341e-05 -4.25340341e-05]\n",
      "[0.35282277 1.         0.67458958 0.90110271 0.99456757 1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         0.         0.08283896]\n",
      "Combined_proba [0.73563226 0.90855099 0.72979926 0.86675588 0.94782361 0.98192491\n",
      " 0.96848668 0.99056748 0.99547511 0.99547511 0.99547511 0.99105274\n",
      " 0.96312299 0.94220848 0.6783312  0.63890022]\n",
      "Remain_indexes:  [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True False]\n",
      "[ 3  4  7  9 10 11 12 13 14 15 16 17 18 19 20]\n",
      "[[ 0  4]\n",
      " [ 4 14]\n",
      " [14 15]\n",
      " [15 15]]\n",
      "[[ 0 24]\n",
      " [24 84]\n",
      " [84 90]\n",
      " [90 90]]\n",
      "##################################################################\n",
      "L shape (567498, 90)\n",
      "All agree, Number of outliers = 35\n",
      "All agree, Number of inliers = 462232\n",
      "num of inliers = 462232\n",
      "num of outliers = 264\n",
      "num of outliers = 4137\n",
      "Training data shape:  (289574, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-1 score from LR: 0.0038429244978363366\n",
      "(289574, 3)\n",
      "(289574,)\n",
      "F-1 score from SVM: 0.27779871843196385\n",
      "length of prediction_high_conf_outliers: 6174\n",
      "length of prediction high conf inliers:  442375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coef from both LR and SVM:  [9.52473332e-07 1.18152166e-04 7.95152450e-05 6.83343566e-05\n",
      " 8.30242413e-05 1.20502709e-04 1.25369563e-04 1.19055755e-04\n",
      " 1.20905405e-04 1.18499517e-04 1.11147248e-04 1.09550073e-04\n",
      " 1.12387642e-04 1.13597386e-04 1.12949604e-05]\n",
      "[0.         1.         0.73043043 0.39894354 0.93848009 1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         0.09825869]\n",
      "Combined_proba [0.67458394 0.91614012 0.72985164 0.82793328 0.94704822 0.98342492\n",
      " 0.97110189 0.99135027 0.99585062 0.99585062 0.99585062 0.99179525\n",
      " 0.96618332 0.94700446 0.6301924 ]\n",
      "Remain_indexes:  [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True False]\n",
      "[ 3  4  7  9 10 11 12 13 14 15 16 17 18 19]\n",
      "[[ 0  4]\n",
      " [ 4 14]\n",
      " [14 14]\n",
      " [14 14]]\n",
      "[[ 0 24]\n",
      " [24 84]\n",
      " [84 84]\n",
      " [84 84]]\n",
      "##################################################################\n",
      "L shape (567498, 84)\n",
      "All agree, Number of outliers = 49\n",
      "All agree, Number of inliers = 466462\n",
      "num of inliers = 421577\n",
      "num of outliers = 6174\n",
      "num of outliers = 6174\n",
      "Training data shape:  (427751, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-1 score from LR: 0.000817052363444116\n",
      "(427751, 3)\n",
      "(427751,)\n",
      "F-1 score from SVM: 0.27706766917293235\n",
      "length of prediction_high_conf_outliers: 254\n",
      "length of prediction high conf inliers:  0\n",
      "Coef from normal training:  [2.91826174e-06 3.11228175e-04 1.65676454e-04 2.02690068e-04\n",
      " 2.19066519e-04 3.24271372e-04 3.43546338e-04 3.26741424e-04\n",
      " 3.32330719e-04 3.25308584e-04 3.03569794e-04 2.98613714e-04\n",
      " 3.08765987e-04 3.14548963e-04]\n",
      "Combined Coef:  [2.91826174e-06 3.11228175e-04 1.65676454e-04 2.02690068e-04\n",
      " 2.19066519e-04 3.24271372e-04 3.43546338e-04 3.26741424e-04\n",
      " 3.32330719e-04 3.25308584e-04 3.03569794e-04 2.98613714e-04\n",
      " 3.08765987e-04 3.14548963e-04]\n",
      "[0.         1.         0.11590429 0.44961043 0.78702012 1.\n",
      " 1.         1.         1.         1.         1.         0.96955555\n",
      " 1.         1.        ]\n",
      "Combined_proba [0.62289168 0.92256616 0.68280586 0.79894302 0.93478553 0.98469504\n",
      " 0.97331631 0.99201308 0.99616858 0.99616858 0.99616858 0.99009106\n",
      " 0.96877464 0.95106542]\n",
      "Remain_indexes:  [False  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True]\n",
      "[ 4  7  9 10 11 12 13 14 15 16 17 18 19]\n",
      "[[ 0  3]\n",
      " [ 3 13]\n",
      " [13 13]\n",
      " [13 13]]\n",
      "[[ 0 18]\n",
      " [18 78]\n",
      " [78 78]\n",
      " [78 78]]\n",
      "##################################################################\n",
      "L shape (567498, 78)\n",
      "All agree, Number of outliers = 60\n",
      "All agree, Number of inliers = 474951\n",
      "num of inliers = 474951\n",
      "num of outliers = 254\n",
      "num of outliers = 4248\n",
      "Training data shape:  (276984, 13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-1 score from LR: 0.00360191294872571\n",
      "(276984, 3)\n",
      "(276984,)\n",
      "F-1 score from SVM: 0.29174638780761364\n",
      "length of prediction_high_conf_outliers: 6831\n",
      "length of prediction high conf inliers:  391053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coef from both LR and SVM:  [ 3.57436597e-08 -2.68765568e-02  2.95079739e-02  2.77676661e-01\n",
      "  2.57169814e-01  2.34844032e-01  1.79118020e-01  1.95619968e-01\n",
      "  1.42881859e-01  7.66600211e-02  4.12644262e-02  4.73108953e-02\n",
      "  5.62313721e-02]\n",
      "[0.13837662 0.         0.50828435 1.         1.         1.\n",
      " 1.         1.         1.         1.         0.8445202  0.99153365\n",
      " 1.        ]\n",
      "Combined_proba [0.86675196 0.63420758 0.77825557 0.93942713 0.98578436 0.9752155\n",
      " 0.99258154 0.99644128 0.99644128 0.99644128 0.97973014 0.97039449\n",
      " 0.95454831]\n",
      "Remain_indexes:  [ True False  True  True  True  True  True  True  True  True  True  True\n",
      "  True]\n",
      "[ 4  9 10 11 12 13 14 15 16 17 18 19]\n",
      "[[ 0  2]\n",
      " [ 2 12]\n",
      " [12 12]\n",
      " [12 12]]\n",
      "[[ 0 12]\n",
      " [12 72]\n",
      " [72 72]\n",
      " [72 72]]\n",
      "##################################################################\n",
      "L shape (567498, 72)\n",
      "All agree, Number of outliers = 60\n",
      "All agree, Number of inliers = 480062\n",
      "num of inliers = 380941\n",
      "num of outliers = 6831\n",
      "num of outliers = 6831\n",
      "Training data shape:  (387772, 12)\n",
      "F-1 score from LR: 0.004785458639964612\n",
      "(387772, 3)\n",
      "(387772,)\n",
      "F-1 score from SVM: 0.2924409761259176\n",
      "length of prediction_high_conf_outliers: 7167\n",
      "length of prediction high conf inliers:  499570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coef from both LR and SVM:  [ 5.49510735e-07  3.05886249e-03  2.38478457e-01  9.76490868e-02\n",
      "  8.71220931e-02  5.15462901e-02  1.18201864e-01  6.72697060e-02\n",
      "  2.32406532e-03 -2.46843363e-02  4.74739248e-03  3.59712170e-02]\n",
      "[0.16769529 0.9002185  1.         1.         1.         1.\n",
      " 1.         1.         0.57606153 0.         1.         1.        ]\n",
      "Combined_proba [0.82030301 0.78635942 0.9434519  0.98672892 0.97686231 0.99307446\n",
      " 0.99667774 0.99667774 0.96850907 0.9146318  0.97236164 0.95756836]\n",
      "Remain_indexes:  [ True False  True  True  True  True  True  True  True  True  True  True]\n",
      "[ 4 10 11 12 13 14 15 16 17 18 19]\n",
      "[[ 0  1]\n",
      " [ 1 11]\n",
      " [11 11]\n",
      " [11 11]]\n",
      "[[ 0  6]\n",
      " [ 6 66]\n",
      " [66 66]\n",
      " [66 66]]\n",
      "##################################################################\n",
      "L shape (567498, 66)\n",
      "All agree, Number of outliers = 60\n",
      "All agree, Number of inliers = 498163\n",
      "num of inliers = 487280\n",
      "num of outliers = 7167\n",
      "num of outliers = 7167\n",
      "Training data shape:  (494447, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-1 score from LR: 0.005500855174964175\n",
      "(494447, 3)\n",
      "(494447,)\n",
      "F-1 score from SVM: 0.29757738896366087\n",
      "length of prediction_high_conf_outliers: 7182\n",
      "length of prediction high conf inliers:  511894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yizhouyan/ENV3/lib/python3.5/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coef from both LR and SVM:  [ 4.09718085e-05  3.63018409e-01  4.95673442e-01  2.54719550e-01\n",
      " -5.54643091e-02 -1.17481605e-01  1.73302600e-01  1.64365159e-02\n",
      "  5.27097861e-01  1.17861668e+00  1.75101678e+00]\n",
      "[0.64901098 1.         1.         1.         0.20184778 0.\n",
      " 1.         0.94879918 1.         1.         1.        ]\n",
      "Combined_proba [0.80963061 0.94697515 0.98755578 0.97830391 0.94377685 0.93457944\n",
      " 0.99688474 0.96728104 0.91995069 0.97408365 0.96021207]\n",
      "Remain_indexes:  [False  True  True  True  True  True  True  True  True  True  True]\n",
      "[10 11 12 13 14 15 16 17 18 19]\n",
      "[[ 0  0]\n",
      " [ 0 10]\n",
      " [10 10]\n",
      " [10 10]]\n",
      "[[ 0  0]\n",
      " [ 0 60]\n",
      " [60 60]\n",
      " [60 60]]\n",
      "##################################################################\n",
      "L shape (567498, 60)\n",
      "All agree, Number of outliers = 4169\n",
      "All agree, Number of inliers = 525426\n",
      "num of inliers = 505660\n",
      "num of outliers = 7300\n",
      "num of outliers = 7257\n",
      "Training data shape:  (512917, 10)\n",
      "F-1 score from LR: 0.010241073046945805\n",
      "(512917, 3)\n",
      "(512917,)\n",
      "F-1 score from SVM: 0.29606320299946437\n",
      "length of prediction_high_conf_outliers: 6931\n",
      "length of prediction high conf inliers:  525472\n",
      "Coef from both LR and SVM:  [0.29122487 0.19641559 0.83585768 0.65141866 0.60988129 0.5775216\n",
      " 0.54395401 0.60056099 0.61146705 0.69100937]\n",
      "[0.24652196 0.         1.         1.         1.         0.98522535\n",
      " 0.73013435 1.         1.         1.        ]\n",
      "Combined_proba [0.90589285 0.92963462 0.97957641 0.9470744  0.93841642 0.9962009\n",
      " 0.95337214 0.92464566 0.97560367 0.96254567]\n",
      "Remain_indexes:  [False  True  True  True  True  True  True  True  True  True]\n",
      "[11 12 13 14 15 16 17 18 19]\n",
      "[[0 0]\n",
      " [0 9]\n",
      " [9 9]\n",
      " [9 9]]\n",
      "[[ 0  0]\n",
      " [ 0 54]\n",
      " [54 54]\n",
      " [54 54]]\n",
      "##################################################################\n",
      "L shape (567498, 54)\n",
      "All agree, Number of outliers = 4375\n",
      "All agree, Number of inliers = 529256\n",
      "num of inliers = 522791\n",
      "num of outliers = 7024\n",
      "num of outliers = 7003\n",
      "Training data shape:  (529794, 9)\n",
      "F-1 score from LR: 0.011123142041539523\n",
      "(529794, 3)\n",
      "(529794,)\n",
      "F-1 score from SVM: 0.29205468595205075\n",
      "length of prediction_high_conf_outliers: 7239\n",
      "length of prediction high conf inliers:  538808\n",
      "Coef from both LR and SVM:  [0.50451619 0.87703994 0.78606035 0.72348277 0.78461919 0.7834607\n",
      " 0.85132915 0.89652439 0.9229045 ]\n",
      "[0.         1.         1.         0.30654149 1.         0.81699743\n",
      " 1.         1.         1.        ]\n",
      "Combined_proba [0.87813132 0.98070791 0.95000656 0.9034095  0.99641138 0.94581676\n",
      " 0.92882042 0.97695527 0.96462071]\n",
      "Remain_indexes:  [False  True  True  True  True  True  True  True  True]\n",
      "[12 13 14 15 16 17 18 19]\n",
      "[[0 0]\n",
      " [0 8]\n",
      " [8 8]\n",
      " [8 8]]\n",
      "[[ 0  0]\n",
      " [ 0 48]\n",
      " [48 48]\n",
      " [48 48]]\n",
      "##################################################################\n",
      "L shape (567498, 48)\n",
      "All agree, Number of outliers = 4519\n",
      "All agree, Number of inliers = 531345\n",
      "num of inliers = 528506\n",
      "num of outliers = 7307\n",
      "num of outliers = 7305\n",
      "Training data shape:  (535811, 8)\n",
      "F-1 score from LR: 0.011188118811881188\n",
      "(535811, 3)\n",
      "(535811,)\n"
     ]
    }
   ],
   "source": [
    "high_confidence_threshold = 0.99\n",
    "low_confidence_threshold = 0.01\n",
    "max_iter = 100\n",
    "min_max_diff = []\n",
    "N_size = 6\n",
    "\n",
    "proba_list = []\n",
    "\n",
    "L_original = L.copy()\n",
    "scores_for_training_original = scores_for_training.copy()\n",
    "\n",
    "index_range_origin = np.array([[0, 60], [60, 120], [120, 150], [150, 156]])\n",
    "coef_index_range_origin = np.array([[0, 10], [10, 20], [20, 25], [25, 26]])\n",
    "# index_range_origin = np.array([[0, 120], [120, 240], [240, 270], [270, 276]])\n",
    "# coef_index_range_origin = np.array([[0, 20], [20, 40], [40, 45], [45, 46]])\n",
    "remain_params_tracking_origin = np.array(range(0,np.max(coef_index_range_origin)))\n",
    "\n",
    "counter = 0\n",
    "last_training_outlier_indexes = []\n",
    "last_training_inlier_indexes = []\n",
    "\n",
    "for i_range in range(0, 100):\n",
    "    index_range = index_range_origin.copy()\n",
    "    coef_index_range = coef_index_range_origin.copy()\n",
    "#     coef_remain_index = range(156)\n",
    "    remain_params_tracking = remain_params_tracking_origin.copy()\n",
    "    \n",
    "    print(\"##################################################################\")\n",
    "    print('L shape', np.shape(L))\n",
    "    num_methods = np.shape(L)[1]\n",
    "\n",
    "    agree_outlier_indexes = np.sum(L,axis=1)==np.shape(L)[1]\n",
    "    print('All agree, Number of outliers = {}'.format(sum(agree_outlier_indexes)))\n",
    "    agree_inlier_indexes = np.sum(L,axis=1)==0\n",
    "    print('All agree, Number of inliers = {}'.format(sum(agree_inlier_indexes)))\n",
    "\n",
    "    disagree_indexes = np.where(np.logical_or(np.sum(L,axis = 1)==0, np.sum(L,axis = 1)==num_methods)==0)[0]\n",
    "\n",
    "#     all_inlier_indexes = np.where(agree_inlier_indexes)[0]\n",
    "    all_inlier_indexes = np.setdiff1d(np.where(agree_inlier_indexes)[0], prediction_high_conf_outliers)\n",
    "    if len(prediction_high_conf_inliers) >0:\n",
    "        all_inlier_indexes = np.intersect1d(np.setdiff1d(np.where(agree_inlier_indexes)[0], prediction_high_conf_outliers), prediction_high_conf_inliers)\n",
    "    print('num of inliers = {}'.format(np.shape(all_inlier_indexes)[0]))\n",
    "#     all_outlier_indexes = np.union1d(np.where(agree_outlier_indexes)[0], self_agree_index_list)\n",
    "\n",
    "#     if(len(np.intersect1d(np.where(agree_outlier_indexes)[0], prediction_high_conf_outliers)) > 0 and\n",
    "#       (len(np.union1d(np.where(agree_outlier_indexes)[0], prediction_high_conf_outliers)) > 2000)):\n",
    "#         all_outlier_indexes = np.intersect1d(np.where(agree_outlier_indexes)[0], prediction_high_conf_outliers)\n",
    "#     else:\n",
    "    all_outlier_indexes = np.union1d(np.where(agree_outlier_indexes)[0], prediction_high_conf_outliers)\n",
    "#     if(len(all_outlier_indexes) > 1000):\n",
    "#         all_outlier_indexes = np.random.RandomState(1).permutation(all_outlier_indexes)[:1000]\n",
    "        \n",
    "#     all_outlier_indexes = np.union1d(np.where(agree_outlier_indexes)[0], prediction_high_conf_outliers)\n",
    "    print('num of outliers = {}'.format(np.shape(all_outlier_indexes)[0]))\n",
    "    all_inlier_indexes = np.setdiff1d(all_inlier_indexes, prediction_classifier_disagree)\n",
    "    \n",
    "    self_agree_index_list = []\n",
    "    if((len(all_outlier_indexes) == 0) or (len(all_inlier_indexes)/ len(all_outlier_indexes) > 1000)):\n",
    "        for i in range(0, len(index_range)):\n",
    "            if(index_range[i,1]-index_range[i,0] <= 6):\n",
    "                continue\n",
    "            temp_index = disagree_indexes[np.where(np.sum(L[disagree_indexes][:,index_range[i,0]: index_range[i,1]], axis = 1)==(index_range[i,1]-index_range[i,0]))[0]]\n",
    "            self_agree_index_list = np.union1d(self_agree_index_list, temp_index)\n",
    "        self_agree_index_list = [int(i) for i in self_agree_index_list]\n",
    "#     self_agree_index_list = np.random.RandomState(1).permutation(self_agree_index_list)[:500]\n",
    "    all_outlier_indexes = np.union1d(all_outlier_indexes, self_agree_index_list)\n",
    "    all_outlier_indexes = np.setdiff1d(all_outlier_indexes, prediction_classifier_disagree)\n",
    "    print('num of outliers = {}'.format(np.shape(all_outlier_indexes)[0]))\n",
    "    \n",
    "#     if((len(all_outlier_indexes) == len(last_training_outlier_indexes)) and \n",
    "#        (len(all_inlier_indexes) == len(last_training_inlier_indexes)) and\n",
    "#        (sum(all_outlier_indexes == last_training_outlier_indexes) == len(last_training_outlier_indexes)) and\n",
    "#        sum(all_inlier_indexes == last_training_inlier_indexes) == len(last_training_inlier_indexes)\n",
    "#       ):\n",
    "    counter = counter+1\n",
    "#     else:\n",
    "#         counter = 0\n",
    "    last_training_outlier_indexes = all_outlier_indexes\n",
    "    last_training_inlier_indexes = all_inlier_indexes\n",
    "    \n",
    "    from sklearn.preprocessing import RobustScaler\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    data_indexes = np.concatenate((all_inlier_indexes, all_outlier_indexes), axis = 0)\n",
    "    data_indexes = np.array([int(i) for i in data_indexes])\n",
    "    labels = np.concatenate((np.zeros(len(all_inlier_indexes)), np.ones(len(all_outlier_indexes))), axis = 0)\n",
    "    transformer = RobustScaler().fit(scores_for_training_original)\n",
    "    scores_transformed = transformer.transform(scores_for_training_original)\n",
    "    training_data = scores_transformed[data_indexes]\n",
    "    print('Training data shape: ', np.shape(training_data))\n",
    "\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.svm import SVC\n",
    "#     clf = SVC(gamma='auto', probability=True, random_state=0)\n",
    "#     clf.fit(training_data, labels)\n",
    "    clf = LogisticRegression(random_state=0, penalty='l2', max_iter=max_iter).fit(training_data, labels) \n",
    "    clf_predictions = clf.predict(scores_transformed)\n",
    "    clf_predict_proba = clf.predict_proba(scores_transformed)[:,1]\n",
    "    print(\"F-1 score from LR:\",metrics.f1_score(y, np.array([int(i) for i in clf_predict_proba > 0.5])))\n",
    "    \n",
    "    transformer = RobustScaler().fit(X)\n",
    "    X_transformed = transformer.transform(X)\n",
    "    X_training_data = X_transformed[data_indexes]\n",
    "    print(np.shape(X_training_data))\n",
    "    print(np.shape(labels))\n",
    "\n",
    "    clf_X = SVC(gamma='auto', probability=True, random_state=0)\n",
    "    clf_X.fit(X_training_data, labels)\n",
    "    clf_predictions_X = clf_X.predict(X_transformed)\n",
    "    clf_predict_proba_X = clf_X.predict_proba(X_transformed)[:,1]\n",
    "    print(\"F-1 score from SVM:\",metrics.f1_score(y, np.array([int(i) for i in clf_predict_proba_X > 0.5])))\n",
    "    cur_f1_scores.append(metrics.f1_score(y, np.array([int(i) for i in clf_predict_proba_X > 0.5])))\n",
    "        \n",
    "    agreed_outlier_indexes = np.where(np.sum(L,axis=1)==np.shape(L)[1])[0]\n",
    "    agreed_inlier_indexes = np.where(np.sum(L,axis=1)==0)[0]\n",
    "        \n",
    "    prediction_result_list.append(clf_predict_proba)\n",
    "    classifier_result_list.append(clf_predict_proba_X)\n",
    "    \n",
    "    prediction_list.append(np.array([int(i) for i in clf_predictions]))\n",
    "    \n",
    "    prediction_high_conf_outliers = np.intersect1d(np.where(prediction_result_list[-1] > high_confidence_threshold)[0],\n",
    "                                                   np.where(classifier_result_list[-1] > high_confidence_threshold)[0])\n",
    "    print('length of prediction_high_conf_outliers:' , len(prediction_high_conf_outliers))\n",
    "    prediction_high_conf_inliers = np.intersect1d(np.where(prediction_result_list[-1] < low_confidence_threshold)[0],\n",
    "                                                   np.where(classifier_result_list[-1] < low_confidence_threshold)[0])\n",
    "    print('length of prediction high conf inliers: ', len(prediction_high_conf_inliers))\n",
    "    \n",
    "    temp_prediction = np.array([int(i) for i in prediction_result_list[-1] > 0.5])\n",
    "    temp_classifier = np.array([int(i) for i in classifier_result_list[-1] > 0.5])\n",
    "    prediction_classifier_disagree = np.where(temp_prediction != temp_classifier)[0]\n",
    "    \n",
    "    if(len(prediction_high_conf_outliers) > 0 and len(prediction_high_conf_inliers) > 0):\n",
    "        new_data_indexes = np.concatenate((prediction_high_conf_outliers, prediction_high_conf_inliers), axis = 0)\n",
    "        new_data_indexes = np.array([int(i) for i in new_data_indexes])\n",
    "        new_labels = np.concatenate((np.ones(len(prediction_high_conf_outliers)), np.zeros(len(prediction_high_conf_inliers))), axis = 0)\n",
    "        clf_prune_2 = LogisticRegression(random_state=0, penalty='l2', max_iter=max_iter).fit(scores_transformed[new_data_indexes], new_labels) \n",
    "        print('Coef from both LR and SVM: ', clf_prune_2.coef_[0])\n",
    "        combined_coef = clf_prune_2.coef_[0]  \n",
    "    else:\n",
    "        print('Coef from normal training: ', clf.coef_[0])\n",
    "        combined_coef = clf.coef_[0]\n",
    "        print('Combined Coef: ',  combined_coef)\n",
    "\n",
    "    if(np.max(coef_index_range) >= 2):\n",
    "        cur_clf_coef = combined_coef \n",
    "        proba_list.append(generate_coef_proba(cur_clf_coef))\n",
    "        combined_proba = get_kf_results(np.stack(proba_list))\n",
    "        print('Combined_proba', combined_proba)\n",
    "        cur_threshold = min(combined_proba) #max(max(0, np.mean(combined_proba)-np.std(combined_proba)),\n",
    "        \n",
    "        if(counter < 0):\n",
    "            remain_indexes_after_cond = (generate_decision_on_proba(combined_proba) == 1)\n",
    "            print('Remain_indexes: ', remain_indexes_after_cond)\n",
    "            remain_params_tracking = remain_params_tracking[remain_indexes_after_cond]\n",
    "            print(remain_params_tracking)\n",
    "\n",
    "            remain_indexes_after_cond_expanded = []\n",
    "            for i in range(0, len(coef_index_range)): #\n",
    "                s_e_range = coef_index_range[i,1]-coef_index_range[i,0]\n",
    "                s1, e1 = coef_index_range[i,0], coef_index_range[i,1]\n",
    "                s2, e2 = index_range[i,0], index_range[i,1]\n",
    "                saved_indexes = np.where(remain_indexes_after_cond[s1:e1] == 1)[0]\n",
    "                for j in range(N_size):\n",
    "                    remain_indexes_after_cond_expanded.extend(np.array(saved_indexes) + j * s_e_range + s2)\n",
    "\n",
    "            new_coef_index_range_seq = []\n",
    "            for i in range(0, len(coef_index_range)): #\n",
    "                s, e = coef_index_range[i,0], coef_index_range[i,1]\n",
    "                new_coef_index_range_seq.append(sum((remain_indexes_after_cond)[s:e]))\n",
    "\n",
    "            coef_index_range = []\n",
    "            index_range = []\n",
    "            cur_sum = 0\n",
    "            for i in range(0, len(new_coef_index_range_seq)):\n",
    "                coef_index_range.append([cur_sum, cur_sum + new_coef_index_range_seq[i]])\n",
    "                index_range.append([cur_sum * 6, 6 * (cur_sum + new_coef_index_range_seq[i])])\n",
    "                cur_sum += new_coef_index_range_seq[i]\n",
    "\n",
    "            coef_index_range = np.array(coef_index_range)\n",
    "            index_range = np.array(index_range)\n",
    "            print(coef_index_range)\n",
    "            print(index_range)\n",
    "\n",
    "            L=L_original[:,remain_indexes_after_cond_expanded]\n",
    "            scores_for_training = scores_for_training_original[:, remain_indexes_after_cond]\n",
    "        else:\n",
    "            remain_indexes_after_cond = ((combined_proba > cur_threshold) == 1)\n",
    "            print('Remain_indexes: ', remain_indexes_after_cond)\n",
    "            remain_params_tracking = remain_params_tracking[remain_indexes_after_cond]\n",
    "            print(remain_params_tracking)\n",
    "\n",
    "            remain_indexes_after_cond_expanded = []\n",
    "            for i in range(0, len(coef_index_range)): #\n",
    "                s_e_range = coef_index_range[i,1]-coef_index_range[i,0]\n",
    "                s1, e1 = coef_index_range[i,0], coef_index_range[i,1]\n",
    "                s2, e2 = index_range[i,0], index_range[i,1]\n",
    "                saved_indexes = np.where(remain_indexes_after_cond[s1:e1] == 1)[0]\n",
    "                for j in range(N_size):\n",
    "                    remain_indexes_after_cond_expanded.extend(np.array(saved_indexes) + j * s_e_range + s2)\n",
    "\n",
    "            new_coef_index_range_seq = []\n",
    "            for i in range(0, len(coef_index_range)): #\n",
    "                s, e = coef_index_range[i,0], coef_index_range[i,1]\n",
    "                new_coef_index_range_seq.append(sum((remain_indexes_after_cond)[s:e]))\n",
    "\n",
    "            coef_index_range = []\n",
    "            index_range = []\n",
    "            cur_sum = 0\n",
    "            for i in range(0, len(new_coef_index_range_seq)):\n",
    "                coef_index_range.append([cur_sum, cur_sum + new_coef_index_range_seq[i]])\n",
    "                index_range.append([cur_sum * 6, 6 * (cur_sum + new_coef_index_range_seq[i])])\n",
    "                cur_sum += new_coef_index_range_seq[i]\n",
    "\n",
    "            coef_index_range = np.array(coef_index_range)\n",
    "            index_range = np.array(index_range)\n",
    "            print(coef_index_range)\n",
    "            print(index_range)\n",
    "\n",
    "            L=L_original[:,remain_indexes_after_cond_expanded]\n",
    "            scores_for_training = scores_for_training_original[:, remain_indexes_after_cond]\n",
    "\n",
    "            L_original = L.copy()\n",
    "            scores_for_training_original = scores_for_training.copy()\n",
    "            index_range_origin = index_range.copy()\n",
    "            coef_index_range_origin = coef_index_range.copy()\n",
    "            remain_params_tracking_origin = remain_params_tracking.copy()\n",
    "            proba_list_new = []\n",
    "            for proba in proba_list:\n",
    "                proba_list_new.append(proba[remain_indexes_after_cond])\n",
    "            proba_list = proba_list_new\n",
    "            counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25]\n",
      "##################################################################\n",
      "L shape (5473, 156)\n",
      "All agree, Number of outliers = 55\n",
      "All agree, Number of inliers = 3788\n",
      "num of inliers = 3788\n",
      "num of outliers = 55\n",
      "num of outliers = 55\n",
      "Training data shape:  (3843, 26)\n",
      "F-1 score from LR: 0.38451612903225807\n",
      "(3843, 10)\n",
      "(3843,)\n",
      "F-1 score from SVM: 0.48945147679324896\n",
      "length of prediction_high_conf_outliers: 57\n",
      "length of prediction high conf inliers:  4596\n",
      "Coef from both LR and SVM:  [0.14236451 0.15635406 0.10511819 0.11718949 0.11599183 0.12917098\n",
      " 0.12295575 0.11890787 0.12193623 0.12599594 0.17927003 0.14618996\n",
      " 0.12512068 0.12150168 0.11431485 0.11153864 0.10734778 0.10519833\n",
      " 0.10321618 0.10088406 0.09577959 0.1006787  0.09917587 0.11325712\n",
      " 0.1017805  0.29550274]\n",
      "[4.16204204e-04 1.62114735e-03 4.20385015e-06 2.44485323e-05\n",
      " 2.09711867e-05 1.01459720e-04 4.95470065e-05 3.03323764e-05\n",
      " 4.38756555e-05 7.06967025e-05 1.13090299e-02 6.11788996e-04\n",
      " 6.38844292e-05 4.16439217e-05 1.68328033e-05 1.15142006e-05\n",
      " 6.14054074e-06 4.26482269e-06 2.91807251e-06 1.70016733e-06\n",
      " 0.00000000e+00 1.60878498e-06 1.00663078e-06 1.46030332e-05\n",
      " 2.12742306e-06 1.00000000e+00]\n",
      "Remain_indexes:  [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False  True]\n",
      "[25]\n",
      "[[0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 1]]\n",
      "[[0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 6]]\n",
      "##################################################################\n",
      "L shape (5473, 6)\n",
      "All agree, Number of outliers = 300\n",
      "All agree, Number of inliers = 4673\n",
      "num of inliers = 4504\n",
      "num of outliers = 300\n",
      "num of outliers = 191\n",
      "Training data shape:  (4695, 1)\n",
      "F-1 score from LR: 0.4116424116424117\n",
      "(4695, 10)\n",
      "(4695,)\n",
      "F-1 score from SVM: 0.5248\n",
      "length of prediction_high_conf_outliers: 246\n",
      "length of prediction high conf inliers:  4553\n",
      "##################################################################\n",
      "L shape (5473, 6)\n",
      "All agree, Number of outliers = 300\n",
      "All agree, Number of inliers = 4673\n",
      "num of inliers = 4526\n",
      "num of outliers = 300\n",
      "num of outliers = 300\n",
      "Training data shape:  (4826, 1)\n",
      "F-1 score from LR: 0.5098777046095956\n",
      "(4826, 10)\n",
      "(4826,)\n",
      "F-1 score from SVM: 0.5120772946859903\n",
      "length of prediction_high_conf_outliers: 272\n",
      "length of prediction high conf inliers:  4555\n",
      "##################################################################\n",
      "L shape (5473, 6)\n",
      "All agree, Number of outliers = 300\n",
      "All agree, Number of inliers = 4673\n",
      "num of inliers = 4539\n",
      "num of outliers = 300\n",
      "num of outliers = 299\n",
      "Training data shape:  (4838, 1)\n",
      "F-1 score from LR: 0.5056603773584906\n",
      "(4838, 10)\n",
      "(4838,)\n",
      "F-1 score from SVM: 0.5141471301535975\n",
      "length of prediction_high_conf_outliers: 271\n",
      "length of prediction high conf inliers:  4570\n",
      "##################################################################\n",
      "L shape (5473, 6)\n",
      "All agree, Number of outliers = 300\n",
      "All agree, Number of inliers = 4673\n",
      "num of inliers = 4553\n",
      "num of outliers = 300\n",
      "num of outliers = 299\n",
      "Training data shape:  (4852, 1)\n",
      "F-1 score from LR: 0.5056603773584906\n",
      "(4852, 10)\n",
      "(4852,)\n",
      "F-1 score from SVM: 0.5143325143325143\n",
      "length of prediction_high_conf_outliers: 270\n",
      "length of prediction high conf inliers:  4589\n",
      "##################################################################\n",
      "L shape (5473, 6)\n",
      "All agree, Number of outliers = 300\n",
      "All agree, Number of inliers = 4673\n",
      "num of inliers = 4568\n",
      "num of outliers = 300\n",
      "num of outliers = 299\n",
      "Training data shape:  (4867, 1)\n",
      "F-1 score from LR: 0.5056603773584906\n",
      "(4867, 10)\n",
      "(4867,)\n",
      "F-1 score from SVM: 0.5116279069767442\n",
      "length of prediction_high_conf_outliers: 270\n",
      "length of prediction high conf inliers:  4598\n",
      "##################################################################\n",
      "L shape (5473, 6)\n",
      "All agree, Number of outliers = 300\n",
      "All agree, Number of inliers = 4673\n",
      "num of inliers = 4574\n",
      "num of outliers = 300\n",
      "num of outliers = 299\n",
      "Training data shape:  (4873, 1)\n",
      "F-1 score from LR: 0.5061378659112371\n",
      "(4873, 10)\n",
      "(4873,)\n",
      "F-1 score from SVM: 0.5136702568351283\n",
      "length of prediction_high_conf_outliers: 270\n",
      "length of prediction high conf inliers:  4600\n",
      "##################################################################\n",
      "L shape (5473, 6)\n",
      "All agree, Number of outliers = 300\n",
      "All agree, Number of inliers = 4673\n",
      "num of inliers = 4576\n",
      "num of outliers = 300\n",
      "num of outliers = 299\n",
      "Training data shape:  (4875, 1)\n",
      "F-1 score from LR: 0.5061378659112371\n",
      "(4875, 10)\n",
      "(4875,)\n",
      "F-1 score from SVM: 0.511240632805995\n",
      "length of prediction_high_conf_outliers: 270\n",
      "length of prediction high conf inliers:  4600\n",
      "##################################################################\n",
      "L shape (5473, 6)\n",
      "All agree, Number of outliers = 300\n",
      "All agree, Number of inliers = 4673\n",
      "num of inliers = 4576\n",
      "num of outliers = 300\n",
      "num of outliers = 299\n",
      "Training data shape:  (4875, 1)\n",
      "F-1 score from LR: 0.5061378659112371\n",
      "(4875, 10)\n",
      "(4875,)\n",
      "F-1 score from SVM: 0.511240632805995\n",
      "length of prediction_high_conf_outliers: 270\n",
      "length of prediction high conf inliers:  4600\n",
      "##################################################################\n",
      "L shape (5473, 6)\n",
      "All agree, Number of outliers = 300\n",
      "All agree, Number of inliers = 4673\n",
      "num of inliers = 4576\n",
      "num of outliers = 300\n",
      "num of outliers = 299\n",
      "Training data shape:  (4875, 1)\n",
      "F-1 score from LR: 0.5061378659112371\n",
      "(4875, 10)\n",
      "(4875,)\n",
      "F-1 score from SVM: 0.511240632805995\n",
      "length of prediction_high_conf_outliers: 270\n",
      "length of prediction high conf inliers:  4600\n",
      "##################################################################\n",
      "L shape (5473, 6)\n",
      "All agree, Number of outliers = 300\n",
      "All agree, Number of inliers = 4673\n",
      "num of inliers = 4576\n",
      "num of outliers = 300\n",
      "num of outliers = 299\n",
      "Training data shape:  (4875, 1)\n",
      "F-1 score from LR: 0.5061378659112371\n",
      "(4875, 10)\n",
      "(4875,)\n",
      "F-1 score from SVM: 0.511240632805995\n",
      "length of prediction_high_conf_outliers: 270\n",
      "length of prediction high conf inliers:  4600\n",
      "##################################################################\n",
      "L shape (5473, 6)\n",
      "All agree, Number of outliers = 300\n",
      "All agree, Number of inliers = 4673\n",
      "num of inliers = 4576\n",
      "num of outliers = 300\n",
      "num of outliers = 299\n",
      "Training data shape:  (4875, 1)\n",
      "F-1 score from LR: 0.5061378659112371\n",
      "(4875, 10)\n",
      "(4875,)\n",
      "F-1 score from SVM: 0.511240632805995\n",
      "length of prediction_high_conf_outliers: 270\n",
      "length of prediction high conf inliers:  4600\n",
      "##################################################################\n",
      "L shape (5473, 6)\n",
      "All agree, Number of outliers = 300\n",
      "All agree, Number of inliers = 4673\n",
      "num of inliers = 4576\n",
      "num of outliers = 300\n",
      "num of outliers = 299\n",
      "Training data shape:  (4875, 1)\n",
      "F-1 score from LR: 0.5061378659112371\n",
      "(4875, 10)\n",
      "(4875,)\n",
      "F-1 score from SVM: 0.511240632805995\n",
      "length of prediction_high_conf_outliers: 270\n",
      "length of prediction high conf inliers:  4600\n",
      "##################################################################\n",
      "L shape (5473, 6)\n",
      "All agree, Number of outliers = 300\n",
      "All agree, Number of inliers = 4673\n",
      "num of inliers = 4576\n",
      "num of outliers = 300\n",
      "num of outliers = 299\n",
      "Training data shape:  (4875, 1)\n",
      "F-1 score from LR: 0.5061378659112371\n",
      "(4875, 10)\n",
      "(4875,)\n",
      "F-1 score from SVM: 0.511240632805995\n",
      "length of prediction_high_conf_outliers: 270\n",
      "length of prediction high conf inliers:  4600\n",
      "##################################################################\n",
      "L shape (5473, 6)\n",
      "All agree, Number of outliers = 300\n",
      "All agree, Number of inliers = 4673\n",
      "num of inliers = 4576\n",
      "num of outliers = 300\n",
      "num of outliers = 299\n",
      "Training data shape:  (4875, 1)\n",
      "F-1 score from LR: 0.5061378659112371\n",
      "(4875, 10)\n",
      "(4875,)\n",
      "F-1 score from SVM: 0.511240632805995\n",
      "length of prediction_high_conf_outliers: 270\n",
      "length of prediction high conf inliers:  4600\n",
      "##################################################################\n",
      "L shape (5473, 6)\n",
      "All agree, Number of outliers = 300\n",
      "All agree, Number of inliers = 4673\n",
      "num of inliers = 4576\n",
      "num of outliers = 300\n",
      "num of outliers = 299\n",
      "Training data shape:  (4875, 1)\n",
      "F-1 score from LR: 0.5061378659112371\n",
      "(4875, 10)\n",
      "(4875,)\n",
      "F-1 score from SVM: 0.511240632805995\n",
      "length of prediction_high_conf_outliers: 270\n",
      "length of prediction high conf inliers:  4600\n",
      "##################################################################\n",
      "L shape (5473, 6)\n",
      "All agree, Number of outliers = 300\n",
      "All agree, Number of inliers = 4673\n",
      "num of inliers = 4576\n",
      "num of outliers = 300\n",
      "num of outliers = 299\n",
      "Training data shape:  (4875, 1)\n",
      "F-1 score from LR: 0.5061378659112371\n",
      "(4875, 10)\n",
      "(4875,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-1 score from SVM: 0.511240632805995\n",
      "length of prediction_high_conf_outliers: 270\n",
      "length of prediction high conf inliers:  4600\n",
      "##################################################################\n",
      "L shape (5473, 6)\n",
      "All agree, Number of outliers = 300\n",
      "All agree, Number of inliers = 4673\n",
      "num of inliers = 4576\n",
      "num of outliers = 300\n",
      "num of outliers = 299\n",
      "Training data shape:  (4875, 1)\n",
      "F-1 score from LR: 0.5061378659112371\n",
      "(4875, 10)\n",
      "(4875,)\n",
      "F-1 score from SVM: 0.511240632805995\n",
      "length of prediction_high_conf_outliers: 270\n",
      "length of prediction high conf inliers:  4600\n",
      "##################################################################\n",
      "L shape (5473, 6)\n",
      "All agree, Number of outliers = 300\n",
      "All agree, Number of inliers = 4673\n",
      "num of inliers = 4576\n",
      "num of outliers = 300\n",
      "num of outliers = 299\n",
      "Training data shape:  (4875, 1)\n",
      "F-1 score from LR: 0.5061378659112371\n",
      "(4875, 10)\n",
      "(4875,)\n",
      "F-1 score from SVM: 0.511240632805995\n",
      "length of prediction_high_conf_outliers: 270\n",
      "length of prediction high conf inliers:  4600\n",
      "##################################################################\n",
      "L shape (5473, 6)\n",
      "All agree, Number of outliers = 300\n",
      "All agree, Number of inliers = 4673\n",
      "num of inliers = 4576\n",
      "num of outliers = 300\n",
      "num of outliers = 299\n",
      "Training data shape:  (4875, 1)\n",
      "F-1 score from LR: 0.5061378659112371\n",
      "(4875, 10)\n",
      "(4875,)\n",
      "F-1 score from SVM: 0.511240632805995\n",
      "length of prediction_high_conf_outliers: 270\n",
      "length of prediction high conf inliers:  4600\n",
      "##################################################################\n",
      "L shape (5473, 6)\n",
      "All agree, Number of outliers = 300\n",
      "All agree, Number of inliers = 4673\n",
      "num of inliers = 4576\n",
      "num of outliers = 300\n",
      "num of outliers = 299\n",
      "Training data shape:  (4875, 1)\n",
      "F-1 score from LR: 0.5061378659112371\n",
      "(4875, 10)\n",
      "(4875,)\n",
      "F-1 score from SVM: 0.511240632805995\n",
      "length of prediction_high_conf_outliers: 270\n",
      "length of prediction high conf inliers:  4600\n",
      "##################################################################\n",
      "L shape (5473, 6)\n",
      "All agree, Number of outliers = 300\n",
      "All agree, Number of inliers = 4673\n",
      "num of inliers = 4576\n",
      "num of outliers = 300\n",
      "num of outliers = 299\n",
      "Training data shape:  (4875, 1)\n",
      "F-1 score from LR: 0.5061378659112371\n",
      "(4875, 10)\n",
      "(4875,)\n",
      "F-1 score from SVM: 0.511240632805995\n",
      "length of prediction_high_conf_outliers: 270\n",
      "length of prediction high conf inliers:  4600\n",
      "##################################################################\n",
      "L shape (5473, 6)\n",
      "All agree, Number of outliers = 300\n",
      "All agree, Number of inliers = 4673\n",
      "num of inliers = 4576\n",
      "num of outliers = 300\n",
      "num of outliers = 299\n",
      "Training data shape:  (4875, 1)\n",
      "F-1 score from LR: 0.5061378659112371\n",
      "(4875, 10)\n",
      "(4875,)\n",
      "F-1 score from SVM: 0.511240632805995\n",
      "length of prediction_high_conf_outliers: 270\n",
      "length of prediction high conf inliers:  4600\n",
      "##################################################################\n",
      "L shape (5473, 6)\n",
      "All agree, Number of outliers = 300\n",
      "All agree, Number of inliers = 4673\n",
      "num of inliers = 4576\n",
      "num of outliers = 300\n",
      "num of outliers = 299\n",
      "Training data shape:  (4875, 1)\n",
      "F-1 score from LR: 0.5061378659112371\n",
      "(4875, 10)\n",
      "(4875,)\n",
      "F-1 score from SVM: 0.511240632805995\n",
      "length of prediction_high_conf_outliers: 270\n",
      "length of prediction high conf inliers:  4600\n",
      "##################################################################\n",
      "L shape (5473, 6)\n",
      "All agree, Number of outliers = 300\n",
      "All agree, Number of inliers = 4673\n",
      "num of inliers = 4576\n",
      "num of outliers = 300\n",
      "num of outliers = 299\n",
      "Training data shape:  (4875, 1)\n",
      "F-1 score from LR: 0.5061378659112371\n",
      "(4875, 10)\n",
      "(4875,)\n",
      "F-1 score from SVM: 0.511240632805995\n",
      "length of prediction_high_conf_outliers: 270\n",
      "length of prediction high conf inliers:  4600\n",
      "##################################################################\n",
      "L shape (5473, 6)\n",
      "All agree, Number of outliers = 300\n",
      "All agree, Number of inliers = 4673\n",
      "num of inliers = 4576\n",
      "num of outliers = 300\n",
      "num of outliers = 299\n",
      "Training data shape:  (4875, 1)\n",
      "F-1 score from LR: 0.5061378659112371\n",
      "(4875, 10)\n",
      "(4875,)\n",
      "F-1 score from SVM: 0.511240632805995\n",
      "length of prediction_high_conf_outliers: 270\n",
      "length of prediction high conf inliers:  4600\n",
      "##################################################################\n",
      "L shape (5473, 6)\n",
      "All agree, Number of outliers = 300\n",
      "All agree, Number of inliers = 4673\n",
      "num of inliers = 4576\n",
      "num of outliers = 300\n",
      "num of outliers = 299\n",
      "Training data shape:  (4875, 1)\n",
      "F-1 score from LR: 0.5061378659112371\n",
      "(4875, 10)\n",
      "(4875,)\n",
      "F-1 score from SVM: 0.511240632805995\n",
      "length of prediction_high_conf_outliers: 270\n",
      "length of prediction high conf inliers:  4600\n",
      "##################################################################\n",
      "L shape (5473, 6)\n",
      "All agree, Number of outliers = 300\n",
      "All agree, Number of inliers = 4673\n",
      "num of inliers = 4576\n",
      "num of outliers = 300\n",
      "num of outliers = 299\n",
      "Training data shape:  (4875, 1)\n",
      "F-1 score from LR: 0.5061378659112371\n",
      "(4875, 10)\n",
      "(4875,)\n",
      "F-1 score from SVM: 0.511240632805995\n",
      "length of prediction_high_conf_outliers: 270\n",
      "length of prediction high conf inliers:  4600\n",
      "##################################################################\n",
      "L shape (5473, 6)\n",
      "All agree, Number of outliers = 300\n",
      "All agree, Number of inliers = 4673\n",
      "num of inliers = 4576\n",
      "num of outliers = 300\n",
      "num of outliers = 299\n",
      "Training data shape:  (4875, 1)\n",
      "F-1 score from LR: 0.5061378659112371\n",
      "(4875, 10)\n",
      "(4875,)\n",
      "F-1 score from SVM: 0.511240632805995\n",
      "length of prediction_high_conf_outliers: 270\n",
      "length of prediction high conf inliers:  4600\n",
      "##################################################################\n",
      "L shape (5473, 6)\n",
      "All agree, Number of outliers = 300\n",
      "All agree, Number of inliers = 4673\n",
      "num of inliers = 4576\n",
      "num of outliers = 300\n",
      "num of outliers = 299\n",
      "Training data shape:  (4875, 1)\n",
      "F-1 score from LR: 0.5061378659112371\n",
      "(4875, 10)\n",
      "(4875,)\n",
      "F-1 score from SVM: 0.511240632805995\n",
      "length of prediction_high_conf_outliers: 270\n",
      "length of prediction high conf inliers:  4600\n"
     ]
    }
   ],
   "source": [
    "high_confidence_threshold = 0.99\n",
    "low_confidence_threshold = 0.01\n",
    "max_iter = 100\n",
    "remain_params_tracking = np.array(range(0,np.max(coef_index_range)))\n",
    "print(remain_params_tracking)\n",
    "min_max_diff = []\n",
    "N_size = 6\n",
    "\n",
    "for i_range in range(0, 30):\n",
    "    print(\"##################################################################\")\n",
    "    print('L shape', np.shape(L))\n",
    "    num_methods = np.shape(L)[1]\n",
    "\n",
    "#     agree_outlier_indexes = (np.sum(L,axis=1)==np.shape(L)[1])\n",
    "#     print('All agree, Number of outliers = {}'.format(sum(agree_outlier_indexes)))\n",
    "#     agree_inlier_indexes = (np.sum(L,axis=1)==0)\n",
    "#     print('All agree, Number of inliers = {}'.format(sum(agree_inlier_indexes)))\n",
    "\n",
    "#     all_inlier_indexes = np.union1d(np.setdiff1d(np.where(agree_inlier_indexes)[0], prediction_high_conf_outliers), prediction_high_conf_inliers)\n",
    "#     print('num of inliers = {}'.format(np.shape(all_inlier_indexes)[0]))\n",
    "#     all_outlier_indexes = np.union1d(np.where(agree_outlier_indexes)[0], prediction_high_conf_outliers)\n",
    "#     print('num of outliers = {}'.format(np.shape(all_outlier_indexes)[0]))\n",
    "\n",
    "#     disagree_indexes = np.where(np.logical_or(np.sum(L,axis = 1)==0, np.sum(L,axis = 1)==num_methods)==0)[0]\n",
    "\n",
    "    ########################################################################\n",
    "\n",
    "    agree_outlier_indexes = np.sum(L,axis=1)==np.shape(L)[1]\n",
    "    print('All agree, Number of outliers = {}'.format(sum(agree_outlier_indexes)))\n",
    "    agree_inlier_indexes = np.sum(L,axis=1)==0\n",
    "    print('All agree, Number of inliers = {}'.format(sum(agree_inlier_indexes)))\n",
    "\n",
    "    disagree_indexes = np.where(np.logical_or(np.sum(L,axis = 1)==0, np.sum(L,axis = 1)==num_methods)==0)[0]\n",
    "    # print('Number of disagreed points = {}'.format(len(disagree_indexes)))\n",
    "    # print('Number of disagreed points (true outliers) = {}'.format(sum(y[disagree_indexes] == 1)))\n",
    "    # print('Number of disagreed points (true inliers) = {}'.format(sum(y[disagree_indexes] == 0)))\n",
    "\n",
    "#     all_inlier_indexes = np.where(agree_inlier_indexes)[0]\n",
    "    all_inlier_indexes = np.setdiff1d(np.where(agree_inlier_indexes)[0], prediction_high_conf_outliers)\n",
    "    if len(prediction_high_conf_inliers) >0:\n",
    "        all_inlier_indexes = np.intersect1d(np.setdiff1d(np.where(agree_inlier_indexes)[0], prediction_high_conf_outliers), prediction_high_conf_inliers)\n",
    "    print('num of inliers = {}'.format(np.shape(all_inlier_indexes)[0]))\n",
    "#     all_outlier_indexes = np.union1d(np.where(agree_outlier_indexes)[0], self_agree_index_list)\n",
    "\n",
    "#     if(len(np.intersect1d(np.where(agree_outlier_indexes)[0], prediction_high_conf_outliers)) > 0 and\n",
    "#       (len(np.union1d(np.where(agree_outlier_indexes)[0], prediction_high_conf_outliers)) > 2000)):\n",
    "#         all_outlier_indexes = np.intersect1d(np.where(agree_outlier_indexes)[0], prediction_high_conf_outliers)\n",
    "#     else:\n",
    "    all_outlier_indexes = np.union1d(np.where(agree_outlier_indexes)[0], prediction_high_conf_outliers)\n",
    "#     if(len(all_outlier_indexes) > 1000):\n",
    "#         all_outlier_indexes = np.random.RandomState(1).permutation(all_outlier_indexes)[:1000]\n",
    "        \n",
    "#     all_outlier_indexes = np.union1d(np.where(agree_outlier_indexes)[0], prediction_high_conf_outliers)\n",
    "    print('num of outliers = {}'.format(np.shape(all_outlier_indexes)[0]))\n",
    "    all_inlier_indexes = np.setdiff1d(all_inlier_indexes, prediction_classifier_disagree)\n",
    "    \n",
    "    self_agree_index_list = []\n",
    "    if((len(all_outlier_indexes) == 0) or (len(all_inlier_indexes)/ len(all_outlier_indexes) > 1000)):\n",
    "        for i in range(0, len(index_range)):\n",
    "            if(index_range[i,1]-index_range[i,0] <= 6):\n",
    "                continue\n",
    "            temp_index = disagree_indexes[np.where(np.sum(L[disagree_indexes][:,index_range[i,0]: index_range[i,1]], axis = 1)==(index_range[i,1]-index_range[i,0]))[0]]\n",
    "            self_agree_index_list = np.union1d(self_agree_index_list, temp_index)\n",
    "        self_agree_index_list = [int(i) for i in self_agree_index_list]\n",
    "#     self_agree_index_list = np.random.RandomState(1).permutation(self_agree_index_list)[:500]\n",
    "    all_outlier_indexes = np.union1d(all_outlier_indexes, self_agree_index_list)\n",
    "    all_outlier_indexes = np.setdiff1d(all_outlier_indexes, prediction_classifier_disagree)\n",
    "    print('num of outliers = {}'.format(np.shape(all_outlier_indexes)[0]))\n",
    "    \n",
    "    from sklearn.preprocessing import RobustScaler\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    data_indexes = np.concatenate((all_inlier_indexes, all_outlier_indexes), axis = 0)\n",
    "    data_indexes = np.array([int(i) for i in data_indexes])\n",
    "    labels = np.concatenate((np.zeros(len(all_inlier_indexes)), np.ones(len(all_outlier_indexes))), axis = 0)\n",
    "    transformer = RobustScaler().fit(scores_for_training)\n",
    "    scores_transformed = transformer.transform(scores_for_training)\n",
    "    training_data = scores_transformed[data_indexes]\n",
    "    print('Training data shape: ', np.shape(training_data))\n",
    "\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.svm import SVC\n",
    "#     clf = SVC(gamma='auto', probability=True, random_state=0)\n",
    "#     clf.fit(training_data, labels)\n",
    "    clf = LogisticRegression(random_state=0, penalty='l2', max_iter=max_iter).fit(training_data, labels) \n",
    "    clf_predictions = clf.predict(scores_transformed)\n",
    "    clf_predict_proba = clf.predict_proba(scores_transformed)[:,1]\n",
    "    print(\"F-1 score from LR:\",metrics.f1_score(y, np.array([int(i) for i in clf_predict_proba > 0.5])))\n",
    "    \n",
    "    transformer = RobustScaler().fit(X)\n",
    "    X_transformed = transformer.transform(X)\n",
    "    X_training_data = X_transformed[data_indexes]\n",
    "    print(np.shape(X_training_data))\n",
    "    print(np.shape(labels))\n",
    "\n",
    "    clf_X = SVC(gamma='auto', probability=True, random_state=0)\n",
    "    clf_X.fit(X_training_data, labels)\n",
    "    clf_predictions_X = clf_X.predict(X_transformed)\n",
    "    clf_predict_proba_X = clf_X.predict_proba(X_transformed)[:,1]\n",
    "    print(\"F-1 score from SVM:\",metrics.f1_score(y, np.array([int(i) for i in clf_predict_proba_X > 0.5])))\n",
    "    cur_f1_scores.append(metrics.f1_score(y, np.array([int(i) for i in clf_predict_proba_X > 0.5])))\n",
    "        \n",
    "    agreed_outlier_indexes = np.where(np.sum(L,axis=1)==np.shape(L)[1])[0]\n",
    "    agreed_inlier_indexes = np.where(np.sum(L,axis=1)==0)[0]\n",
    "        \n",
    "    prediction_result_list.append(clf_predict_proba)\n",
    "    classifier_result_list.append(clf_predict_proba_X)\n",
    "    \n",
    "    prediction_list.append(np.array([int(i) for i in clf_predictions]))\n",
    "    \n",
    "    prediction_high_conf_outliers = np.intersect1d(np.where(prediction_result_list[-1] > high_confidence_threshold)[0],\n",
    "                                                   np.where(classifier_result_list[-1] > high_confidence_threshold)[0])\n",
    "    print('length of prediction_high_conf_outliers:' , len(prediction_high_conf_outliers))\n",
    "    prediction_high_conf_inliers = np.intersect1d(np.where(prediction_result_list[-1] < low_confidence_threshold)[0],\n",
    "                                                   np.where(classifier_result_list[-1] < low_confidence_threshold)[0])\n",
    "    print('length of prediction high conf inliers: ', len(prediction_high_conf_inliers))\n",
    "    \n",
    "    temp_prediction = np.array([int(i) for i in prediction_result_list[-1] > 0.5])\n",
    "    temp_classifier = np.array([int(i) for i in classifier_result_list[-1] > 0.5])\n",
    "    prediction_classifier_disagree = np.where(temp_prediction != temp_classifier)[0]\n",
    "    \n",
    "    if np.max(coef_index_range) >= 2:\n",
    "        if(len(prediction_high_conf_outliers) > 0 and len(prediction_high_conf_inliers) > 0):\n",
    "            new_data_indexes = np.concatenate((prediction_high_conf_outliers, prediction_high_conf_inliers), axis = 0)\n",
    "            new_data_indexes = np.array([int(i) for i in new_data_indexes])\n",
    "            new_labels = np.concatenate((np.ones(len(prediction_high_conf_outliers)), np.zeros(len(prediction_high_conf_inliers))), axis = 0)\n",
    "            clf_prune_2 = LogisticRegression(random_state=0, penalty='l2', max_iter=max_iter).fit(scores_transformed[new_data_indexes], new_labels) \n",
    "            print('Coef from both LR and SVM: ', clf_prune_2.coef_[0])\n",
    "            combined_coef = clf_prune_2.coef_[0]  \n",
    "        else:\n",
    "            print('Coef from normal training: ', clf.coef_[0])\n",
    "            combined_coef = clf.coef_[0]\n",
    "            print('Combined Coef: ',  combined_coef)\n",
    "\n",
    "        if(np.max(coef_index_range) >= 2 and len(set(combined_coef)) > 1):\n",
    "            cur_clf_coef = combined_coef \n",
    "            remain_indexes_after_cond = (generate_coef_decisions(cur_clf_coef) == 1)\n",
    "            print('Remain_indexes: ', remain_indexes_after_cond)\n",
    "            remain_params_tracking = remain_params_tracking[remain_indexes_after_cond]\n",
    "            print(remain_params_tracking)\n",
    "            \n",
    "            remain_indexes_after_cond_expanded = []\n",
    "            for i in range(0, len(coef_index_range)): #\n",
    "                s_e_range = coef_index_range[i,1]-coef_index_range[i,0]\n",
    "                s1, e1 = coef_index_range[i,0], coef_index_range[i,1]\n",
    "                s2, e2 = index_range[i,0], index_range[i,1]\n",
    "                saved_indexes = np.where(remain_indexes_after_cond[s1:e1] == 1)[0]\n",
    "                for j in range(N_size):\n",
    "                    remain_indexes_after_cond_expanded.extend(np.array(saved_indexes) + j * s_e_range + s2)\n",
    "\n",
    "            new_coef_index_range_seq = []\n",
    "            for i in range(0, len(coef_index_range)): #\n",
    "                s, e = coef_index_range[i,0], coef_index_range[i,1]\n",
    "                new_coef_index_range_seq.append(sum((remain_indexes_after_cond)[s:e]))\n",
    "\n",
    "            coef_index_range = []\n",
    "            index_range = []\n",
    "            cur_sum = 0\n",
    "            for i in range(0, len(new_coef_index_range_seq)):\n",
    "                coef_index_range.append([cur_sum, cur_sum + new_coef_index_range_seq[i]])\n",
    "                index_range.append([cur_sum * 6, 6 * (cur_sum + new_coef_index_range_seq[i])])\n",
    "                cur_sum += new_coef_index_range_seq[i]\n",
    "\n",
    "            coef_index_range = np.array(coef_index_range)\n",
    "            index_range = np.array(index_range)\n",
    "            print(coef_index_range)\n",
    "            print(index_range)\n",
    "\n",
    "            L=L[:,remain_indexes_after_cond_expanded]\n",
    "            scores_for_training = scores_for_training[:, remain_indexes_after_cond]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True, False, False, False, False, False, False, False])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Weights:  [0.25209058 0.25209058 0.25209058 0.25209058 0.25209058 0.25209058\n",
      " 0.25209058 0.25209058 0.25209058 0.25209058 0.25209058 0.25209058\n",
      " 0.25209058 0.25209058 0.25209058 0.25209058 0.25209058 0.25209058\n",
      " 0.25209058 0.25209058 0.25209058 0.25209058 0.25209058 0.25209058\n",
      " 0.25209058 0.25209058]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18])"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute weights\n",
    "coef = combined_coef\n",
    "print('Current Weights: ', coef)\n",
    "np.array(range(0,26))[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fdde2917400>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(len(cur_f1_scores)), cur_f1_scores, label='best-f1')\n",
    "# plt.plot(range(len(svm_diffs)), svm_diffs, label='svm_diffs')\n",
    "plt.plot(range(len(lr_svm_diffs)), lr_svm_diffs, label='lr_svm_diffs')\n",
    "# new_result = 0.1 * np.array(lr_svm_diffs) + 0.9 * np.array(high_conf_outlier_portion)\n",
    "# plt.plot(range(len(high_conf_outlier_portion)), high_conf_outlier_portion, label='high_conf_outlier_portion')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'smoothed_lr_svm_diffs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-c4342b4d90de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmoothed_lr_svm_diffs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmoothed_lr_svm_diffs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msmoothed_lr_svm_diffs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msmoothed_lr_svm_diffs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhigh_conf_outlier_portion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'smoothed_lr_svm_diffs' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(smoothed_lr_svm_diffs)-1):\n",
    "    if((smoothed_lr_svm_diffs[i+1] < smoothed_lr_svm_diffs[i]) and (smoothed_lr_svm_diffs[i+1] > 0)):\n",
    "        break\n",
    "print(i + 1)\n",
    "print(high_conf_outlier_portion[i + 1])\n",
    "print(cur_f1_scores[i + 1])\n",
    "print(np.max(cur_f1_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(high_conf_outlier_portion)):\n",
    "    print(i, high_conf_outlier_portion[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_list(L, factor = 2):\n",
    "    new_list = []\n",
    "    for i in range(0, len(L)):\n",
    "        all_sum = 0\n",
    "        count = 0\n",
    "        for j in range(max(0, i-factor), i):\n",
    "            all_sum += L[j]\n",
    "            count += 1\n",
    "        all_sum += L[i]\n",
    "        count += 1\n",
    "        for j in range(i + 1, min(len(L), i + 1 + factor)):\n",
    "            all_sum += L[j]\n",
    "            count += 1\n",
    "        new_list.append(all_sum/count)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_lr_svm_diffs = smooth_list(lr_svm_diffs, factor = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.12075112907059662,\n",
       " 0.19633943427620631,\n",
       " 0.2374613739006418,\n",
       " 0.2702638459710007,\n",
       " 0.2926075588305206,\n",
       " 0.30995959115759447,\n",
       " 0.3208937485143808,\n",
       " 0.324459234608985,\n",
       " 0.3265985262657476,\n",
       " 0.3304017114333254,\n",
       " 0.33586879011171855,\n",
       " 0.34181126693605896,\n",
       " 0.3456144521036368,\n",
       " 0.3487045400522938,\n",
       " 0.3534585215117661,\n",
       " 0.3577371048252912,\n",
       " 0.36058949370097454,\n",
       " 0.36344188257665794,\n",
       " 0.3658188733063941,\n",
       " 0.37152365105776086,\n",
       " 0.3762776325172332,\n",
       " 0.37699072973615405,\n",
       " 0.37556453529831235,\n",
       " 0.37152365105776086,\n",
       " 0.3750891371523651,\n",
       " 0.37532683622533874,\n",
       " 0.34347516044687426,\n",
       " 0.34941763727121466]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_conf_outlier_portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_diff = (min_max_diff-min(min_max_diff))/(max(min_max_diff)-min(min_max_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fa8064f74e0>"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl41NXZxvHvmeyBJIQk7JAAgrJEWRJAUbTivtu64VJBLXbVtr5tbW1rF63Wulatu6AWqxWtdd8QtK4kLBpkXwKELYGEDNknk/P+kUxIIOvMJPNLcn+uy4tkMpk5k5Gbk+d3znOMtRYREen6XKEegIiIBIcCXUSkm1Cgi4h0Ewp0EZFuQoEuItJNKNBFRLoJBbqISDehQBcR6SYU6CIi3UR4Zz5ZcnKyTUtL68ynFBHp8pYtW7bXWpvS2v06NdDT0tLIzs7uzKcUEenyjDFb23I/lVxERLoJBbqISDehQBcR6SY6tYbeFI/HQ15eHhUVFaEeijQQHR3NkCFDiIiICPVQRKSNQh7oeXl5xMXFkZaWhjEm1MMRwFrLvn37yMvLY/jw4aEejoi0UchLLhUVFSQlJSnMHcQYQ1JSkn5rEuliQh7ogMLcgfSeiHQ9jgh0EZG2KCyt4o2vd4Z6GI6lQBeRLuOV5Xn8+PkVFJVWhXoojqRAb6PXXnuNO++8M9TDaJfevXsDsHPnTi666KL622fNmsXRRx/Nfffdx9q1a5kwYQITJ05k06ZNoRqqSJsUldUG+f5yT4hH4kwhX+XSVZx33nmcd955oR6GXwYNGsTChQsB2L17N1lZWWzcuBGAO++8k4suuojf/va3oRyiSJsU1wV5sQK9SY4K9D++/g2rd7qD+phjB8Vz67njWrxPbm4uZ5xxBtOmTeOzzz4jMzOTOXPmcOutt5Kfn8+CBQtYvXo12dnZPPTQQ8yePZv4+Hiys7PZvXs3d911V6MZcENLlizh1ltvpU+fPuTk5HDJJZeQnp7OAw88QHl5Oa+++iojR47k9ddf57bbbqOqqoqkpCQWLFhA//79ufHGG0lKSuL3v/897777LrfffjtLlizB5Tr8l6stW7Zw+eWXU1JSwvnnn9/o9Z1zzjmsWrWK0047jR07djBhwgQuvPBCHnnkEcLCwli0aBGLFy8O7Ict0sHc5dV1fyrQm6KSS52NGzdy0003sXbtWtauXcvzzz/PJ598wt13381f/vKXw+6/a9cuPvnkE9544w1uvvnmFh/7q6++4tFHH2XNmjU899xzrF+/nqVLl3Ldddfx4IMPAnD88cfzxRdfsGLFCi677DLuuusuAO644w5efPFFFi9ezA033MC8efOaDHOAG2+8kR/84Afk5OQwcODAJu/z2muvMXLkSFauXMmtt97K97//fX72s58pzKVL0Ay9ZY6aobc2k+5Iw4cPJz09HYBx48Yxc+ZMjDGkp6eTm5t72P0vuOACXC4XY8eOZc+ePS0+dmZmZn3Ajhw5ktNOOw2A9PT0+iDNy8vj0ksvZdeuXVRVVdVv6ImNjeWJJ55gxowZ3HfffYwcObLZ5/n00095+eWXAbjqqqv41a9+1b4fgojDuSs8jf6UxjRDrxMVFVX/scvlqv/c5XJRXV3d4v2ttQE/9k9+8hN+/OMfk5OTw2OPPdZoU09OTg5JSUns3Nn6ci2tH5fuTDP0linQHaK4uJjBgwcD8Mwzz9TfvnXrVu655x5WrFjB22+/zZdfftnsY0yfPp0XXngBgAULFnTsgEVC4GAN/fBJlijQHeMPf/gDF198MZMnTyY5ORmonflfe+213H333QwaNIinnnqK6667rtkt+Q888AAPP/ww6enp7NixozOHL9LhrLX1F0M1Q2+aaa1cEEwZGRn20BOL1qxZw5gxYzptDNJ2em/ESSo8Xo763TsAnH30QB6+fFKIR9R5jDHLrLUZrd1PM3QR6RIazsq1bLFpjlrl0pXl5ORw1VVXNbotKiqqxZq3v26//XZeeumlRrddfPHF3HLLLUF/LhGncCvQW9VqoBtjngbOAfKttePrbusLvAikAbnAJdbaIn8HYa3t8qsz0tPTWblyZac81y233NLh4d2ZpTiRtvAtVUyMjVANvRltKbnMB8445LabgUXW2lHAorrP/RIdHc2+ffsUIA7iO+AiOjo61EMRqecL8aF9Y3FXaJVLU1qdoVtrPzbGpB1y8/nASXUfPwMsAfzaxTJkyBDy8vIoKCjw59ulg/iOoBNxCt9SxSGJMXyz090tfrMPNn9r6P2ttbvqPt4N9G/ujsaYucBcgGHDhh329YiICB1zJiKtqp+hJ8birbGUVXnpFaXLgA0FvMrF1tZKmq2XWGsft9ZmWGszUlJSAn06EemhfBdCh/SNBbQWvSn+BvoeY8xAgLo/84M3JBGRwxWXe4iJCCO5V2T959KYv4H+GnB13cdXA/8NznBERJrmrvCQEBNBfExE7ecK9MO0GujGmH8BnwNHGmPyjDHXAncCpxpjNgCn1H0uItJhiss9xMeEk1AX6JqhH64tq1xmNfOlmUEei4hIs9zl1bUz9Oi6GbqWLh5GW/9FpEsoLvcQHx2hGXoLFOgi0iX4aui9o2sLC6qhH06BLiJdQm0NPYIwlyEuOlwz9CYo0EXE8WpqLCWV1fUrXOKjI3QMXRMU6CLieAcqqrEW4uvKLQkxESq5NEGBLiKO55uN+y6IxseE6xi6JijQRcTxfPVyX8klIUYtdJuiQBcRx/OVVxJUQ2+RAl1EHK9+hh6tGXpLFOgi4nj1NfRYXw09grIqLx5vTSiH5TgKdBFxPN8F0IarXGpv1yy9IQW6iDhecbkHl4HedQdaxMfU7RZVP5dGFOgi4njuitpdor4j59TPpWkKdBFxvOJyT32Iw8GLoyq5NKZAFxHHc9d1WvTRDL1pCnQRcTzf4RY+9acWaS16Iwp0EXE8d0V1o5KLZuhNU6CLiOMVH1JyiQp3ERnmUj+XQyjQRcTx3IdcFDXGEK/doodRoIuIo1V4vFRW19TXzX3iY8JVQz+EAl1EHM0X2ocGunqiH06BLiKO5q5vzBXe6Pb4aAX6oRToIuJoxXUXPhOamKGrht6YAl1EHM1d3nTJpbaGrlUuDSnQRcTRDj1+zsc3Q7fWhmJYjqRAFxFHO/RwC5/46Ai8NZayKm8ohuVICnQRcbSDJZfGF0W1W/RwCnQRcbTicg/RES6iwsMa3a5+LodToIuIo7nLqw+rn0ODGXqZAt1HgS4ijnZoHxef+p7oWulST4EuIo7mrvC0PENXDb1eQIFujPmZMeYbY8wqY8y/jDHRwRqYiAgcPH7uUL6LpAr0g/wOdGPMYOAGIMNaOx4IAy4L1sBERODw4+d84nQM3WECLbmEAzHGmHAgFtgZ+JBERA5yl1cf1scFIMxliIsK1wy9Ab8D3Vq7A7gb2AbsAoqtte8Fa2AiIjU1ttkaOtQuXdSyxYMCKbkkAucDw4FBQC9jzJVN3G+uMSbbGJNdUFDg/0hFpMcpqarG2sP7uPjEq4VuI4GUXE4BtlhrC6y1HuAV4LhD72Stfdxam2GtzUhJSQng6USkp/GtMW9q2SJAQky4jqFrIJBA3wZMM8bEGmMMMBNYE5xhiYg0f7iFT3y0Wug2FEgN/UtgIbAcyKl7rMeDNC4RkYONuWIOvygKdacWqYZer+mfUhtZa28Fbg3SWEREGnE3c7iFjw6Kbkw7RUXEsdzNtM71SYiJoKzKi8db05nDciwFuog4Vv3hFrHN1dBriwxa6VJLgS4ijlVc7sEY6B3ZTA09Vg26GlKgi4hjues6Lbpcpsmvq0FXYwp0EXGs4nJPsytcoEELXQU6oEAXEQdzVzR9uIWPZuiNKdBFxLGaO9zCR8fQNaZAFxHHcjfTOtdHM/TGFOgi4litzdCjwl1EhrnUz6WOAl1EHMtd4Wl2DTqAMUa7RRtQoIuII1VWe6nw1DR5uEVD8THhqqHXUaCLiCO11sfFJ0E90esp0EXEkVprnesTH61A91Ggi4gjHWyd2/oMXTX0Wgp0EXGk1jot+tTW0LXKBRToIuJQvll3W2roxeUerLWdMSxHU6CLiCP5Zt0t9XKB2hm8t8ZSVuXtjGE5mgJdRByprSUX7RY9SIEuIo7kLvcQFe4iOiKsxfupn8tBCnQRcaTa1rktz86hwQy9TIGuQBcRR3JXtNyYy6e+J7pWuijQRcSZahtztXxBFFRDb0iBLiKO5C5v+XALH98qGO0WVaCLiEO1tYYeF60Zuo8CXUQcqa019DCXIS5KHRdBgS4iDlRTY3G3crhFQ+qJXkuBLiKOU1pVTY1tfdu/T3xMhE4tQoEuIg50sNNi66tcABJiwnVRFAW6iDhQWw+38ImPVskFFOgi4kDFbezj4pMQE6GLoijQRcSB2npakY8uitZSoIuI47S1F7pPQkwEZVVePN6ajhyW4wUU6MaYPsaYhcaYtcaYNcaYY4M1MBHpudxtPH7Ox9cioKdfGA10hv4A8I619ijgGGBN4EMSkZ7OXVGNMRAX1cZVLrFq0AXQtp9WE4wxCcAMYDaAtbYKqArOsESkJ3OXe4iLCsflMm26f7y2/wOBzdCHAwXAPGPMCmPMk8aYXkEal4j0YO429nHx8dXaVXLxXzgwCXjEWjsRKAVuPvROxpi5xphsY0x2QUFBAE8nIj1FcXnb+rj4xKuFLhBYoOcBedbaL+s+X0htwDdirX3cWpthrc1ISUkJ4OlEpKdwV7S9jws0mKH38LXofge6tXY3sN0Yc2TdTTOB1UEZlYj0aO2eoauGDgRwUbTOT4AFxphIYDMwJ/AhiUhP5y6vbnMfF4DoCBeRYa4e36AroEC31q4EMoI0FhERwHf8XNtn6MYY4mPCe/wMXTtFRcRRqqprKPd421VygboWuqqhi4g4R3v7uPjER0do2WKoByAi0pC7nX1cfBJiFOgKdBFxlPYebuGjjosKdBFxGF8/lvbP0MN7fC8XBbqIOEp7D7fw8Z1aZK3tiGF1CQp0EXGUQGro3hpLWZW3I4bVJSjQRcRRitvZC91H/VwU6CLiMO4KD5HhLqIjwtr1fernokAXEYdxt3OXqE99P5cyBbqIiCO4y6tJaOeSRWg4Q++5K10U6CLiKO6K9h1u4eNbt64auoiIQ7S3da6PTi1SoIuIw/hbQ49TT3QFuog4i78z9DCXIS4qXKtcREScwFqLu6J9h1s01NP7uSjQRcQxSqu8eGusXzN0qOuJ3oNPLVKgi4hjuP3s4+KTEBOui6IiIk5Q7GcfF5/46J59apECXUQcw+1nHxefBNXQRUScwd/WuT7xPfzUIgW6iDiGv4db+CTERFBa5cXjrQnmsLoMBbqIOIa/x8/5xEfXfl9PnaUr0EXEMXxBHOfvKpfYnt2gS4EuIo5RXO4hLiqcMJfx6/vje/j2fwW6iDiGv50WfXp6gy4Fuog4hrs8sEDv6cfQKdBFxDH8PdzCp6cfQ6dAFxHHKPazda6PaugiIg7hrvCvda5PdISLyDBXj23QpUAXEccoDrCGbowhPiZcM3R/GWPCjDErjDFvBGNAItIzebw1lFV5A5qhQ932f9XQ/XYjsCYIjyMiPdiBus1Avt2e/oqP7rn9XAIKdGPMEOBs4MngDEdEeqr61rmxgc3QE3pwg65AZ+j3A78EemYnHBEJmkAPt/DpycfQ+R3oxphzgHxr7bJW7jfXGJNtjMkuKCjw9+lEpJsL9HALn4SYcPVy8cN04DxjTC7wAnCyMeafh97JWvu4tTbDWpuRkpISwNOJSHfmu5AZyCoXqJ3hF5d7sNYGY1hdit+Bbq39tbV2iLU2DbgM+NBae2XQRiYiPUrwZugReGssZVXeYAyrS9E6dBFxBN9moGDU0KFn7hYNSqBba5dYa88JxmOJSM9UXO4hMsxFdERgsdST+7lohi4ijlDbOjccY/zrhe5T38+lTIEuIhISgTbm8jk4Q+95K10U6CLiCIH2QvfxnUeqGrqISIgEK9B78qlFCnQRcQR3RXXASxbh4AHTmqGLiIRIbQ09sMZcAGEuQ1xUuFa5iIiEgrUWd3lgh1s01NH9XIpKq8jdW9phj+8vBbqIhFxZlZfqGhuUGjrU9UTvwFOL/vTGai557HPHtRdQoItIyPnKI0GboUeHd9hFUWstn23aS/6BSrY4bJauQBeRkCsOUutcn4QOPLUor6icPe5KALJzizrkOfylQBeRkPOVR7pCDX3plkIAwl2GrNzCDnkOfwV+SVlEJED1M/SY4ERSR55alL21kPjocDLT+rJsq2boIiKNuIPUOtcnPjqC0iovHm/wD1NbuqWQjLS+TBnel817S9lbUhn05/CXAl1EQq7+cIug1dBrZ/oHgtzPZV9JJZsKSslISyQjLRHAUbN0BbqIhJyv5BIXhI1F0HE90bPrwntKWl/GD04gMtxFtoPq6Ap0EQk5d3k1vaPCCQ8LTiR1VD+XrC2FRIa7SB+SQFR4GMcMSagPeSdQoHdjvvWyG/YcCPVQRFpUHMRdotBxM/Ss3EImDOlDVHgYABlpfVm1o5gKjzOOu1Ogd0PWWt5fvYcLHv6Uy5/4kose/ZxNBSWhHpZIs9wVnqCVW+DgDD2YgV5WVc2qnW4yhyfW35aRmojHa/lq+/6gPU8gFOjdSE2N5a2cXZz190/43rPZFJV5+O3ZY4gIM8yZl8U+B12NF2ko6DP06OAfQ7di2368NZbMtL71t01OrQ13p5RdtA69G/DWWN74eicPfbiRDfkljEjuxT0XH8P5EwYRHuZicmoilz3+Bd97NpvnvzeN6IiwUA9ZpBF3uYehfWOD9ngdMUNfuqUQY2BS6sEZep/YSEb16+2YC6OaoXdhHm8NL2Vv55R7P+LGF1ZiDPx91kTe//mJfGfykPoLTBOHJfLAZRNYsX0/N/37K2pqnNVQSCSYnRYBoiNcRISZoDboyt5ayJgB8Yctrcyo22DkhL9XmqF3QZXVXhYuy+ORJZvIKypn7MB4Hr1yEqeNHYDL1fQBu2eMH8hvzhzD7W+tYWjfWG4+86hOHrVI89wV1UFbgw5gjCEhiNv/Pd4alm/dz6WZQw/7WkZqIv9auo0N+SUcOSAuKM/nLwV6B9hXUsnbq3YT5jLMmjIsqI+9vbCMy5/8gu2F5RwztA9/PG8cJx/Vr00npV93wnC2Fpby6EebSE2KDfrYRPxR7a2hpLI6aNv+feKjg9eg65udbso93vrNRA35aupZuYUK9O7CXeHh3VW7ef3rXXy6cS/eul+/Siurue6EEUF5juIyD3PmZ7G/zMP8OZmcODqlTUHuY4zhD+eOY0dROb99dRWD+sRw4uiUoIxNxF++3ZzBLLmAryd6cALdVyOf0uCCqM/QvjGkxEWxbGsRV05LDcrz+UuBHoCyqmoWrcnn9a92smRdAVXeGob2jeH6GSM4++iBPPThRm57cw394qM575hBAT1XZbWX6/+ZzdZ9pTx7zVSOHZnk1+OEh7l48PJJXPLo5/xowXJe+v6xjBkYH9DYRAIR7Na5PvExERSXVQXlsZZuKSQ1KZZ+8dGHfc0YQ2ZaoiM6LyrQ26my2stH6wp4/etdfLB6D+UeL/3jo7jq2FTOPWYQxwxJqJ8133fpBPaVLuWmf68kuVckxx2R7NdzWmv55cKv+WJzIQ9cNsHvMPfpHRXO07MzueDhT7lmfhav/mg6/Zv4H1WkMwT7cAufhJgItu0L/AAKay3ZW4s4+ah+zd5ncmpf3srZze7iCgYkhO7vkla5tNGu4nJu+U8OGbd9wNznlvHJhgK+PWkwL8ydxuc3z+R354xlwtA+jUog0RFhPPHdDEYk92buc8v4ZmexX89993vr+O/Knfzi9CM5f8LgoLyeAQnRPD07E3e5h2vmZ1Fa2XHHdYm05GDr3CDP0KPDcQehOdemglIKS6vIbKJ+7uP7WvbW0M7SFeitKC7zcMfbazjpb0t4KTuPU8f2Z/6cTJbecgq3X5jOtBFJza4sgdpZwvxrMomPDmf2vCy2F5a16/mf/3IbDy/exKwpQ/nhSSMDfTmNjB0Uz0NXTGLt7gPc8K8V9XV/kc4U7MMtfHyrXAI999NXSslson7uM2ZgPDERYSE/wUiB3owKj5dHlmzihLs+5PGPN3N2+kAW3XQi914ygZOO7EdEO5oIDUyI4ZlrplBVXcN3n15KYWnb6nqL1+bzu/+u4sTRKfz5/PHtugDaVt86sh9/PG8ci9bm86fXv3HcobfS/QX7cAuf+JgIvDWWsqrA+qxkbSkkuXckw5N7NXufiDAXE4f10Qzdaaq9NbywdBsn/W0Jf31nLZNTE3nrhhO499IJAe1kG9U/jqeuzmDn/nKumZ9FWVXLvwqu2lHMj55fzlED4nj4iklB60LXlCunpTJ3xgie+XwrD324kfIA/wKItEdH1tAh8N2iWVsLyUjt2+qEKiOtL6t3uikJYflSgV7HWss7q3Zz+v0fc/MrOQzsE82Lc6cxb86UoK0CyUjry99nTeTrvP38aMHyZk9TySsqY878LBJjI3l6dia9ozr+2vXNZxzFWekDuOf99Uz403tcMz+L57/cxh53RYc/t/RsxeUewl2GmCC3pAhGP5fdxRVsLywnc3jz5RafjNREaiys3Ba6Rl1a5QJ8sXkff31nLSu27WdkSi8evXIyp4/r3yEljtPHDeDPF4znlv+s4jev5HDXRUc3ep7iMg+z52VR4fGy4Lqpnbb6xOUyPDhrEldM3ccHa/bwwZo9fLg2H/4D6YMTOGVMf2aO6ce4QfEd8nORnsu37T/Y/1/Vz9DL/A/0pS2sPz/UxGF9cJnamvvxo/xb0RYovwPdGDMUeBboD1jgcWvtA8EaWGdYt/sAd769hsXrChgQH82d307nogY9UDrKFVNTyXdX8sCiDfSPj+b/Tj8SaLzW/JlrpjC6f+fuOgtzGaYfkcz0I5L5/Tlj2ZBfUhvuq/dw/6L13PfBegYmRDNzTD9mjunPsSOS1OirHbbtK+OjDQV8vL6AXcXl/OHccWS0ISi6u+JyT9BXuMDBQN8fQMklO7eQXpFhjBnY+t/FuOgIjhoQH9Ij6QKZoVcDN1lrlxtj4oBlxpj3rbWrgzS2DrO7uIJ731/HwmV59IoK5+Yzj2L2cWmdGk4/PWUU+QcqeGjxRvrHR3HltNT6teb3XzqB40aG5l94H2MMo/vHMbp/HD886Qj2llTy4dp8Fq3ZwyvLd/DPL7YRHeFi/KAExg+u/S99cAIjU3p1+D+IXUVpZTVfbN7Hx+sL+HjDXrbsrV0TPSQxBmvh8ie/5P5LJ3BW+sAQjzR0Siqryc4tYmS/5i84+is1OZbeUeH8d+UOTh83wK/HWLqlkEmpiW3+fzozLZGXluVR7a0Jyd8DvwPdWrsL2FX38QFjzBpgMODYQC+prOaxjzbxxP82462xzJk+nB9/6wgSe0V2+liMMfz5/PEUHKji9699w5J1BSxam88vTj+SCyYGZ615MCX3juKSjKFckjGUCo+Xz+uCKievmBeztjP/s1ygtsvd2IHxpA8+GPSj+vXuESFvrWX1Ljcfr9/Lx+sLyN5aiMdriYkI49iRSVx9bCozRqcwPLkXRWUernsmix89v5xbzhoTtPYQXc3d765jz4EK/nHlpKA/dnx0BNdMT+PvH27km53FjBuU0K7vLy73sG7PgXb9gzs5rS/PfL6VtbsPMH5w+54vGIJSQzfGpAETgS+D8XjB5qlbuXL/BxvYV1rFuccM4henHcmwpOD1X/ZHeJiLB2dN5Ionv2DR2vwOWWveEaIjwvjWkf341pG1O+e8NZbNBSXk7CgmZ0cx3+xws3BZHs98vhWAqHAXYwbGc87RA7l86jBiI7vXpRtrLU99soXHPt5MwYHaQ0SOGhDHNdOHM2N0ChlpifVHlvn07RXJ89+bxk9fWMltb65hx/5yfnv2WMJa2NPQ3azYVsQzn+fy3WmpTBrW/KadQFx7wgjmf5bLfe+v58mrM9v1vcu3FmEtTTbkao5vg1FWbmHXDHRjTG/gZeCn1lp3E1+fC8wFGDasc7v7WWt5b/Ue/vrOWjYXlDJleF+eOmsME4b26dRxtCQmMox5s6ewaO0ezjtmUJe84BjmMozqH8eo/nF8e9IQoDbkt+wtZVVdyGdvLeK2N9fwjyWbuPb44Vx1bGrAvTv2llTy8rI81u4+wKljay/aHhqcHa2mxvKnN1Yz/7NcThiVzC9PP5IZo1PadDE7OiKMh6+YxO1vruHpT7ewa38F9182oUdcl/B4a/j1KzkMiI/mF2d0XCvnhJgI5s4Ywd3vrWfl9v3t+ru/NLeQiDDDxKFtD/SBCTEM7hND9tYi5kwf7s+QA2IC2UhijIkA3gDetdbe29r9MzIybHZ2tt/P1x7LtxVxx1tryMotYmRKL3595hhmjmlbm1npGMu2FvLQhxtZvK6AuOhwZh+Xxpzpw+nbjpJXTY3l0017eWHpdt5bvRuP19Zv8U6IieC8YwZx0eQhHN2gp05Hqaz28vN/f8WbX+/i2uOHc8tZY1rcNdySpz7Zwm1vrmbi0D48eXVmu34mXdE/lmzkrnfW8cR3Mzh1bP8Ofa6SympO+OuHjB+cwHPXTm3z9130yGd4reU/P5zerue78YUVfLF5H1/8embQ/h80xiyz1ma0dr9AVrkY4ClgTVvCvKPU1Fj2lVaRf6CCggOV5B+o5KN1BbyZs4vk3lHcfuF4Ls0Y2iNquE43ObUv8+ZMYdWOYh76cCMPfriRpz7ZwpXTUrnuhOH0i2t+VrvHXcFL2dt5MXs72wvLSYyN4LvHpnFZ5lBGpPTm0417Wbgsj39nb+e5L7Yyql9vvjN5CBdOHNwhSz8PVHi4/rllfLZpH7856yjmzgisVHbt8cMZlBDNjS+u5DuPfMb8OZmkJgX/QqET5O4t5YEPNnDm+AEdHuZQ24zuByeN5C9vrSUrt7DFLfw+FR4vX+cVM3t6WrufLyM1kf+u3EleUXlQj9VrC79n6MaY44H/ATmAb4fMb6y1bzX3Pf7O0D/ftI+N+QfqA7v2v9oA31tSdVgPktjIML53wgjmzhhBr07YlCP+Wb/nAP9YvJHXvtpJeJiLyzI3/gnYAAALp0lEQVSHcv2JIxncJwaoLdssWZfPv5ZuZ/G6fLw1lmNHJDFr6jBOH9e/yfKKu8LDm1/vYuGyPJZtLcJlYMboFL4zaQinju0flHJGvruC2fOyWL/nAHdddHR9mSkYsnMLue7ZbMKM4anZmY4qDwaDtZYrnvySnB3FfPDzEzttn0V5lZcZf1vMiORevDB3Wqsz56VbCrnksc/9+g1izS43Zz7wP+679BgunBic/zfaOkMPqOTSXv4G+jXzs/hwbT4uA0m9o+gXF0VKXO2f/eKi6Rff8Lbazzu7lir+y91be4rSy8vzsBa+PWkwAxJieCl7O7uKK0juHcVFk4dwWeZQ0lrop3GozQUlvLJ8By8vz2NXcQVx0eGce8wgrpyaythB/u3+3VxQUt+P5x9XTOKkI5tvqeqvTQUlzJ63lIIDlTw4a1KnzGI7y8JlefzfS19x+4XjuWJq5x4GMe/TLfzx9dUsuG4q01tpZf3w4o387d11rPjdqe1eBeetsUz443ucO2EQf7kwPZAh1+tWgb5jfzkRLkPfXpEqnXRjO/eX8/jHm/nX0m1UeWuYMSqFWVOGMnNM/3Y1QzuUt8by+aZ9vLw8j7dX7aLCU8PU4X2ZM304p47t3+aVJV9t38+c+VkAzJudyTEdOHsuOFDJtc9ksWpHMX88bxznHD2IorIqiso87C+rorC0iv1lnrrbqigqrf14f5mHsYPi+b/Tj6z/Tccp9pVUMvPejzgipTf/vv5Yv683+KvC4+Vbdy9hYEI0L//guBZn6bPnLWVHUTnv//xEv57r6qeXsru4gnd/NsPf4TbSrQJdepai0iqqvDUd8uv4/rIqXszazrOfb2XH/nKGJMZw9bFpXJI5tMXmUEvW5fODfy4nOS6SZ6+Z2mLnvWApq6rmJ8+vYNHa/GbvE+4y9ImNJDE2gsRekcRFhfPJxr0YA98/cSTXzxhJTKQzflv96QsreDNnF2/dcAKjOnkXtM+CL7dyy39WMW92Jt9q5sCKYMywH1y0gXveX89Xvz+NhNjAd8F2+EVRkY7SkRu9+sRGcv2JI7n2+OG8v3oP8z7N5fa31nDfB+v5zqQhzJ6exsiU3o2+55Xlefxy4deM7h/H/GsyW7x4G0yxkeE8dtVkFi7Lo6zKS2KvCPrERtI3NpLE2Ej69IogLir8sJnmjv3l/OWtNdz/wQZeys7j12cdxdnpA0O6wuuj9QW8unInN8wcFbIwB7h48lAe/WgT976/npOObPpM3rW73RyorG5T/5bm+Fo6LN9W1Ow/HB1BgS49UniYizPTB3Jm+kBW7Shm3qe5vJhVu0LmxNEpzJmexoxRKTzxv83c8fZajhuZxGNXTSYuyOdetmWcl01p3/6NwX1iePjySXx32j7++Ppqfvz8Cp5N28rvzx0bks0uZVXV3PKfHEak9OJH3wrtxrnIcBc3nDyKXyz8mvdW72myJYDvkIr2bCg61IShfQh3GbJyCzs10FWQlh5v/OAE7rnkGD69+WR+dspoVu9yM3teFsfeuYg73l7LOUcPZN6czE4P80BNHZHE6z85nr9cmM7GghLOfegTfv3K1+wrqezUcTzwwQbyisq548J0RyxWuHDiYEYk9+Le99ZT08QpXUtzCxmUEM2QRP+XHMZEhjFucALZndyoS4EuUiclLoobTxnFp786mfsvncCwvrFcf+II/n7ZREcEkT/CXIbLpw5j8f+dxDXTh/NSdh4n3b2EJ/+3udl+/MG0akcxT36yhVlThjJ1RGCHmwdLeJiLG08Zxbo9B3gzZ1ejr1lrydpS2Kb+563JSE3kq+37qaru+J+zjwJd5BCR4S4umDiYl75/HL8+0//dn06SEBPB784Zyzs/ncGkYYnc9uYazrj/Y15dsYMV24rYXlgW9JOqquu29yfGRnLzGWOC+tiBOvfoQYzu35v7PlhPdYN/2LYXlpN/oDIobY0z0xKprK5hlZ+Hw/tDNXSRHuSIfr2ZPyeTxevy+fMba/jpiysbfb1XZBjJcVEk944iqVdk/cfJvSPpFxdFalIvhif3atMGrfmf5ZKzo5iHLp8YlJUeweRyGX52ymh+sGA5/125k+9Mrt0A1J4DLVozObX2MbJzCzus+dihFOgiPYwxhpOP6s/xR6SwbvcB9pZUUlBSyd6SSvYeqGJfae3HW/eVsWxrEYVlVTRc3WwMDEqIYURKL0am9GZESi9GJNf+OTAhGmMM2wvLuOe99cw8qh9nO7Tf++njBjBuUDwPLNrAeRMGERHmImtLIQkxEYzq17v1B2hFSlwUaUmxZOcWMTc4y9FbpUAX6aEiw12kD2l91Uu1t4aiMg973BVs2VvK5oJSNu8tYXNBKS9lb6e0QakmJiKM4cm9qPB4MQb+dMF4xzbEc7kMPz91NNc+k83CZXnMmjKs7kDoxKCV2San9mXJunystZ3yc1Cgi0iLwsNcpNS11jh02aO1lvwDlWwqKGFTQSmbC2qDflthGbeeO9Zxu1UPdfJR/ZgwtA8PLtrAiaNT2FxQyiUZQ4P2+Jlpiby8PI8te0sZkRL4rL81CnQR8Zsxhv7x0fSPjw75sYn+MMZw02mjueqppfxy4dcAberG2Fa+tezZuUWdEuha5SIiPdrxRyQzJa0vn2zcS1S4i/Qgbr4amdKbxNgIsuoutnY0BbqI9Gi+WTrU7vCMDA9eLBpjmJyayLJO2mCkQBeRHm/qiCR+eFJtj59gy0jry+a9pezthB26qqGLiAC/7KCzTY8bmcTZ6QMpq/RCB5fRFegiIh3o6CF9ePiKSZ3yXCq5iIh0Ewp0EZFuQoEuItJNKNBFRLoJBbqISDehQBcR6SYU6CIi3YQCXUSkmzDWHn5Iaoc9mTEFwFY/vz0Z2BvE4ThNd3990P1fo15f1+fU15hqrU1p7U6dGuiBMMZkW2szQj2OjtLdXx90/9eo19f1dfXXqJKLiEg3oUAXEekmulKgPx7qAXSw7v76oPu/Rr2+rq9Lv8YuU0MXEZGWdaUZuoiItKBLBLox5gxjzDpjzEZjzM2hHk+wGWNyjTE5xpiVxpjsUI8nGIwxTxtj8o0xqxrc1tcY874xZkPdn4mhHGMgmnl9fzDG7Kh7H1caY84K5RgDYYwZaoxZbIxZbYz5xhhzY93t3eI9bOH1den30PElF2NMGLAeOBXIA7KAWdba1SEdWBAZY3KBDGutE9e/+sUYMwMoAZ611o6vu+0uoNBae2fdP8yJ1tpfhXKc/mrm9f0BKLHW3h3KsQWDMWYgMNBau9wYEwcsAy4AZtMN3sMWXt8ldOH3sCvM0KcAG621m621VcALwPkhHpO0wlr7MXDoUefnA8/UffwMtX+BuqRmXl+3Ya3dZa1dXvfxAWANMJhu8h628Pq6tK4Q6IOB7Q0+z6Mb/OAPYYH3jDHLjDFzQz2YDtTfWrur7uPdQP9QDqaD/NgY83VdSaZLliMOZYxJAyYCX9IN38NDXh904fewKwR6T3C8tXYScCbwo7pf57s1W1vrc3a9r/0eAUYCE4BdwD2hHU7gjDG9gZeBn1pr3Q2/1h3ewyZeX5d+D7tCoO8Ahjb4fEjdbd2GtXZH3Z/5wH+oLTN1R3vqape+GmZ+iMcTVNbaPdZar7W2BniCLv4+GmMiqA27BdbaV+pu7jbvYVOvr6u/h10h0LOAUcaY4caYSOAy4LUQjylojDG96i7KYIzpBZwGrGr5u7qs14Cr6z6+GvhvCMcSdL6gq3MhXfh9NMYY4ClgjbX23gZf6hbvYXOvr6u/h45f5QJQt3TofiAMeNpae3uIhxQ0xpgR1M7KAcKB57vD6zPG/As4idrudXuAW4FXgX8Dw6jtunmJtbZLXlhs5vWdRO2v6hbIBa5vUG/uUowxxwP/A3KAmrqbf0NtnbnLv4ctvL5ZdOH3sEsEuoiItK4rlFxERKQNFOgiIt2EAl1EpJtQoIuIdBMKdBGRbkKBLiLSTSjQRUS6CQW6iEg38f9XL4YCnnz5CQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(len(cur_f1_scores)), min_max_diff, label='min_max_diff')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3997050147492625, 0.47187293183322304, 0.48916797488226066, 0.4958627030340177, 0.5050746268656716, 0.5080858571008527, 0.5106631609699094, 0.5132382892057027, 0.5130737943056363, 0.5159235668789809, 0.513840830449827, 0.5163511187607573, 0.5164051355206847, 0.5213189312109152, 0.520656479909451, 0.5221313786298281, 0.519831223628692, 0.5204941044357103, 0.5205020920502091, 0.5192200557103064, 0.5204872646733112, 0.51974592653963, 0.5192943770672547, 0.5177637014596529, 0.517062517062517, 0.5172320217096337, 0.5186385737439221, 0.5199150292087095]\n",
      "[0.12075112907059662, 0.19633943427620631, 0.2374613739006418, 0.2702638459710007, 0.2926075588305206, 0.30995959115759447, 0.3208937485143808, 0.324459234608985, 0.3265985262657476, 0.3304017114333254, 0.33586879011171855, 0.34181126693605896, 0.3456144521036368, 0.3487045400522938, 0.3534585215117661, 0.3577371048252912, 0.36058949370097454, 0.36344188257665794, 0.3658188733063941, 0.37152365105776086, 0.3762776325172332, 0.37699072973615405, 0.37556453529831235, 0.37152365105776086, 0.3750891371523651, 0.37532683622533874, 0.34347516044687426, 0.34941763727121466]\n"
     ]
    }
   ],
   "source": [
    "print(cur_f1_scores)\n",
    "print(high_conf_outlier_portion)\n",
    "# print(min_max_diff)\n",
    "# print(( np.array(lr_svm_diffs))[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of difference:  284\n",
      "0.43511450381679384\n",
      "0.3398058252427185\n",
      "0.5432780847145489\n",
      "0.5432780847145489\n",
      "4167\n",
      "359\n",
      "0.687603305785124\n",
      "0.687603305785124\n"
     ]
    }
   ],
   "source": [
    "query_index = 0\n",
    "for query_index in range(np.shape(prediction_result_list)[0]):\n",
    "    prediction = np.array(prediction_result_list)[query_index,:]\n",
    "    classify = np.array(classifier_result_list)[query_index,:]\n",
    "    prediction = np.array([int(i) for i in prediction>0.5])\n",
    "    classify = np.array([int(i) for i in classify>0.5])\n",
    "    print('number of difference: ', sum(prediction!=classify))\n",
    "    diff = np.where(prediction != classify)[0]\n",
    "    print(metrics.f1_score(y[diff], classify[diff]))\n",
    "    print(metrics.f1_score(y[diff], prediction[diff]))\n",
    "    \n",
    "    same = np.where(prediction == classify)[0]\n",
    "    print(metrics.f1_score(y[same], classify[same]))\n",
    "    print(metrics.f1_score(y[same], prediction[same]))\n",
    "    same = np.intersect1d(np.intersect1d(np.where(prediction == classify)[0], \n",
    "                          np.union1d(np.where(np.array(classifier_result_list)[query_index,:] > 0.99)[0], \n",
    "                                    np.where(np.array(classifier_result_list)[query_index,:] < 0.01)[0])\n",
    "                         ),\n",
    "                          np.union1d(np.where(np.array(prediction_result_list)[query_index,:] > 0.99)[0], \n",
    "                                    np.where(np.array(prediction_result_list)[query_index,:] < 0.01)[0])\n",
    "                         )\n",
    "    print(len(same))\n",
    "    print(sum(classify[same]))\n",
    "    print(metrics.f1_score(y[same], classify[same]))\n",
    "    print(metrics.f1_score(y[same], prediction[same]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1068,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.00022781, 0.00043399, 0.0012116 , ..., 0.00124072, 0.00250151,\n",
       "        0.18181332]),\n",
       " array([1.65645778e-04, 2.85762514e-04, 6.78681483e-04, ...,\n",
       "        9.10293632e-04, 1.24575154e-03, 9.37642671e-01]),\n",
       " array([1.10471336e-04, 2.30912512e-04, 6.12271583e-04, ...,\n",
       "        9.70832357e-04, 1.34142865e-03, 9.22294895e-01]),\n",
       " array([1.18120320e-04, 1.34857469e-04, 5.02687133e-04, ...,\n",
       "        7.41587123e-04, 1.22642603e-03, 9.70676185e-01]),\n",
       " array([1.45045644e-04, 1.46358133e-04, 3.52544136e-04, ...,\n",
       "        1.90195636e-03, 1.56679354e-03, 9.75049665e-01]),\n",
       " array([1.62525249e-04, 1.65351948e-04, 4.43056035e-04, ...,\n",
       "        2.07967560e-03, 1.64262618e-03, 9.79657404e-01])]"
      ]
     },
     "execution_count": 1068,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iterative train LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 922,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L shape (5473, 156)\n",
      "All agree, Number of outliers = 55\n",
      "All agree, Number of inliers = 3788\n",
      "num of inliers = 3788\n",
      "num of outliers = 362\n",
      "(4150, 156)\n",
      "(4150,)\n",
      "F-1 score: 0.5108359133126935\n",
      "max/min =  -1.074120537432934\n",
      "0\n",
      "[[ 0 36]\n",
      " [36 66]\n",
      " [66 84]\n",
      " [84 90]]\n",
      "400\n",
      "3876\n",
      "L shape (5473, 90)\n",
      "All agree, Number of outliers = 58\n",
      "All agree, Number of inliers = 3807\n",
      "num of inliers = 4000\n",
      "num of outliers = 440\n",
      "(4440, 90)\n",
      "(4440,)\n",
      "F-1 score: 0.532448377581121\n",
      "max/min =  -2.345950438514712\n",
      "0\n",
      "[[ 0 30]\n",
      " [30 48]\n",
      " [48 54]\n",
      " [54 60]]\n",
      "451\n",
      "3887\n",
      "L shape (5473, 60)\n",
      "All agree, Number of outliers = 58\n",
      "All agree, Number of inliers = 3842\n",
      "num of inliers = 4041\n",
      "num of outliers = 501\n",
      "(4542, 60)\n",
      "(4542,)\n",
      "F-1 score: 0.5511811023622047\n",
      "max/min =  -2.8474950545048823\n",
      "0\n",
      "[[ 0 30]\n",
      " [30 36]\n",
      " [36 42]\n",
      " [42 48]]\n",
      "508\n",
      "3909\n",
      "L shape (5473, 48)\n",
      "All agree, Number of outliers = 58\n",
      "All agree, Number of inliers = 3877\n",
      "num of inliers = 4096\n",
      "num of outliers = 546\n",
      "(4642, 48)\n",
      "(4642,)\n",
      "F-1 score: 0.5501066098081023\n",
      "max/min =  -14.184708256880233\n",
      "0\n",
      "[[ 0 18]\n",
      " [18 18]\n",
      " [18 24]\n",
      " [24 30]]\n",
      "507\n",
      "3955\n",
      "L shape (5473, 30)\n",
      "All agree, Number of outliers = 94\n",
      "All agree, Number of inliers = 4155\n",
      "num of inliers = 4226\n",
      "num of outliers = 545\n",
      "(4771, 30)\n",
      "(4771,)\n",
      "F-1 score: 0.5579024034959942\n",
      "max/min =  19.12498862037904\n",
      "0\n",
      "[[ 0 18]\n",
      " [18 18]\n",
      " [18 24]\n",
      " [24 30]]\n",
      "482\n",
      "4052\n",
      "L shape (5473, 30)\n",
      "All agree, Number of outliers = 94\n",
      "All agree, Number of inliers = 4155\n",
      "num of inliers = 4253\n",
      "num of outliers = 533\n",
      "(4786, 30)\n",
      "(4786,)\n",
      "F-1 score: 0.5605282465150404\n",
      "max/min =  37.45262860119805\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "breakcond = 0\n",
    "for i in range(0, 26):\n",
    "    print('L shape', np.shape(L))\n",
    "    num_methods = np.shape(L)[1]\n",
    "\n",
    "    # agree_outlier_indexes = (np.sum(L,axis=1)==np.shape(L)[1])\n",
    "    # print('All agree, Number of outliers = {}'.format(sum(agree_outlier_indexes)))\n",
    "    # agree_inlier_indexes = (np.sum(L,axis=1)==0)\n",
    "    # print('All agree, Number of inliers = {}'.format(sum(agree_inlier_indexes)))\n",
    "\n",
    "    # all_inlier_indexes = np.union1d(np.setdiff1d(np.where(agree_inlier_indexes)[0], prediction_high_conf_outliers), prediction_high_conf_inliers)\n",
    "    # print('num of inliers = {}'.format(np.shape(all_inlier_indexes)[0]))\n",
    "    # all_outlier_indexes = np.union1d(np.where(agree_outlier_indexes)[0], prediction_high_conf_outliers)\n",
    "    # print('num of outliers = {}'.format(np.shape(all_outlier_indexes)[0]))\n",
    "\n",
    "    # disagree_indexes = np.where(np.logical_or(np.sum(L,axis = 1)==0, np.sum(L,axis = 1)==num_methods)==0)[0]\n",
    "\n",
    "    ########################################################################\n",
    "\n",
    "    agree_outlier_indexes = np.sum(L,axis=1)==np.shape(L)[1]\n",
    "    print('All agree, Number of outliers = {}'.format(sum(agree_outlier_indexes)))\n",
    "    agree_inlier_indexes = np.sum(L,axis=1)==0\n",
    "    print('All agree, Number of inliers = {}'.format(sum(agree_inlier_indexes)))\n",
    "\n",
    "    disagree_indexes = np.where(np.logical_or(np.sum(L,axis = 1)==0, np.sum(L,axis = 1)==num_methods)==0)[0]\n",
    "    # print('Number of disagreed points = {}'.format(len(disagree_indexes)))\n",
    "    # print('Number of disagreed points (true outliers) = {}'.format(sum(y[disagree_indexes] == 1)))\n",
    "    # print('Number of disagreed points (true inliers) = {}'.format(sum(y[disagree_indexes] == 0)))\n",
    "\n",
    "    self_agree_index_list = []\n",
    "    for i in range(0, len(index_range)):\n",
    "        if(index_range[i,1]-index_range[i,0] < 1):\n",
    "            continue\n",
    "        temp_index = disagree_indexes[np.where(np.sum(L[disagree_indexes][:,index_range[i,0]: index_range[i,1]], axis = 1)==(index_range[i,1]-index_range[i,0]))[0]]\n",
    "        self_agree_index_list = np.union1d(self_agree_index_list, temp_index)\n",
    "    self_agree_index_list = [int(i) for i in self_agree_index_list]\n",
    "    # print(sum(y[self_agree_index_list])/len(self_agree_index_list))\n",
    "    # print(sum(y[self_agree_index_list]))\n",
    "    # print(len(self_agree_index_list))\n",
    "\n",
    "#     all_inlier_indexes = np.where(agree_inlier_indexes)[0]\n",
    "    all_inlier_indexes = np.union1d(np.setdiff1d(np.where(agree_inlier_indexes)[0], prediction_high_conf_outliers), prediction_high_conf_inliers)\n",
    "    print('num of inliers = {}'.format(np.shape(all_inlier_indexes)[0]))\n",
    "#     all_outlier_indexes = np.union1d(np.where(agree_outlier_indexes)[0], self_agree_index_list)\n",
    "    all_outlier_indexes = np.union1d(np.union1d(np.where(agree_outlier_indexes)[0], self_agree_index_list), prediction_high_conf_outliers)\n",
    "    print('num of outliers = {}'.format(np.shape(all_outlier_indexes)[0]))\n",
    "\n",
    "    ####################################################################\n",
    "#     agree_outlier_indexes = np.sum(L,axis=1)==np.shape(L)[1]\n",
    "#     print('All agree, Number of outliers = {}'.format(sum(agree_outlier_indexes)))\n",
    "#     agree_inlier_indexes = np.sum(L,axis=1)==0\n",
    "#     print('All agree, Number of inliers = {}'.format(sum(agree_inlier_indexes)))\n",
    "\n",
    "#     disagree_indexes = np.where(np.logical_or(np.sum(L,axis = 1)==0, np.sum(L,axis = 1)==np.shape(L)[1])==0)[0]\n",
    "#     # print('Number of disagreed points = {}'.format(len(disagree_indexes)))\n",
    "#     # print('Number of disagreed points (true outliers) = {}'.format(sum(y[disagree_indexes] == 1)))\n",
    "#     # print('Number of disagreed points (true inliers) = {}'.format(sum(y[disagree_indexes] == 0)))\n",
    "\n",
    "#     self_agree_index_list = []\n",
    "#     for i in range(0, len(index_range)):\n",
    "#         if(index_range[i,1]-index_range[i,0] < 1):\n",
    "#             continue\n",
    "#         temp_index = disagree_indexes[np.where(np.sum(L[disagree_indexes][:,index_range[i,0]: index_range[i,1]], axis = 1)==(index_range[i,1]-index_range[i,0]))[0]]\n",
    "#         self_agree_index_list = np.union1d(self_agree_index_list, temp_index)\n",
    "\n",
    "#     if(np.shape(prediction_result_list)[0] >= 1):\n",
    "#         temp_prediction_result = np.array(prediction_result_list).T\n",
    "#         temp_index = disagree_indexes[np.where(np.sum(temp_prediction_result[disagree_indexes], axis = 1)==np.shape(prediction_result_list)[0])[0]]\n",
    "#         self_agree_index_list = np.union1d(self_agree_index_list, temp_index)\n",
    "#     self_agree_index_list = [int(i) for i in self_agree_index_list]\n",
    "#     # print(sum(y[self_agree_index_list])/len(self_agree_index_list))\n",
    "#     # print(sum(y[self_agree_index_list]))\n",
    "#     # print(len(self_agree_index_list))\n",
    "\n",
    "#     all_inlier_indexes = np.union1d(np.setdiff1d(np.where(agree_inlier_indexes)[0], prediction_high_conf_outliers), prediction_high_conf_inliers)\n",
    "#     print('num of inliers = {}'.format(np.shape(all_inlier_indexes)[0]))\n",
    "#     all_outlier_indexes = np.union1d(np.union1d(np.where(agree_outlier_indexes)[0], self_agree_index_list), prediction_high_conf_outliers)\n",
    "#     print('num of outliers = {}'.format(np.shape(all_outlier_indexes)[0]))\n",
    "    \n",
    "    \n",
    "    from sklearn.preprocessing import RobustScaler\n",
    "    data_indexes = np.concatenate((all_inlier_indexes, all_outlier_indexes), axis = 0)\n",
    "    data_indexes = np.array([int(i) for i in data_indexes])\n",
    "    labels = np.concatenate((np.zeros(len(all_inlier_indexes)), np.ones(len(all_outlier_indexes))), axis = 0)\n",
    "    transformer = RobustScaler().fit(scores)\n",
    "    scores_transformed = transformer.transform(scores)\n",
    "    training_data = scores_transformed[data_indexes]\n",
    "    print(np.shape(training_data))\n",
    "    print(np.shape(labels))\n",
    "\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    clf = LogisticRegression(random_state=0, penalty='l2').fit(training_data, labels)\n",
    "    clf_predictions = clf.predict(scores_transformed)\n",
    "    clf_predict_proba = clf.predict_proba(scores_transformed)[:,1]\n",
    "    print(\"F-1 score:\",metrics.f1_score(y, clf_predictions))\n",
    "    cur_f1_scores.append(metrics.f1_score(y, clf_predictions))\n",
    "\n",
    "    # from sklearn.svm import LinearSVC\n",
    "    # clf = LinearSVC()\n",
    "    # clf.fit(training_data, labels)\n",
    "    # clf_predictions = clf.predict(scores_transformed)\n",
    "    # clf_predict_proba = clf.decision_function(scores_transformed)\n",
    "    # print(\"F-1 score:\",metrics.f1_score(y, clf_predictions))\n",
    "    # print('F-1 score for disagreed:', metrics.f1_score(y[disagree_indexes], clf_predictions[disagree_indexes]))\n",
    "    # cur_f1_scores.append(metrics.f1_score(y, clf_predictions))\n",
    "\n",
    "\n",
    "    #     print('F-1 score for disagreed:', metrics.f1_score(y[disagree_indexes], clf_predictions[disagree_indexes]))\n",
    "    #     print('F-1 score for agreed inliers:', metrics.f1_score(y[agree_inlier_indexes], clf_predictions[agree_inlier_indexes], pos_label=0))\n",
    "    #     print('F-1 score or agreed outliers:', metrics.f1_score(y[agree_outlier_indexes], clf_predictions[agree_outlier_indexes]))\n",
    "\n",
    "    agreed_outlier_indexes = np.where(np.sum(L,axis=1)==np.shape(L)[1])[0]\n",
    "    agreed_inlier_indexes = np.where(np.sum(L,axis=1)==0)[0]\n",
    "    minmax_diff = max(clf.coef_[0])/min(clf.coef_[0])\n",
    "    print('max/min = ', minmax_diff)\n",
    "    if(minmax_diff > 0 and minmax_diff < 10):\n",
    "        break\n",
    "        \n",
    "#     print(clf.coef_[0])\n",
    "#     print(np.std(scores_transformed, 0)*clf.coef_[0])\n",
    "    prediction_result_list.append(clf_predict_proba)\n",
    "    prediction_list.append(np.array([int(i) for i in clf_predictions]))\n",
    "\n",
    "    cutoff = #max(0, np.sort(clf.coef_[0])[0])\n",
    "    print(cutoff)\n",
    "    cur_clf_coef = clf.coef_[0] #* np.std(scores_transformed, 0)\n",
    "    remain_indexes_after_cond = (cur_clf_coef > cutoff) #np.logical_and(cur_clf_coef > cutoff, abs(cur_clf_coef) > 0.01) # # \n",
    "    if sum(remain_indexes_after_cond) == len(remain_indexes_after_cond):\n",
    "        if(breakcond == 1):\n",
    "            break\n",
    "        breakcond = 1\n",
    "    new_index_range_seq = []\n",
    "    for i in range(0, len(index_range)): #\n",
    "        s, e = index_range[i,0], index_range[i,1]\n",
    "        new_index_range_seq.append(sum((remain_indexes_after_cond)[s:e]))\n",
    "\n",
    "    index_range = []\n",
    "    cur_sum = 0\n",
    "    for i in range(0, len(new_index_range_seq)):\n",
    "        index_range.append([cur_sum, cur_sum + new_index_range_seq[i]])\n",
    "        cur_sum += new_index_range_seq[i]\n",
    "\n",
    "    index_range = np.array(index_range)\n",
    "    print(index_range)\n",
    "\n",
    "    L=L[:,remain_indexes_after_cond]\n",
    "    scores = scores[:, remain_indexes_after_cond]\n",
    "    \n",
    "    prediction_high_conf_outliers = np.where(prediction_result_list[-1] > 0.99)[0]\n",
    "    print(len(prediction_high_conf_outliers))\n",
    "    prediction_high_conf_inliers = np.where(prediction_result_list[-1] < 0.01)[0]\n",
    "    print(len(prediction_high_conf_inliers))\n",
    "    # coef_remain_index = np.array(coef_remain_index)[np.where(clf.coef_[0] > cutoff)[0]]\n",
    "    # print('remain_coef_range: ', coef_remain_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11]\n"
     ]
    }
   ],
   "source": [
    "print(clf.n_iter_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1046,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5960264900662251\n",
      "0.31088082901554404\n"
     ]
    }
   ],
   "source": [
    "others = np.setdiff1d(np.array(range(5473)), data_indexes)\n",
    "print(metrics.f1_score(y[data_indexes], prediction_list[0][data_indexes]))\n",
    "print(metrics.f1_score(y[others], prediction_list[0][others]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02546559 0.09015333 0.28049589 0.12241808 0.44261866 0.64009336\n",
      " 0.10406313 0.0774474  0.85665126 0.45597751 0.45579484 0.74259613\n",
      " 0.86102329 0.47317641 0.24905887 0.81722536 0.98371082 0.96484526\n",
      " 0.09020024 0.03232177 0.10672986 0.53612269 0.31817522 0.00380425\n",
      " 0.16862244 0.01750938 0.78584606 0.51391231 0.34875351 0.16824163]\n",
      "[0 0 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 0 0 0 0 1 1 1 1]\n",
      "[ 1  2  7  2  8  9  1  2  8  2  1 10 10  2  3 10  5  5  2  2  1  3  2  1\n",
      "  5  1 15 13  2  1]\n"
     ]
    }
   ],
   "source": [
    "print(prediction_result_list[0][others][0:30])\n",
    "print(y[others][0:30])\n",
    "print(np.sum(L, axis = 1)[others][0:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39506172839506176"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "others_predictions =([int(i) for i in prediction_result_list[0][others]>0.1])\n",
    "metrics.f1_score(y[others], others_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.intersect1d(all_inlier_indexes, np.where(prediction_result_list[-1] > 0.5)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.88370321, 0.56152584, 0.61750205, 0.56582256, 0.68257264,\n",
       "       0.51605153, 0.63529795, 0.51128205])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_result_list[-1][temp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 670,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_high_conf_inliers = np.where(prediction_result_list[-1] < 0.001)[0]\n",
    "# print(prediction_high_conf_inliers)\n",
    "metrics.f1_score(y[prediction_high_conf_inliers], prediction_list[0][prediction_high_conf_inliers])\n",
    "\n",
    "print(disagree_indexes)\n",
    "cur_indexes = np.setdiff1d(np.intersect1d(np.where(prediction_result_list[0] > 0.99)[0],disagree_indexes), self_agree_index_list)\n",
    "print(len(cur_indexes))\n",
    "metrics.f1_score(y[cur_indexes], prediction_list[0][cur_indexes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 5473)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(classifier_result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 5473)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(prediction_result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_list = np.concatenate((prediction_result_list, classifier_result_list), axis = 0)\n",
    "result_list=np.array(classifier_result_list)\n",
    "# result_list = []\n",
    "# for i in range(0,6):\n",
    "#     result_list.append(np.array(prediction_result_list)[i,:])\n",
    "#     result_list.append(np.array(classifier_result_list)[i,:])\n",
    "# result_list = np.array(result_list)\n",
    "# np.shape(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151\n",
      "5322\n",
      "0.5638554216867471\n",
      "0.5638554216867471\n",
      "0.5638554216867471\n",
      "0.5638554216867471\n",
      "0.5638554216867471\n",
      "0.5638554216867471\n",
      "0.0\n",
      "0.1983471074380165\n",
      "0.34848484848484856\n",
      "0.6101694915254238\n",
      "0.7729468599033816\n",
      "0.7766990291262136\n",
      "0.6146202980837473\n"
     ]
    }
   ],
   "source": [
    "disagree_indexes = []\n",
    "for i in range(np.shape(result_list)[1]):\n",
    "    for j in range(np.shape(result_list)[0]-1):\n",
    "        if((result_list[j][i] > 0.5) != ((result_list[j+1][i] > 0.5))):\n",
    "            disagree_indexes.append(i)\n",
    "            break\n",
    "disagree_indexes = np.array(disagree_indexes)\n",
    "print(len(disagree_indexes))\n",
    "agree_indexes = np.setdiff1d(range(np.shape(result_list)[1]), disagree_indexes)\n",
    "print(len(agree_indexes))\n",
    "\n",
    "for j in range(np.shape(result_list)[0]):\n",
    "    predictions = np.array([int(i) for i in result_list[j]>0.5])\n",
    "    print(metrics.f1_score(y[agree_indexes], predictions[agree_indexes]))\n",
    "    \n",
    "for j in range(np.shape(result_list)[0]):\n",
    "    predictions = np.array([int(i) for i in result_list[j]>0.5])\n",
    "    print(metrics.f1_score(y[disagree_indexes], predictions[disagree_indexes]))\n",
    "\n",
    "new_labels = np.zeros(np.shape(result_list)[1])\n",
    "new_labels[agree_indexes] = np.array([int(i) for i in result_list[0,:] > 0.5])[agree_indexes]\n",
    "new_labels[disagree_indexes] = y[disagree_indexes]\n",
    "\n",
    "print(metrics.f1_score(y, new_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5893854748603351"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disagree_indexes = []\n",
    "for i in range(np.shape(result_list)[1]):\n",
    "    for j in range(np.shape(result_list)[0]-1):\n",
    "        if((result_list[j][i] > 0.5) != ((result_list[j+1][i] > 0.5))):\n",
    "            disagree_indexes.append(i)\n",
    "            break\n",
    "print(len(disagree_indexes))\n",
    "\n",
    "kf_final_results = []\n",
    "for i in range(np.shape(result_list)[1]):\n",
    "    kf_final_results.append(filter_update_list(0.1, list(result_list[:,i])))\n",
    "\n",
    "kf_final_results = np.array(kf_final_results)\n",
    "predictions = np.array([int(i) for i in kf_final_results>0.5])\n",
    "metrics.f1_score(y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.0432483008403625e-05, 6.772873649085378e-05, 0.0001507330750954765, 0.00033546262790251185, 0.0007465858083766792, 0.001661557273173934, 0.003697863716482929, 0.008229747049020023, 0.01831563888873418, 0.04076220397836621, 0.09071795328941247, 0.20189651799465538, 0.44932896411722156, 1.0]\n",
      "[1.67585161e-05 3.72967635e-05 8.30054736e-05 1.84732079e-04\n",
      " 4.11128802e-04 9.14983976e-04 2.03633429e-03 4.53194530e-03\n",
      " 1.00860298e-02 2.24468720e-02 4.99564324e-02 1.11180085e-01\n",
      " 2.47435830e-01 5.50678566e-01]\n"
     ]
    }
   ],
   "source": [
    "final = 13\n",
    "weights = []\n",
    "for i in range(final + 1):\n",
    "    weights.append(np.exp(-0.8 * (final-i)))\n",
    "print(weights)\n",
    "weights = weights/sum(weights)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.77840891e-06 3.66067915e-06 7.29587547e-06 ... 1.92738230e-05\n",
      " 3.72854654e-05 4.48199386e-01]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_results = np.zeros(np.shape(result_list)[1])\n",
    "for i in range(13):\n",
    "    sum_results += weights[i] * result_list[i,:]\n",
    "print(sum_results)\n",
    "predictions = np.array([int(i) for i in sum_results > 0.5])\n",
    "metrics.f1_score(y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18598472268349386\n",
      "0.18601561202458064\n",
      "0.18604651162790695\n",
      "0.1860774214985878\n",
      "0.18610834164174145\n",
      "0.18613927206248962\n",
      "0.18617021276595747\n",
      "0.1862011637572735\n",
      "0.1862321250415697\n",
      "0.18626309662398138\n",
      "0.18629407850964735\n",
      "0.18632507070370985\n",
      "0.18635607321131448\n",
      "0.18638708603761026\n",
      "0.18641810918774968\n",
      "0.1864491426668886\n",
      "0.1864801864801865\n",
      "0.18651124063280597\n",
      "0.18654230512991338\n",
      "0.18657337997667833\n",
      "0.1866044651782739\n",
      "0.1866355607398767\n",
      "0.18666666666666665\n",
      "0.18669778296382727\n",
      "0.1867289096365455\n",
      "0.18676004669001167\n",
      "0.18679119412941964\n",
      "0.18682235195996663\n",
      "0.18685352018685356\n",
      "0.1868846988152845\n",
      "0.18691588785046725\n",
      "0.18694708729761308\n",
      "0.18697829716193656\n",
      "0.18700951744865585\n",
      "0.18704074816299263\n",
      "0.18707198931017205\n",
      "0.18710324089542266\n",
      "0.1871345029239766\n",
      "0.18716577540106952\n",
      "0.1871970583319405\n",
      "0.18722835172183216\n",
      "0.18725965557599064\n",
      "0.18729096989966557\n",
      "0.18732229469811004\n",
      "0.18735362997658078\n",
      "0.18738497574033794\n",
      "0.18741633199464527\n",
      "0.18744769874476988\n",
      "0.1874790759959826\n",
      "0.18751046375355768\n",
      "0.18754186202277293\n",
      "0.18757327080890973\n",
      "0.18760469011725295\n",
      "0.18763611995309096\n",
      "0.18766756032171583\n",
      "0.18769901122842297\n",
      "0.18773047267851156\n",
      "0.18776194467728413\n",
      "0.18779342723004694\n",
      "0.1878249203421097\n",
      "0.18785642401878563\n",
      "0.1878879382653917\n",
      "0.18791946308724833\n",
      "0.1879509984896795\n",
      "0.18798254447801277\n",
      "0.18801410105757932\n",
      "0.1880456682337139\n",
      "0.1880772460117548\n",
      "0.18810883439704398\n",
      "0.18814043339492695\n",
      "0.18817204301075272\n",
      "0.18820366324987398\n",
      "0.18823529411764706\n",
      "0.18826693561943184\n",
      "0.1882985877605918\n",
      "0.18833025054649402\n",
      "0.18836192398250926\n",
      "0.1883936080740118\n",
      "0.18842530282637954\n",
      "0.1884570082449941\n",
      "0.18848872433524066\n",
      "0.18852045110250798\n",
      "0.18855218855218858\n",
      "0.1885839366896784\n",
      "0.1886156955203772\n",
      "0.1886474650496884\n",
      "0.18867924528301885\n",
      "0.18871103622577926\n",
      "0.1887428378833839\n",
      "0.18877465026125062\n",
      "0.18880647336480108\n",
      "0.18883830719946046\n",
      "0.1888701517706577\n",
      "0.1889020070838253\n",
      "0.18893387314439944\n",
      "0.18896574995782012\n",
      "0.1889976375295309\n",
      "0.18902953586497892\n",
      "0.18906144496961513\n",
      "0.18909336484889414\n",
      "0.1891252955082742\n",
      "0.18915723695321734\n",
      "0.18918918918918917\n",
      "0.18922115222165906\n",
      "0.18925312605610004\n",
      "0.18928511069798887\n",
      "0.18931710615280598\n",
      "0.1893491124260355\n",
      "0.18938112952316535\n",
      "0.18941315744968712\n",
      "0.1894451962110961\n",
      "0.18947724581289122\n",
      "0.18950930626057533\n",
      "0.18954137755965475\n",
      "0.18957345971563982\n",
      "0.1896055527340444\n",
      "0.18963765662038604\n",
      "0.1896697713801863\n",
      "0.18970189701897017\n",
      "0.18973403354226664\n",
      "0.18976618095560827\n",
      "0.18979833926453143\n",
      "0.18983050847457628\n",
      "0.18986268859128663\n",
      "0.18989487962021023\n",
      "0.18992708156689844\n",
      "0.18995929443690637\n",
      "0.18999151823579302\n",
      "0.1900237529691211\n",
      "0.19005599864245715\n",
      "0.19008825526137135\n",
      "0.1901205228314378\n",
      "0.19015280135823429\n",
      "0.19018509084734248\n",
      "0.19021739130434784\n",
      "0.19024970273483946\n",
      "0.19028202514441048\n",
      "0.19031435853865758\n",
      "0.1903467029231815\n",
      "0.1903790583035866\n",
      "0.1904114246854811\n",
      "0.19044380207447714\n",
      "0.1904761904761905\n",
      "0.19050858989624087\n",
      "0.1905410003402518\n",
      "0.1905734218138506\n",
      "0.1906058543226685\n",
      "0.19063829787234043\n",
      "0.1906707524685053\n",
      "0.1907032181168057\n",
      "0.1907356948228883\n",
      "0.19076818259240336\n",
      "0.19080068143100512\n",
      "0.1908331913443517\n",
      "0.19086571233810498\n",
      "0.1908982444179308\n",
      "0.19093078758949883\n",
      "0.19096334185848252\n",
      "0.19099590723055934\n",
      "0.19102848371141054\n",
      "0.19106107130672126\n",
      "0.1910936700221805\n",
      "0.19112627986348124\n",
      "0.1911589008363202\n",
      "0.1911915329463981\n",
      "0.19122417619941948\n",
      "0.19125683060109291\n",
      "0.19128949615713067\n",
      "0.19132217287324907\n",
      "0.1913548607551683\n",
      "0.19138755980861244\n",
      "0.19142027003930953\n",
      "0.19145299145299147\n",
      "0.19148572405539405\n",
      "0.19151846785225718\n",
      "0.1915512228493244\n",
      "0.1915839890523435\n",
      "0.19161676646706585\n",
      "0.1916495550992471\n",
      "0.1916823549546466\n",
      "0.19171516603902772\n",
      "0.19174798835815784\n",
      "0.19178082191780824\n",
      "0.19181366672375405\n",
      "0.1918465227817746\n",
      "0.1918793900976529\n",
      "0.19191226867717612\n",
      "0.1919451585261354\n",
      "0.1919780596503257\n",
      "0.19201097205554604\n",
      "0.19204389574759945\n",
      "0.19207683073229295\n",
      "0.1921097770154374\n",
      "0.1921427346028478\n",
      "0.19217570350034316\n",
      "0.19220868371374636\n",
      "0.1922416752488843\n",
      "0.192274678111588\n",
      "0.1923076923076923\n",
      "0.19234071784303625\n",
      "0.19237375472346271\n",
      "0.19240680295481877\n",
      "0.1924398625429553\n",
      "0.19247293349372746\n",
      "0.19250601581299417\n",
      "0.1925391095066185\n",
      "0.1925722145804677\n",
      "0.19260533104041272\n",
      "0.19263845889232886\n",
      "0.1926715981420953\n",
      "0.19270474879559532\n",
      "0.19273791085871625\n",
      "0.1927710843373494\n",
      "0.19280426923739025\n",
      "0.1928374655647383\n",
      "0.19287067332529703\n",
      "0.19290389252497417\n",
      "0.19293712316968134\n",
      "0.19297036526533423\n",
      "0.19300361881785286\n",
      "0.19303688383316098\n",
      "0.19307016031718668\n",
      "0.19310344827586207\n",
      "0.19313674771512332\n",
      "0.19317005864091066\n",
      "0.19320338105916857\n",
      "0.1932367149758454\n",
      "0.1932700603968939\n",
      "0.19330341732827064\n",
      "0.1933367857759365\n",
      "0.19337016574585633\n",
      "0.19340355724399932\n",
      "0.19343696027633853\n",
      "0.19347037484885127\n",
      "0.193503800967519\n",
      "0.19353723863832728\n",
      "0.19357068786726583\n",
      "0.19360414866032843\n",
      "0.19363762102351312\n",
      "0.19367110496282208\n",
      "0.1937046004842615\n",
      "0.1937381075938419\n",
      "0.19377162629757785\n",
      "0.19380515660148814\n",
      "0.19383869851159571\n",
      "0.19387225203392763\n",
      "0.19390581717451524\n",
      "0.1939393939393939\n",
      "0.1939729823346034\n",
      "0.19400658236618742\n",
      "0.19404019404019404\n",
      "0.19407381736267543\n",
      "0.194107452339688\n",
      "0.19414109897729243\n",
      "0.19417475728155342\n",
      "0.19420842725853996\n",
      "0.19424210891432533\n",
      "0.19427580225498697\n",
      "0.1943095072866065\n",
      "0.19434322401526982\n",
      "0.194376952447067\n",
      "0.19441069258809235\n",
      "0.19444444444444445\n",
      "0.19447820802222607\n",
      "0.1945119833275443\n",
      "0.19454577036651036\n",
      "0.19457956914523977\n",
      "0.1946133796698523\n",
      "0.194647201946472\n",
      "0.19468103598122719\n",
      "0.19471488178025032\n",
      "0.19474873934967835\n",
      "0.19478260869565214\n",
      "0.19481648982431726\n",
      "0.19485038274182323\n",
      "0.194884287454324\n",
      "0.1949182039679777\n",
      "0.1949521322889469\n",
      "0.1949860724233983\n",
      "0.19502002437750304\n",
      "0.19505398815743644\n",
      "0.19508796376937815\n",
      "0.1951219512195122\n",
      "0.19515595051402684\n",
      "0.19518996165911465\n",
      "0.19522398466097265\n",
      "0.19525801952580196\n",
      "0.19529206625980822\n",
      "0.19532612486920126\n",
      "0.1953601953601954\n",
      "0.19539427773900908\n",
      "0.1954283720118653\n",
      "0.1954624781849913\n",
      "0.1954965962646186\n",
      "0.19553072625698326\n",
      "0.1955648681683255\n",
      "0.19559902200489\n",
      "0.19563318777292577\n",
      "0.19566736547868624\n",
      "0.19570155512842913\n",
      "0.19573575672841664\n",
      "0.19576997028491522\n",
      "0.1958041958041958\n",
      "0.19583843329253367\n",
      "0.19587268275620845\n",
      "0.19590694420150429\n",
      "0.1959412176347096\n",
      "0.19597550306211722\n",
      "0.19600980049002448\n",
      "0.19604410992473306\n",
      "0.19607843137254902\n",
      "0.19611276483978288\n",
      "0.19614711033274956\n",
      "0.19618146785776844\n",
      "0.19621583742116327\n",
      "0.1962502190292623\n",
      "0.19628461268839817\n",
      "0.19631901840490798\n",
      "0.19635343618513323\n",
      "0.19638786603541994\n",
      "0.19642230796211857\n",
      "0.19645676197158393\n",
      "0.19649122807017544\n",
      "0.19652570626425692\n",
      "0.19656019656019655\n",
      "0.1965946989643672\n",
      "0.19662921348314608\n",
      "0.19666374012291485\n",
      "0.19669827889005972\n",
      "0.19673282979097137\n",
      "0.19676739283204495\n",
      "0.1968019680196802\n",
      "0.1968365553602812\n",
      "0.19687115486025664\n",
      "0.19690576652601968\n",
      "0.19694039036398808\n",
      "0.1969750263805839\n",
      "0.19700967458223392\n",
      "0.19704433497536947\n",
      "0.19707900756642618\n",
      "0.19711369236184442\n",
      "0.19714838936806903\n",
      "0.19718309859154928\n",
      "0.19721782003873922\n",
      "0.1972525537160972\n",
      "0.19728729963008632\n",
      "0.19732205778717407\n",
      "0.19735682819383263\n",
      "0.1973916108565386\n",
      "0.1974264057817733\n",
      "0.19746121297602257\n",
      "0.19749603244577676\n",
      "0.19753086419753085\n",
      "0.19756570823778444\n",
      "0.19760056457304168\n",
      "0.19763543320981117\n",
      "0.19767031415460642\n",
      "0.19770520741394526\n",
      "0.1977401129943503\n",
      "0.19777503090234855\n",
      "0.1978099611444719\n",
      "0.1978449037272567\n",
      "0.1978798586572438\n",
      "0.19791482594097898\n",
      "0.19794980558501235\n",
      "0.19798479759589888\n",
      "0.198019801980198\n",
      "0.1980548187444739\n",
      "0.19808984789529535\n",
      "0.19812488943923579\n",
      "0.1981599433828733\n",
      "0.19819500973279064\n",
      "0.19823008849557522\n",
      "0.1982651796778191\n",
      "0.19830028328611898\n",
      "0.1983353993270763\n",
      "0.19837052780729722\n",
      "0.19840566873339238\n",
      "0.19844082211197733\n",
      "0.19847598794967217\n",
      "0.19851116625310175\n",
      "0.1985463570288956\n",
      "0.19858156028368792\n",
      "0.19861677602411776\n",
      "0.19865200425682866\n",
      "0.19868724498846907\n",
      "0.198722498225692\n",
      "0.19875776397515527\n",
      "0.19879304224352148\n",
      "0.19882833303745787\n",
      "0.19886363636363635\n",
      "0.1988989522287338\n",
      "0.19893428063943164\n",
      "0.19896962160241607\n",
      "0.19900497512437812\n",
      "0.1990403412120135\n",
      "0.19907571987202274\n",
      "0.1991111111111111\n",
      "0.19914651493598862\n",
      "0.1991819313533701\n",
      "0.1992173603699751\n",
      "0.199252801992528\n",
      "0.199288256227758\n",
      "0.19932372308239904\n",
      "0.19935920256318976\n",
      "0.1993946946768738\n",
      "0.19943019943019943\n",
      "0.19946571682991987\n",
      "0.199501246882793\n",
      "0.1995367895955817\n",
      "0.19957234497505347\n",
      "0.19960791302798075\n",
      "0.1996434937611408\n",
      "0.19967908718131575\n",
      "0.19971469329529246\n",
      "0.1997503121098627\n",
      "0.19978594363182303\n",
      "0.199821587867975\n",
      "0.1998572448251249\n",
      "0.19989291451008387\n",
      "0.19992859692966797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1999642920906981\n",
      "0.19999999999999998\n",
      "0.20003572066440434\n",
      "0.20007145409074673\n",
      "0.20010720028586743\n",
      "0.20014295925661185\n",
      "0.2001787310098302\n",
      "0.20021451555237754\n",
      "0.20025031289111392\n",
      "0.20028612303290413\n",
      "0.2003219459846181\n",
      "0.2003577817531306\n",
      "0.20039363034532115\n",
      "0.20042949176807442\n",
      "0.20046536602827994\n",
      "0.20050125313283207\n",
      "0.20053715308863024\n",
      "0.20057306590257878\n",
      "0.20060899158158693\n",
      "0.20064493013256895\n",
      "0.20068088156244399\n",
      "0.2007168458781362\n",
      "0.20075282308657463\n",
      "0.20078881319469344\n",
      "0.20082481620943157\n",
      "0.20086083213773312\n",
      "0.20089686098654708\n",
      "0.20093290276282738\n",
      "0.2009689574735331\n",
      "0.20100502512562812\n",
      "0.20104110572608147\n",
      "0.20107719928186712\n",
      "0.2011133057999641\n",
      "0.2011494252873563\n",
      "0.20118555775103286\n",
      "0.20122170319798777\n",
      "0.20125786163522014\n",
      "0.201294033069734\n",
      "0.20133021750853855\n",
      "0.20136641495864796\n",
      "0.20140262542708148\n",
      "0.2014388489208633\n",
      "0.20147508544702283\n",
      "0.20151133501259447\n",
      "0.2015475976246176\n",
      "0.2015838732901368\n",
      "0.20162016201620164\n",
      "0.20165646380986674\n",
      "0.20169277867819196\n",
      "0.20172910662824206\n",
      "0.201765447667087\n",
      "0.2018018018018018\n",
      "0.2018381690394666\n",
      "0.20187454938716654\n",
      "0.20191094285199207\n",
      "0.2019473494410386\n",
      "0.2019837691614067\n",
      "0.20202020202020202\n",
      "0.20205664802453543\n",
      "0.2020931071815229\n",
      "0.2021295794982855\n",
      "0.20216606498194944\n",
      "0.20220256363964614\n",
      "0.2022390754785121\n",
      "0.20227560050568896\n",
      "0.2023121387283237\n",
      "0.20234869015356818\n",
      "0.2023852547885797\n",
      "0.2024218326405205\n",
      "0.2024584237165582\n",
      "0.2024950280238655\n",
      "0.20253164556962025\n",
      "0.2025682763610056\n",
      "0.20260492040520983\n",
      "0.20264157770942642\n",
      "0.20267824828085415\n",
      "0.2027149321266968\n",
      "0.20275162925416365\n",
      "0.20278833967046894\n",
      "0.20282506338283232\n",
      "0.20286180039847854\n",
      "0.2028985507246377\n",
      "0.20293531436854503\n",
      "0.2029720913374411\n",
      "0.2030088816385717\n",
      "0.20304568527918782\n",
      "0.20308250226654578\n",
      "0.20311933260790713\n",
      "0.20315617631053873\n",
      "0.20319303338171263\n",
      "0.20322990382870623\n",
      "0.20326678765880218\n",
      "0.20330368487928843\n",
      "0.20334059549745823\n",
      "0.20337751952061012\n",
      "0.20341445695604796\n",
      "0.20345140781108081\n",
      "0.20348837209302323\n",
      "0.20352534980919498\n",
      "0.20356234096692113\n",
      "0.20359934557353207\n",
      "0.20363636363636364\n",
      "0.20367339516275684\n",
      "0.20371044016005824\n",
      "0.20374749863561942\n",
      "0.20378457059679767\n",
      "0.20382165605095542\n",
      "0.2038587550054605\n",
      "0.20389586746768612\n",
      "0.20393299344501095\n",
      "0.20397013294481878\n",
      "0.2040072859744991\n",
      "0.2040444525414465\n",
      "0.20408163265306123\n",
      "0.20411882631674866\n",
      "0.2041560335399198\n",
      "0.2041932543299909\n",
      "0.20423048869438365\n",
      "0.20426773664052522\n",
      "0.20430499817584824\n",
      "0.20434227330779053\n",
      "0.20437956204379562\n",
      "0.20441686439131226\n",
      "0.20445418035779483\n",
      "0.20449150995070292\n",
      "0.2045288531775018\n",
      "0.2045662100456621\n",
      "0.20460358056265987\n",
      "0.20464096473597662\n",
      "0.2046783625730994\n",
      "0.20471577408152075\n",
      "0.20475319926873858\n",
      "0.20479063814225634\n",
      "0.20482809070958302\n",
      "0.20486555697823303\n",
      "0.20490303695572634\n",
      "0.20494053064958828\n",
      "0.2049780380673499\n",
      "0.2050155592165477\n",
      "0.20505309410472355\n",
      "0.20509064273942504\n",
      "0.20512820512820512\n",
      "0.20516578127862248\n",
      "0.2052033711982411\n",
      "0.20524097489463075\n",
      "0.20527859237536655\n",
      "0.20531622364802932\n",
      "0.20535386872020533\n",
      "0.20539152759948653\n",
      "0.20542920029347025\n",
      "0.2054668868097597\n",
      "0.20550458715596331\n",
      "0.20554230133969537\n",
      "0.2055800293685756\n",
      "0.20561777125022948\n",
      "0.20565552699228792\n",
      "0.2056932966023875\n",
      "0.20573108008817045\n",
      "0.20576887745728462\n",
      "0.20580668871738333\n",
      "0.2058445138761257\n",
      "0.2058823529411765\n",
      "0.20592020592020593\n",
      "0.20595807282089004\n",
      "0.20599595365091042\n",
      "0.20603384841795438\n",
      "0.20607175712971482\n",
      "0.20610967979389033\n",
      "0.20614761641818516\n",
      "0.20618556701030927\n",
      "0.20622353157797826\n",
      "0.20626151012891344\n",
      "0.20629950267084177\n",
      "0.20633750921149596\n",
      "0.20637552975861434\n",
      "0.206413564319941\n",
      "0.20645161290322583\n",
      "0.20648967551622419\n",
      "0.2065277521666974\n",
      "0.2065658428624124\n",
      "0.20660394761114184\n",
      "0.20664206642066424\n",
      "0.2066801992987636\n",
      "0.20671834625322996\n",
      "0.20675650729185896\n",
      "0.206794682422452\n",
      "0.20683287165281627\n",
      "0.20687107499076468\n",
      "0.20690929244411602\n",
      "0.20694752402069477\n",
      "0.2069857697283312\n",
      "0.20702402957486138\n",
      "0.20706230356812722\n",
      "0.20710059171597633\n",
      "0.20713889402626223\n",
      "0.20717721050684426\n",
      "0.20721554116558744\n",
      "0.2072538860103627\n",
      "0.20729224504904684\n",
      "0.20733061828952243\n",
      "0.20736900573967787\n",
      "0.2074074074074074\n",
      "0.20744582330061123\n",
      "0.2074842534271953\n",
      "0.20752269779507132\n",
      "0.20756115641215714\n",
      "0.2075996292863763\n",
      "0.20763811642565813\n",
      "0.20767661783793806\n",
      "0.20771513353115725\n",
      "0.20775366351326285\n",
      "0.20779220779220778\n",
      "0.20783076637595102\n",
      "0.2078693392724573\n",
      "0.20790792648969744\n",
      "0.20792263343872047\n",
      "0.20796130952380953\n",
      "0.20800000000000002\n",
      "0.20803870487532564\n",
      "0.20807742415782618\n",
      "0.20811615785554727\n",
      "0.20815490597654068\n",
      "0.2081936685288641\n",
      "0.20823244552058112\n",
      "0.20827123695976155\n",
      "0.20831004285448107\n",
      "0.20834886321282148\n",
      "0.20838769804287047\n",
      "0.2084265473527218\n",
      "0.20846541115047548\n",
      "0.20850428944423724\n",
      "0.208543182242119\n",
      "0.2085820895522388\n",
      "0.20862101138272066\n",
      "0.20865994774169466\n",
      "0.20869889863729696\n",
      "0.2087378640776699\n",
      "0.20877684407096173\n",
      "0.20881583862532688\n",
      "0.20885484774892582\n",
      "0.20889387144992524\n",
      "0.20893290973649786\n",
      "0.20897196261682244\n",
      "0.20901103009908392\n",
      "0.20905011219147343\n",
      "0.20908920890218813\n",
      "0.20912832023943131\n",
      "0.2091674462114125\n",
      "0.20920658682634732\n",
      "0.20924574209245742\n",
      "0.20928491201797078\n",
      "0.2093240966111215\n",
      "0.20936329588014982\n",
      "0.20940250983330214\n",
      "0.20944173847883102\n",
      "0.20948098182499533\n",
      "0.20952023988005997\n",
      "0.20955951265229616\n",
      "0.20959880014998125\n",
      "0.20963810238139885\n",
      "0.20967741935483875\n",
      "0.20971675107859686\n",
      "0.2097560975609756\n",
      "0.20979545881028333\n",
      "0.20983483483483484\n",
      "0.20987422564295097\n",
      "0.2099136312429591\n",
      "0.20995305164319247\n",
      "0.20999248685199098\n",
      "0.21003193687770055\n",
      "0.21007140172867345\n",
      "0.2101108814132682\n",
      "0.21015037593984964\n",
      "0.21018988531678884\n",
      "0.21022940955246333\n",
      "0.21026894865525672\n",
      "0.21030850263355905\n",
      "0.2103480714957667\n",
      "0.2103876552502823\n",
      "0.21042725390551478\n",
      "0.21046686746987953\n",
      "0.21050649595179816\n",
      "0.21054613935969868\n",
      "0.21058579770201546\n",
      "0.21062547098718912\n",
      "0.21066515922366685\n",
      "0.21070486241990197\n",
      "0.21074458058435436\n",
      "0.2107843137254902\n",
      "0.21082406185178199\n",
      "0.2108638249717088\n",
      "0.21090360309375586\n",
      "0.2109433962264151\n",
      "0.21098320437818457\n",
      "0.21102302755756888\n",
      "0.2110628657730791\n",
      "0.21110271903323266\n",
      "0.21114258734655336\n",
      "0.21118247072157156\n",
      "0.21122236916682413\n",
      "0.2112622826908541\n",
      "0.21130221130221133\n",
      "0.2113421550094518\n",
      "0.2113821138211382\n",
      "0.21142208774583965\n",
      "0.21146207679213166\n",
      "0.21150208096859627\n",
      "0.21154210028382214\n",
      "0.21158213474640428\n",
      "0.21162218436494415\n",
      "0.21166224914805\n",
      "0.2117023291043363\n",
      "0.2117424242424243\n",
      "0.21178253457094148\n",
      "0.2118226600985222\n",
      "0.2118628008338071\n",
      "0.21190295678544352\n",
      "0.21194312796208528\n",
      "0.2119833143723929\n",
      "0.2120235160250332\n",
      "0.21206373292867983\n",
      "0.2121039650920129\n",
      "0.21214421252371915\n",
      "0.2121844752324919\n",
      "0.2122247532270311\n",
      "0.2122650465160433\n",
      "0.21230535510824156\n",
      "0.2123456790123457\n",
      "0.21238601823708206\n",
      "0.2124263727911837\n",
      "0.21246674268339036\n",
      "0.2125071279224482\n",
      "0.21254752851711023\n",
      "0.21258794447613613\n",
      "0.21262837580829214\n",
      "0.21266882252235117\n",
      "0.21270928462709285\n",
      "0.2127497621313035\n",
      "0.2127902550437762\n",
      "0.2128307633733105\n",
      "0.21287128712871287\n",
      "0.2129118263187964\n",
      "0.21295238095238095\n",
      "0.21299295103829302\n",
      "0.21303353658536583\n",
      "0.2130741376024395\n",
      "0.21311475409836067\n",
      "0.21315538608198284\n",
      "0.2131960335621663\n",
      "0.213236696547778\n",
      "0.21327737504769173\n",
      "0.21331806907078804\n",
      "0.2133587786259542\n",
      "0.21339950372208435\n",
      "0.2134402443680794\n",
      "0.21348100057284705\n",
      "0.21352177234530176\n",
      "0.21356255969436483\n",
      "0.21360336262896445\n",
      "0.21364418115803552\n",
      "0.2136850152905199\n",
      "0.21372586503536609\n",
      "0.21376673040152963\n",
      "0.21380761139797283\n",
      "0.21384850803366487\n",
      "0.21388942031758176\n",
      "0.21393034825870647\n",
      "0.21397129186602873\n",
      "0.21401225114854516\n",
      "0.21405322611525945\n",
      "0.21409421677518192\n",
      "0.21413522313733002\n",
      "0.21417624521072798\n",
      "0.21421728300440698\n",
      "0.21425833652740517\n",
      "0.2142994057887675\n",
      "0.21434049079754602\n",
      "0.21438159156279962\n",
      "0.21442270809359418\n",
      "0.2144638403990025\n",
      "0.21450498848810437\n",
      "0.21454615236998653\n",
      "0.2145873320537428\n",
      "0.2146285275484738\n",
      "0.21466973886328722\n",
      "0.21471096600729783\n",
      "0.21475220898962732\n",
      "0.21479346781940442\n",
      "0.2148347425057648\n",
      "0.21487603305785125\n",
      "0.21491733948481354\n",
      "0.2149586617958085\n",
      "0.215\n",
      "0.21504135410655895\n",
      "0.21508272412466334\n",
      "0.2151241100634982\n",
      "0.21516551193225558\n",
      "0.21520692974013472\n",
      "0.21524836349634197\n",
      "0.2152898132100905\n",
      "0.2153312788906009\n",
      "0.21537276054710078\n",
      "0.21541425818882468\n",
      "0.21545577182501446\n",
      "0.21549730146491905\n",
      "0.21553884711779447\n",
      "0.21558040879290397\n",
      "0.21562198649951783\n",
      "0.21566358024691362\n",
      "0.21570519004437586\n",
      "0.21574681590119643\n",
      "0.21578845782667438\n",
      "0.21583011583011583\n",
      "0.21587178992083414\n",
      "0.21591348010814987\n",
      "0.21595518640139075\n",
      "0.21599690880989184\n",
      "0.21603864734299516\n",
      "0.21608040201005024\n",
      "0.2161221728204137\n",
      "0.21616395978344932\n",
      "0.2162057629085283\n",
      "0.21624758220502902\n",
      "0.21628941768233703\n",
      "0.21633126934984523\n",
      "0.21637313721695375\n",
      "0.21641502129307005\n",
      "0.21645692158760893\n",
      "0.21649883810999226\n",
      "0.21654077086964943\n",
      "0.21658271987601704\n",
      "0.21662468513853905\n",
      "0.21666666666666665\n",
      "0.21670866446985848\n",
      "0.21675067855758046\n",
      "0.2167927089393058\n",
      "0.21683475562451512\n",
      "0.2168768186226964\n",
      "0.21691889794334496\n",
      "0.21696099359596352\n",
      "0.2170031055900621\n",
      "0.2170452339351582\n",
      "0.21708737864077673\n",
      "0.2171295397164498\n",
      "0.2171717171717172\n",
      "0.21721391101612592\n",
      "0.2172561212592305\n",
      "0.2172983479105928\n",
      "0.21734059097978228\n",
      "0.21738285047637568\n",
      "0.21742512640995723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21746741879011866\n",
      "0.21750972762645918\n",
      "0.21755205292858532\n",
      "0.21759439470611133\n",
      "0.21763675296865873\n",
      "0.2176791277258567\n",
      "0.2177215189873418\n",
      "0.21776392676275808\n",
      "0.21780635106175728\n",
      "0.21784879189399844\n",
      "0.21789124926914832\n",
      "0.2179337231968811\n",
      "0.21797621368687853\n",
      "0.21801872074882994\n",
      "0.2180612443924322\n",
      "0.21810378462738975\n",
      "0.21814634146341463\n",
      "0.2181889149102264\n",
      "0.2182315049775522\n",
      "0.2182741116751269\n",
      "0.21831673501269283\n",
      "0.218359375\n",
      "0.21840203164680602\n",
      "0.2184447049628761\n",
      "0.21848739495798322\n",
      "0.21853010164190773\n",
      "0.21857282502443792\n",
      "0.21861556511536956\n",
      "0.21865832192450618\n",
      "0.2187010954616588\n",
      "0.21874388573664644\n",
      "0.21878669275929552\n",
      "0.2188295165394402\n",
      "0.2188723570869225\n",
      "0.21891521441159195\n",
      "0.21895808852330592\n",
      "0.21900097943192948\n",
      "0.21904388714733544\n",
      "0.21908681167940428\n",
      "0.21912975303802432\n",
      "0.21917271123309157\n",
      "0.2192156862745098\n",
      "0.21925867817219064\n",
      "0.21930168693605334\n",
      "0.21934471257602514\n",
      "0.2193877551020408\n",
      "0.21943081452404317\n",
      "0.2194738908519827\n",
      "0.2195169840958178\n",
      "0.21956009426551457\n",
      "0.21960322137104696\n",
      "0.21964636542239685\n",
      "0.21968952642955392\n",
      "0.21973270440251572\n",
      "0.2197758993512876\n",
      "0.2198191112858828\n",
      "0.21986234021632253\n",
      "0.21990558615263572\n",
      "0.21994884910485935\n",
      "0.21999212908303814\n",
      "0.22003542609722496\n",
      "0.22007874015748033\n",
      "0.22012207127387282\n",
      "0.2201654194564789\n",
      "0.2202087847153831\n",
      "0.22025216706067768\n",
      "0.22029556650246307\n",
      "0.22033898305084743\n",
      "0.22038241671594716\n",
      "0.22042586750788642\n",
      "0.22046933543679748\n",
      "0.2205128205128205\n",
      "0.22055632274610376\n",
      "0.22059984214680348\n",
      "0.22064337872508388\n",
      "0.22068693249111723\n",
      "0.22073050345508388\n",
      "0.22077409162717218\n",
      "0.22081769701757853\n",
      "0.2208613196365073\n",
      "0.22090495949417113\n",
      "0.2209486166007905\n",
      "0.2209922909665942\n",
      "0.22103598260181892\n",
      "0.2210796915167095\n",
      "0.221123417721519\n",
      "0.2211671612265084\n",
      "0.22121092204194698\n",
      "0.221254700178112\n",
      "0.22129849564528897\n",
      "0.22134230845377154\n",
      "0.22138613861386136\n",
      "0.22142998613586848\n",
      "0.22147385103011094\n",
      "0.22151773330691502\n",
      "0.22156163297661513\n",
      "0.221605550049554\n",
      "0.22164948453608246\n",
      "0.22169343644655962\n",
      "0.22173740579135268\n",
      "0.22178139258083718\n",
      "0.22182539682539684\n",
      "0.22186941853542372\n",
      "0.22191345772131796\n",
      "0.22195751439348818\n",
      "0.22200158856235105\n",
      "0.2220456802383317\n",
      "0.2220897894318633\n",
      "0.22213391615338762\n",
      "0.22217806041335453\n",
      "0.2222222222222222\n",
      "0.22226640159045724\n",
      "0.22231059852853452\n",
      "0.22235481304693716\n",
      "0.22239904515615677\n",
      "0.22244329486669318\n",
      "0.22248756218905474\n",
      "0.22253184713375795\n",
      "0.22257614971132786\n",
      "0.2226204699322979\n",
      "0.2226648078072097\n",
      "0.22270916334661356\n",
      "0.22275353656106797\n",
      "0.22279792746113988\n",
      "0.22284233605740486\n",
      "0.2228867623604466\n",
      "0.22293120638085742\n",
      "0.22297566812923816\n",
      "0.2230201476161979\n",
      "0.22306464485235433\n",
      "0.22310915984833368\n",
      "0.22315369261477044\n",
      "0.22319824316230785\n",
      "0.22324281150159742\n",
      "0.22328739764329936\n",
      "0.22333200159808228\n",
      "0.22337662337662337\n",
      "0.22342126298960835\n",
      "0.22346592044773136\n",
      "0.2235105957616953\n",
      "0.22355528894221155\n",
      "0.22359999999999997\n",
      "0.22364472894578918\n",
      "0.22368947579031612\n",
      "0.2237342405443266\n",
      "0.2237790232185749\n",
      "0.2238238238238238\n",
      "0.22386864237084497\n",
      "0.2239134788704186\n",
      "0.2239583333333333\n",
      "0.2240032057703867\n",
      "0.22404809619238475\n",
      "0.22409300461014234\n",
      "0.22413793103448276\n",
      "0.22418287547623827\n",
      "0.22422783794624945\n",
      "0.22427281845536612\n",
      "0.22431781701444622\n",
      "0.22436283363435677\n",
      "0.22440786832597354\n",
      "0.22445292110018067\n",
      "0.22449799196787146\n",
      "0.22454308093994776\n",
      "0.2245881880273202\n",
      "0.22463331324090818\n",
      "0.22467845659163987\n",
      "0.22472361809045227\n",
      "0.22476879774829112\n",
      "0.22481399557611098\n",
      "0.2248592115848753\n",
      "0.2249044457855562\n",
      "0.2249496981891348\n",
      "0.22499496880660091\n",
      "0.22504025764895333\n",
      "0.22508556472719954\n",
      "0.22513089005235604\n",
      "0.22517623363544814\n",
      "0.22522159548751008\n",
      "0.2252669756195849\n",
      "0.22531237404272467\n",
      "0.2253577907679903\n",
      "0.22540322580645164\n",
      "0.22544867916918734\n",
      "0.2254941508672852\n",
      "0.22553964091184184\n",
      "0.2255851493139629\n",
      "0.22563067608476284\n",
      "0.22567622123536538\n",
      "0.22572178477690286\n",
      "0.22576736672051692\n",
      "0.2258129670773581\n",
      "0.22585858585858584\n",
      "0.2259042230753687\n",
      "0.22594987873888442\n",
      "0.22599555286031942\n",
      "0.22604124545086937\n",
      "0.22608695652173913\n",
      "0.22613268608414241\n",
      "0.22617843414930203\n",
      "0.22622420072845006\n",
      "0.22626998583282734\n",
      "0.22631578947368422\n",
      "0.22636161166227983\n",
      "0.22640745240988255\n",
      "0.2264533117277699\n",
      "0.22649918962722854\n",
      "0.22654508611955418\n",
      "0.22659100121605188\n",
      "0.2266369349280357\n",
      "0.2266828872668289\n",
      "0.22672885824376393\n",
      "0.22677484787018254\n",
      "0.22682085615743558\n",
      "0.22686688311688308\n",
      "0.22691292875989447\n",
      "0.22695899309784814\n",
      "0.22700507614213197\n",
      "0.227051177904143\n",
      "0.2270972983952874\n",
      "0.22714343762698092\n",
      "0.22718959561064822\n",
      "0.22723577235772358\n",
      "0.22728196787965035\n",
      "0.22732818218788128\n",
      "0.22737441529387836\n",
      "0.2274206672091131\n",
      "0.2274669379450661\n",
      "0.22751322751322753\n",
      "0.22755953592509667\n",
      "0.22760586319218243\n",
      "0.22765220932600283\n",
      "0.22769857433808552\n",
      "0.2277449582399674\n",
      "0.2277913610431948\n",
      "0.22780159216166565\n",
      "0.22784810126582278\n",
      "0.2278946293649173\n",
      "0.22794117647058826\n",
      "0.2279877425944842\n",
      "0.2280343277482632\n",
      "0.22808093194359289\n",
      "0.22812755519215042\n",
      "0.22817419750562262\n",
      "0.2282208588957055\n",
      "0.22826753937410513\n",
      "0.22831423895253686\n",
      "0.22836095764272563\n",
      "0.22840769545640605\n",
      "0.22845445240532242\n",
      "0.22850122850122848\n",
      "0.22854802375588776\n",
      "0.22859483818107332\n",
      "0.22864167178856795\n",
      "0.22868852459016398\n",
      "0.22873539659766348\n",
      "0.22878228782287827\n",
      "0.22882919827762968\n",
      "0.22887612797374893\n",
      "0.22892307692307695\n",
      "0.22897004513746408\n",
      "0.2290170326287708\n",
      "0.22906403940886702\n",
      "0.22915811088295684\n",
      "0.22920517560073936\n",
      "0.22925225965488905\n",
      "0.22929936305732485\n",
      "0.22934648581997535\n",
      "0.22939362795477908\n",
      "0.22944078947368424\n",
      "0.22948797038864893\n",
      "0.22953517071164134\n",
      "0.22958239045463893\n",
      "0.22962962962962966\n",
      "0.22967688824861088\n",
      "0.22972416632358994\n",
      "0.2297714638665843\n",
      "0.22981878088962104\n",
      "0.22986611740473742\n",
      "0.2299134734239802\n",
      "0.22996084895940652\n",
      "0.23000824402308329\n",
      "0.2300556586270872\n",
      "0.23010309278350513\n",
      "0.23015054650443392\n",
      "0.23019801980198024\n",
      "0.23024551268826077\n",
      "0.2302930251754024\n",
      "0.23034055727554179\n",
      "0.23038810900082576\n",
      "0.23043568036341108\n",
      "0.23048327137546465\n",
      "0.23053088204916344\n",
      "0.23057851239669427\n",
      "0.23062616243025416\n",
      "0.23067383216205042\n",
      "0.2307215216043002\n",
      "0.23076923076923078\n",
      "0.23081695966907964\n",
      "0.23086470831609435\n",
      "0.23091247672253257\n",
      "0.23096026490066224\n",
      "0.23100807286276137\n",
      "0.23105590062111803\n",
      "0.23110374818803062\n",
      "0.23115161557580774\n",
      "0.23119950279676815\n",
      "0.23124740986324083\n",
      "0.23129533678756478\n",
      "0.23134328358208955\n",
      "0.23139125025917479\n",
      "0.2314392368311904\n",
      "0.23148724331051646\n",
      "0.23153526970954355\n",
      "0.23158331604067237\n",
      "0.2316313823163138\n",
      "0.2316794685488894\n",
      "0.23172757475083058\n",
      "0.23177570093457944\n",
      "0.23182384711258824\n",
      "0.23187201329731974\n",
      "0.23192019950124687\n",
      "0.23196840573685307\n",
      "0.23201663201663203\n",
      "0.23206487835308798\n",
      "0.23211314475873543\n",
      "0.23216143124609945\n",
      "0.2322097378277154\n",
      "0.23225806451612901\n",
      "0.2323064113238968\n",
      "0.23235477826358522\n",
      "0.23240316534777172\n",
      "0.23245157258904395\n",
      "0.2325\n",
      "0.2325484475932486\n",
      "0.23259691538140898\n",
      "0.23264540337711068\n",
      "0.23269391159299418\n",
      "0.23274244004171016\n",
      "0.23279098873591986\n",
      "0.23283955768829542\n",
      "0.23288814691151918\n",
      "0.2329367564182843\n",
      "0.23298538622129433\n",
      "0.2330340363332637\n",
      "0.2330827067669173\n",
      "0.23313139753499063\n",
      "0.2331801086502298\n",
      "0.23322884012539186\n",
      "0.23327759197324416\n",
      "0.2333263642065649\n",
      "0.23337515683814306\n",
      "0.23342396988077807\n",
      "0.23347280334728035\n",
      "0.23352165725047083\n",
      "0.23357053160318123\n",
      "0.23361942641825412\n",
      "0.23366834170854273\n",
      "0.23371727748691098\n",
      "0.23376623376623373\n",
      "0.2338152105593966\n",
      "0.2338642078792959\n",
      "0.2339132257388388\n",
      "0.23396226415094337\n",
      "0.23401132312853848\n",
      "0.23406040268456377\n",
      "0.23410950283196982\n",
      "0.234158623583718\n",
      "0.2342077649527807\n",
      "0.2342569269521411\n",
      "0.23430610959479314\n",
      "0.2343553128937421\n",
      "0.23440453686200377\n",
      "0.2344537815126051\n",
      "0.23450304685858375\n",
      "0.23455233291298866\n",
      "0.23460163968887957\n",
      "0.2346509671993272\n",
      "0.23470031545741327\n",
      "0.23474968447623054\n",
      "0.23479907426888277\n",
      "0.23484848484848483\n",
      "0.23489791622816247\n",
      "0.23494736842105265\n",
      "0.23499684144030322\n",
      "0.2350463352990733\n",
      "0.235095850010533\n",
      "0.2351453855878635\n",
      "0.23519494204425714\n",
      "0.2352445193929174\n",
      "0.23529411764705888\n",
      "0.23534373681990725\n",
      "0.23539337692469944\n",
      "0.23544303797468358\n",
      "0.2354927199831188\n",
      "0.23554242296327565\n",
      "0.23559214692843572\n",
      "0.23564189189189189\n",
      "0.2356916578669483\n",
      "0.23574144486692017\n",
      "0.23579125290513417\n",
      "0.23584108199492812\n",
      "0.23589093214965123\n",
      "0.23594080338266388\n",
      "0.23599069570733772\n",
      "0.23604060913705582\n",
      "0.2360905436852126\n",
      "0.23614049936521372\n",
      "0.23619047619047617\n",
      "0.23624047417442842\n",
      "0.23629049333051028\n",
      "0.2363405336721728\n",
      "0.23639059521287864\n",
      "0.23644067796610166\n",
      "0.23649078194532738\n",
      "0.2365409071640526\n",
      "0.2365910536357854\n",
      "0.2366412213740458\n",
      "0.2366914103923648\n",
      "0.23674162070428514\n",
      "0.23679185232336092\n",
      "0.2368421052631579\n",
      "0.2368923795372532\n",
      "0.23694267515923564\n",
      "0.23699299214270544\n",
      "0.23704333050127444\n",
      "0.23709369024856597\n",
      "0.23714407139821503\n",
      "0.23719447396386822\n",
      "0.23724489795918366\n",
      "0.23729534339783118\n",
      "0.23734581029349214\n",
      "0.2373962986598596\n",
      "0.23744680851063835\n",
      "0.2374973398595446\n",
      "0.23754789272030652\n",
      "0.23759846710666382\n",
      "0.23764906303236796\n",
      "0.23769968051118218\n",
      "0.23775031955688114\n",
      "0.2378009801832517\n",
      "0.23785166240409206\n",
      "0.23790236623321256\n",
      "0.23795309168443496\n",
      "0.23800383877159312\n",
      "0.23805460750853244\n",
      "0.2381053979091103\n",
      "0.23815620998719586\n",
      "0.2382070437566702\n",
      "0.2382578992314261\n",
      "0.2383087764253683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2383596753524135\n",
      "0.2384105960264901\n",
      "0.23846153846153847\n",
      "0.238512502671511\n",
      "0.23856348867037194\n",
      "0.23861449647209748\n",
      "0.2386655260906758\n",
      "0.23871657754010694\n",
      "0.2387676508344031\n",
      "0.2388187459875883\n",
      "0.2388698630136986\n",
      "0.23892100192678226\n",
      "0.23897216274089933\n",
      "0.2390233454701221\n",
      "0.2390745501285347\n",
      "0.23912577673023358\n",
      "0.23917702528932702\n",
      "0.2392282958199357\n",
      "0.2392795883361921\n",
      "0.2393309028522411\n",
      "0.23938223938223935\n",
      "0.23943359794035612\n",
      "0.23948497854077255\n",
      "0.23953638119768192\n",
      "0.2395878059252898\n",
      "0.23963925273781403\n",
      "0.23969072164948454\n",
      "0.23974221267454351\n",
      "0.23979372582724537\n",
      "0.23984526112185686\n",
      "0.23989681857265693\n",
      "0.23994839819393682\n",
      "0.24\n",
      "0.24005162400516242\n",
      "0.24010327022375214\n",
      "0.24015493867010973\n",
      "0.24020662935858808\n",
      "0.24025834230355217\n",
      "0.2403100775193798\n",
      "0.24036183502046093\n",
      "0.24041361482119772\n",
      "0.24046541693600518\n",
      "0.24051724137931038\n",
      "0.24056908816555295\n",
      "0.24062095730918498\n",
      "0.24067284882467113\n",
      "0.24072476272648838\n",
      "0.24077669902912627\n",
      "0.24082865774708676\n",
      "0.24088063889488456\n",
      "0.24093264248704663\n",
      "0.24098466853811273\n",
      "0.241036717062635\n",
      "0.24108878807517822\n",
      "0.24114088159031977\n",
      "0.2411929976226497\n",
      "0.2412451361867704\n",
      "0.24129729729729732\n",
      "0.24134948096885814\n",
      "0.24140168721609345\n",
      "0.24145391605365643\n",
      "0.24150616749621293\n",
      "0.24155844155844156\n",
      "0.24161073825503357\n",
      "0.24166305760069293\n",
      "0.2417153996101364\n",
      "0.2417677642980936\n",
      "0.24182015167930662\n",
      "0.24187256176853056\n",
      "0.2419249945805333\n",
      "0.2419774501300954\n",
      "0.2420299284320104\n",
      "0.2420824295010846\n",
      "0.24213495335213714\n",
      "0.2421875\n",
      "0.24224006945951815\n",
      "0.24229266174554925\n",
      "0.2423452768729642\n",
      "0.24238468233246302\n",
      "0.24243743199129486\n",
      "0.24249020461471485\n",
      "0.24254300021772263\n",
      "0.24259581881533104\n",
      "0.24264866042256591\n",
      "0.2427015250544662\n",
      "0.24275441272608408\n",
      "0.2428073234524847\n",
      "0.24286025724874652\n",
      "0.24291321412996075\n",
      "0.24296619411123233\n",
      "0.2430191972076789\n",
      "0.2430722234344316\n",
      "0.24312527280663462\n",
      "0.24317834533944555\n",
      "0.24323144104803493\n",
      "0.24328455994758683\n",
      "0.2433377020532984\n",
      "0.24339086738038015\n",
      "0.24344405594405594\n",
      "0.24349726775956285\n",
      "0.2435505028421513\n",
      "0.24360376120708505\n",
      "0.2436570428696413\n",
      "0.24371034784511048\n",
      "0.24376367614879652\n",
      "0.24381702779601658\n",
      "0.24387040280210162\n",
      "0.24392380118239543\n",
      "0.2439772229522558\n",
      "0.24403066812705365\n",
      "0.24408413672217352\n",
      "0.24413762875301337\n",
      "0.2441911442349847\n",
      "0.2442446831835124\n",
      "0.24429824561403507\n",
      "0.24435183154200488\n",
      "0.24440544098288725\n",
      "0.24445907395216154\n",
      "0.24451273046532046\n",
      "0.2445664105378705\n",
      "0.24462011418533158\n",
      "0.2446738414232374\n",
      "0.24472759226713528\n",
      "0.24478136673258624\n",
      "0.24483516483516482\n",
      "0.24488898659045943\n",
      "0.24494283201407213\n",
      "0.24499670112161867\n",
      "0.24505059392872858\n",
      "0.24510451045104514\n",
      "0.24515845070422532\n",
      "0.24521241470394015\n",
      "0.24526640246587406\n",
      "0.24532041400572557\n",
      "0.24537444933920705\n",
      "0.2454285084820445\n",
      "0.24548259144997797\n",
      "0.24553669825876134\n",
      "0.2455908289241622\n",
      "0.24564498346196253\n",
      "0.24569916188795765\n",
      "0.2457533642179572\n",
      "0.24580759046778464\n",
      "0.24586184065327743\n",
      "0.24591611479028694\n",
      "0.2459704128946787\n",
      "0.24602473498233215\n",
      "0.24607908106914073\n",
      "0.24613345117101196\n",
      "0.24618784530386742\n",
      "0.24624226348364284\n",
      "0.2462967057262879\n",
      "0.24635117204776652\n",
      "0.2464056624640566\n",
      "0.24646017699115044\n",
      "0.2465147156450542\n",
      "0.2465692784417884\n",
      "0.24662386539738768\n",
      "0.24667847652790084\n",
      "0.2467331118493909\n",
      "0.2467877713779353\n",
      "0.24684245512962552\n",
      "0.24689716312056736\n",
      "0.24695189536688095\n",
      "0.2470066518847007\n",
      "0.24706143269017516\n",
      "0.24711623779946762\n",
      "0.24716603689708821\n",
      "0.24722098710538018\n",
      "0.2472759617522793\n",
      "0.24733096085409254\n",
      "0.24738598442714127\n",
      "0.24744103248776148\n",
      "0.2474961050523036\n",
      "0.24755120213713266\n",
      "0.24760632375862837\n",
      "0.24766146993318486\n",
      "0.24771664067721097\n",
      "0.2477718360071301\n",
      "0.24782705593938048\n",
      "0.24788230049041463\n",
      "0.24793756967670014\n",
      "0.247992863514719\n",
      "0.24804818202096807\n",
      "0.24810352521195894\n",
      "0.24815889310421782\n",
      "0.24821428571428572\n",
      "0.2482697030587184\n",
      "0.24832514515408666\n",
      "0.24838061201697567\n",
      "0.2484361036639857\n",
      "0.24849162011173187\n",
      "0.248547161376844\n",
      "0.24860272747596687\n",
      "0.24865831842576028\n",
      "0.24871393424289867\n",
      "0.24876957494407156\n",
      "0.24882524054598346\n",
      "0.24888093106535364\n",
      "0.2489366465189165\n",
      "0.24899238692342138\n",
      "0.24904815229563274\n",
      "0.2491039426523297\n",
      "0.24915975801030696\n",
      "0.24921559838637378\n",
      "0.24927146379735485\n",
      "0.24932735426008967\n",
      "0.24938326979143305\n",
      "0.2494392104082548\n",
      "0.24949517612743996\n",
      "0.24955116696588872\n",
      "0.24960718294051626\n",
      "0.24966322406825328\n",
      "0.24971929036604537\n",
      "0.24977538185085357\n",
      "0.24983149853965397\n",
      "0.2498876404494382\n",
      "0.24994380759721285\n",
      "0.24999999999999994\n",
      "0.250056217674837\n",
      "0.2501124606387764\n",
      "0.25016872890888636\n",
      "0.2502250225022502\n",
      "0.25028134143596664\n",
      "0.2503376857271499\n",
      "0.2503940553929295\n",
      "0.25045045045045045\n",
      "0.2505068709168732\n",
      "0.2505633168093736\n",
      "0.25061978814514313\n",
      "0.2506762849413886\n",
      "0.2507328072153326\n",
      "0.2507893549842129\n",
      "0.2508459282652831\n",
      "0.2509025270758123\n",
      "0.2509591514330851\n",
      "0.2510158013544018\n",
      "0.25107247685707834\n",
      "0.25112917795844625\n",
      "0.2511859046758527\n",
      "0.25124265702666065\n",
      "0.2512994350282486\n",
      "0.2513562386980109\n",
      "0.2514130680533575\n",
      "0.2514699231117141\n",
      "0.2515268038905225\n",
      "0.2515837104072398\n",
      "0.2516406426793392\n",
      "0.25169760072430963\n",
      "0.2517545845596559\n",
      "0.25181159420289856\n",
      "0.2518686296715742\n",
      "0.2519256909832352\n",
      "0.2519827781554498\n",
      "0.25203989120580234\n",
      "0.252097030151893\n",
      "0.2521541950113379\n",
      "0.25221138580176905\n",
      "0.2522686025408349\n",
      "0.2523258452461993\n",
      "0.25238311393554247\n",
      "0.2524404086265607\n",
      "0.25249772933696635\n",
      "0.2525550760844878\n",
      "0.25261244888686957\n",
      "0.2526698477618723\n",
      "0.25272727272727274\n",
      "0.25278472380086386\n",
      "0.2528422010004548\n",
      "0.2528997043438708\n",
      "0.2529572338489536\n",
      "0.2530147895335609\n",
      "0.25307237141556665\n",
      "0.25312997951286137\n",
      "0.2531876138433516\n",
      "0.25324527442496014\n",
      "0.2533029612756264\n",
      "0.253360674413306\n",
      "0.2534184138559708\n",
      "0.25347617962160934\n",
      "0.2535339717282261\n",
      "0.25359179019384265\n",
      "0.25364963503649635\n",
      "0.2537075062742414\n",
      "0.25376540392514835\n",
      "0.2538233280073043\n",
      "0.25388127853881276\n",
      "0.25393925553779395\n",
      "0.2539972590223846\n",
      "0.25405528901073793\n",
      "0.25411334552102377\n",
      "0.25417142857142855\n",
      "0.2542295381801554\n",
      "0.2542876743654242\n",
      "0.25434583714547115\n",
      "0.25440402653854954\n",
      "0.25446224256292904\n",
      "0.2545204852368963\n",
      "0.2545787545787546\n",
      "0.2546370506068239\n",
      "0.2546953733394411\n",
      "0.2547537227949599\n",
      "0.2548120989917507\n",
      "0.2548705019482008\n",
      "0.2549289316827143\n",
      "0.2549873882137125\n",
      "0.25504587155963304\n",
      "0.25510438173893096\n",
      "0.2551629187700781\n",
      "0.255221482671563\n",
      "0.25528007346189163\n",
      "0.2553386911595867\n",
      "0.25539733578318785\n",
      "0.255456007351252\n",
      "0.2555147058823529\n",
      "0.25557343139508165\n",
      "0.25563218390804604\n",
      "0.25569096343987124\n",
      "0.25574977000919963\n",
      "0.25580860363469055\n",
      "0.2558674643350207\n",
      "0.2559263521288838\n",
      "0.25598526703499075\n",
      "0.25604420907207004\n",
      "0.2561031782588669\n",
      "0.2561621746141442\n",
      "0.256221198156682\n",
      "0.25628024890527773\n",
      "0.256339326878746\n",
      "0.25639843209591884\n",
      "0.2564575645756457\n",
      "0.25651672433679357\n",
      "0.2565759113982464\n",
      "0.25663512577890607\n",
      "0.2566943674976916\n",
      "0.25675363657353956\n",
      "0.25681293302540414\n",
      "0.2568722568722569\n",
      "0.25693160813308685\n",
      "0.2569909868269008\n",
      "0.2570503929727231\n",
      "0.2571098265895954\n",
      "0.25716928769657726\n",
      "0.25722877631274577\n",
      "0.25728829245719576\n",
      "0.2573478361490396\n",
      "0.25740740740740736\n",
      "0.2574670062514471\n",
      "0.2575266327003242\n",
      "0.25758628677322215\n",
      "0.25764596848934196\n",
      "0.2577056778679027\n",
      "0.257765414928141\n",
      "0.25782517968931135\n",
      "0.25788497217068646\n",
      "0.25794479239155654\n",
      "0.25800464037122967\n",
      "0.25806451612903225\n",
      "0.25812441968430827\n",
      "0.2581843510564198\n",
      "0.25824431026474687\n",
      "0.2583042973286876\n",
      "0.258364312267658\n",
      "0.25842435510109224\n",
      "0.2584844258484426\n",
      "0.25854452452917925\n",
      "0.2586046511627907\n",
      "0.2586648057687835\n",
      "0.2587249883666822\n",
      "0.25878519897602975\n",
      "0.2588454376163874\n",
      "0.2589057043073341\n",
      "0.25896599906846757\n",
      "0.2590263219194037\n",
      "0.2590866728797763\n",
      "0.25914705196923793\n",
      "0.25920745920745925\n",
      "0.2592678946141292\n",
      "0.25932835820895517\n",
      "0.2593888500116632\n",
      "0.2594493700419972\n",
      "0.25950991831972\n",
      "0.25957049486461253\n",
      "0.2596310996964744\n",
      "0.25969173283512376\n",
      "0.2597523943003971\n",
      "0.25981308411214954\n",
      "0.25987380229025475\n",
      "0.25993454885460493\n",
      "0.25999532382511104\n",
      "0.26005612722170257\n",
      "0.2601169590643275\n",
      "0.2601778193729527\n",
      "0.2602387081675638\n",
      "0.26029962546816476\n",
      "0.26036057129477874\n",
      "0.2604215456674473\n",
      "0.260482548606231\n",
      "0.26054358013120904\n",
      "0.2606046402624795\n",
      "0.2606657290201594\n",
      "0.26072684642438454\n",
      "0.2607879924953096\n",
      "0.26084916725310814\n",
      "0.26091037071797274\n",
      "0.26097160291011495\n",
      "0.26103286384976526\n",
      "0.2610941535571731\n",
      "0.2611554720526068\n",
      "0.26121681935635427\n",
      "0.2612781954887219\n",
      "0.2613396004700353\n",
      "0.2614010343206394\n",
      "0.26146249706089814\n",
      "0.26152398871119475\n",
      "0.2615855092919313\n",
      "0.26164705882352945\n",
      "0.26170863732642974\n",
      "0.26177024482109224\n",
      "0.26183188132799623\n",
      "0.26189354686764016\n",
      "0.2619552414605418\n",
      "0.2620169651272385\n",
      "0.26207871788828657\n",
      "0.2621404997642621\n",
      "0.26220231077576045\n",
      "0.2622641509433962\n",
      "0.2623260202878037\n",
      "0.2623879188296366\n",
      "0.2624498465895681\n",
      "0.26251180358829085\n",
      "0.26257378984651714\n",
      "0.26263580538497877\n",
      "0.26269785022442715\n",
      "0.2627599243856333\n",
      "0.26282202788938785\n",
      "0.2628841607565012\n",
      "0.2629463230078033\n",
      "0.26300851466414377\n",
      "0.2630707357463922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26313298627543774\n",
      "0.26319526627218937\n",
      "0.26325757575757575\n",
      "0.26331991475254557\n",
      "0.2633822832780673\n",
      "0.2634446813551291\n",
      "0.26350710900473934\n",
      "0.26356956624792605\n",
      "0.26363205310573734\n",
      "0.2636945695992412\n",
      "0.26375711574952565\n",
      "0.26381969157769863\n",
      "0.26388229710488853\n",
      "0.2639449323522431\n",
      "0.2640075973409307\n",
      "0.26407029209213967\n",
      "0.2641330166270784\n",
      "0.26419577096697555\n",
      "0.2642499403768185\n",
      "0.2643129770992366\n",
      "0.264376043903603\n",
      "0.26443914081145586\n",
      "0.26450226784435427\n",
      "0.26456542502387775\n",
      "0.2646286123716265\n",
      "0.2646918299092212\n",
      "0.2647550776583035\n",
      "0.2648183556405354\n",
      "0.26488166387759987\n",
      "0.2649450023912004\n",
      "0.2650083712030615\n",
      "0.26507177033492824\n",
      "0.2651351998085667\n",
      "0.2651986596457635\n",
      "0.2652621498683266\n",
      "0.2653256704980843\n",
      "0.26538922155688627\n",
      "0.2654528030666028\n",
      "0.2655164150491253\n",
      "0.26558005752636626\n",
      "0.26564373052025897\n",
      "0.26570743405275776\n",
      "0.26577116814583834\n",
      "0.2658349328214971\n",
      "0.2658987281017518\n",
      "0.2659625540086413\n",
      "0.26602641056422566\n",
      "0.26609029779058596\n",
      "0.26615421570982467\n",
      "0.2662181643440653\n",
      "0.26628214371545306\n",
      "0.2663461538461538\n",
      "0.26641019475835537\n",
      "0.2664742664742665\n",
      "0.2665383690161174\n",
      "0.26660250240615974\n",
      "0.26666666666666666\n",
      "0.26673086181993255\n",
      "0.2667950878882735\n",
      "0.266859344894027\n",
      "0.2669236328595519\n",
      "0.2669879518072289\n",
      "0.26705230175946015\n",
      "0.26711668273866923\n",
      "0.2671810947673016\n",
      "0.2672455378678244\n",
      "0.2673100120627262\n",
      "0.2673745173745174\n",
      "0.26743905382573013\n",
      "0.26750362143891837\n",
      "0.2675682202366578\n",
      "0.26763285024154587\n",
      "0.26769751147620197\n",
      "0.2677622039632673\n",
      "0.2678269277254049\n",
      "0.26789168278529985\n",
      "0.267956469165659\n",
      "0.26802128688921145\n",
      "0.268086135978708\n",
      "0.2681510164569216\n",
      "0.2682159283466473\n",
      "0.2682808716707022\n",
      "0.26834584645192544\n",
      "0.26841085271317827\n",
      "0.2684758904773443\n",
      "0.2685409597673291\n",
      "0.2686060606060606\n",
      "0.26867119301648884\n",
      "0.26873635702158627\n",
      "0.2688015526443474\n",
      "0.2688667799077894\n",
      "0.2689320388349515\n",
      "0.2689973294488953\n",
      "0.26906265177270516\n",
      "0.2691280058294874\n",
      "0.2691933916423712\n",
      "0.26925880923450785\n",
      "0.26932425862907144\n",
      "0.26938973984925846\n",
      "0.26945525291828787\n",
      "0.26952079785940164\n",
      "0.2695863746958638\n",
      "0.2696519834509613\n",
      "0.2697176241480039\n",
      "0.2697832968103238\n",
      "0.26984900146127616\n",
      "0.2699147381242387\n",
      "0.2699805068226121\n",
      "0.2700463075798196\n",
      "0.2701121404193077\n",
      "0.2701780053645452\n",
      "0.2702439024390244\n",
      "0.2703098316662601\n",
      "0.27037579306979015\n",
      "0.27044178667317553\n",
      "0.2705078125\n",
      "0.2705738705738706\n",
      "0.2706399609184172\n",
      "0.2707060835572929\n",
      "0.270772238514174\n",
      "0.2708384258127597\n",
      "0.2709046454767726\n",
      "0.2709708975299584\n",
      "0.27103718199608606\n",
      "0.27110349889894786\n",
      "0.2711698482623593\n",
      "0.2712362301101591\n",
      "0.2713026444662096\n",
      "0.27136909135439624\n",
      "0.2714355707986281\n",
      "0.27150208282283755\n",
      "0.27156862745098037\n",
      "0.27163520470703606\n",
      "0.27170181461500736\n",
      "0.2717684571989208\n",
      "0.27183513248282626\n",
      "0.2719018404907975\n",
      "0.27196858124693174\n",
      "0.27203535477534985\n",
      "0.27210216110019647\n",
      "0.2721690002456399\n",
      "0.2722358722358722\n",
      "0.27230277709510936\n",
      "0.272369714847591\n",
      "0.2724366855175805\n",
      "0.27250368912936546\n",
      "0.27257072570725704\n",
      "0.27263779527559057\n",
      "0.2727048978587251\n",
      "0.2727720334810438\n",
      "0.27283920216695395\n",
      "0.27290640394088667\n",
      "0.2729736388272974\n",
      "0.2730409068506654\n",
      "0.2731082080354942\n",
      "0.27317554240631164\n",
      "0.27324290998766954\n",
      "0.2733103108041441\n",
      "0.27337774488033556\n",
      "0.2734452122408687\n",
      "0.27351271291039253\n",
      "0.27358024691358024\n",
      "0.2736478142751297\n",
      "0.2737154150197629\n",
      "0.2737830491722263\n",
      "0.27385071675729117\n",
      "0.2739184177997528\n",
      "0.27398615232443124\n",
      "0.27405392035617115\n",
      "0.2741217219198417\n",
      "0.27418955704033654\n",
      "0.2742574257425742\n",
      "0.2743253280514979\n",
      "0.2743932639920753\n",
      "0.27446123358929897\n",
      "0.2745292368681863\n",
      "0.2745972738537794\n",
      "0.2746653445711452\n",
      "0.27473344904537567\n",
      "0.27480158730158727\n",
      "0.27486975936492186\n",
      "0.2749379652605459\n",
      "0.27500620501365103\n",
      "0.27507447864945384\n",
      "0.27514278619319593\n",
      "0.27521112767014405\n",
      "0.2752795031055901\n",
      "0.27534791252485086\n",
      "0.27541635595326874\n",
      "0.27548483341621083\n",
      "0.2755533449390699\n",
      "0.27562189054726366\n",
      "0.2756904702662354\n",
      "0.2757590841214535\n",
      "0.2758277321384117\n",
      "0.27589641434262946\n",
      "0.27596513075965134\n",
      "0.27603388141504737\n",
      "0.27610266633441316\n",
      "0.27617148554336984\n",
      "0.2762403390675642\n",
      "0.27630922693266835\n",
      "0.27637814916438014\n",
      "0.2764471057884232\n",
      "0.2765160968305465\n",
      "0.2765851223165252\n",
      "0.2766541822721598\n",
      "0.27672327672327673\n",
      "0.2767924056957282\n",
      "0.27686156921539234\n",
      "0.276930767308173\n",
      "0.277\n",
      "0.2770692673168292\n",
      "0.2771385692846423\n",
      "0.27720790592944705\n",
      "0.2772772772772773\n",
      "0.27734668335419277\n",
      "0.27741612418627937\n",
      "0.2774855997996494\n",
      "0.2775551102204409\n",
      "0.27762465547481835\n",
      "0.27769423558897244\n",
      "0.27776385058912006\n",
      "0.2778335005015045\n",
      "0.2779031853523953\n",
      "0.2779729051680883\n",
      "0.27804265997490585\n",
      "0.27811244979919675\n",
      "0.2781822746673362\n",
      "0.2782521346057258\n",
      "0.2783220296407938\n",
      "0.278391959798995\n",
      "0.2784619251068108\n",
      "0.27853192559074913\n",
      "0.27860196127734477\n",
      "0.27867203219315895\n",
      "0.27874213836477985\n",
      "0.27881227981882234\n",
      "0.27888245658192806\n",
      "0.2789526686807654\n",
      "0.27902291614202973\n",
      "0.27909319899244334\n",
      "0.2791635172587553\n",
      "0.2792338709677419\n",
      "0.27930426014620624\n",
      "0.2793746848209783\n",
      "0.27944514501891554\n",
      "0.27951564076690216\n",
      "0.2795861720918496\n",
      "0.27965673902069665\n",
      "0.27972734158040896\n",
      "0.2797979797979798\n",
      "0.2798686537004294\n",
      "0.27993936331480546\n",
      "0.280010108668183\n",
      "0.28008088978766427\n",
      "0.28015170670037925\n",
      "0.28022255943348506\n",
      "0.28029344801416645\n",
      "0.28036437246963564\n",
      "0.28043533282713234\n",
      "0.2805063291139241\n",
      "0.2805773613573057\n",
      "0.2806484295845998\n",
      "0.28071953382315684\n",
      "0.2807906741003548\n",
      "0.2808618504435995\n",
      "0.2809330628803245\n",
      "0.28100431143799137\n",
      "0.2810755961440893\n",
      "0.2811469170261355\n",
      "0.2812182741116751\n",
      "0.2812896674282813\n",
      "0.2813610970035551\n",
      "0.2814325628651257\n",
      "0.2815040650406504\n",
      "0.28157560355781447\n",
      "0.28164717844433146\n",
      "0.28171878972794306\n",
      "0.2817904374364191\n",
      "0.28186212159755786\n",
      "0.2819338422391858\n",
      "0.2820055993891576\n",
      "0.2820773930753564\n",
      "0.28214922332569387\n",
      "0.2822210901681101\n",
      "0.28229299363057325\n",
      "0.2823649337410805\n",
      "0.2824369105276574\n",
      "0.282508924018358\n",
      "0.28258097424126494\n",
      "0.28265306122448974\n",
      "0.28272518499617255\n",
      "0.2827973455844819\n",
      "0.2828695430176156\n",
      "0.2829417773237999\n",
      "0.2830140485312899\n",
      "0.28308635666836995\n",
      "0.2831587017633529\n",
      "0.2832310838445808\n",
      "0.28330350294042445\n",
      "0.2833759590792839\n",
      "0.28344845228958815\n",
      "0.28352098259979525\n",
      "0.28359355003839265\n",
      "0.28366615463389655\n",
      "0.28373879641485283\n",
      "0.28381147540983603\n",
      "0.28388419164745066\n",
      "0.2839569451563301\n",
      "0.28402973596513714\n",
      "0.2841025641025641\n",
      "0.28417542959733266\n",
      "0.2842483324781939\n",
      "0.2843212727739287\n",
      "0.284394250513347\n",
      "0.28446726572528885\n",
      "0.28454031843862354\n",
      "0.2846134086822502\n",
      "0.2846865364850977\n",
      "0.28475970187612437\n",
      "0.2848329048843188\n",
      "0.28490614553869886\n",
      "0.2849794238683127\n",
      "0.28505273990223823\n",
      "0.28512609366958314\n",
      "0.2851994851994852\n",
      "0.2852729145211122\n",
      "0.2853463816636621\n",
      "0.28541988665636264\n",
      "0.2854934295284721\n",
      "0.28556701030927834\n",
      "0.2856406290281\n",
      "0.28571428571428575\n",
      "0.2857879803972143\n",
      "0.28586171310629516\n",
      "0.28593548387096773\n",
      "0.28600929272070214\n",
      "0.2860831396849987\n",
      "0.28615702479338845\n",
      "0.2862309480754327\n",
      "0.2863049095607235\n",
      "0.28637890927888343\n",
      "0.2864529472595657\n",
      "0.2865270235324541\n",
      "0.28660113812726334\n",
      "0.2866752910737387\n",
      "0.2867494824016563\n",
      "0.2868237121408232\n",
      "0.2868979803210772\n",
      "0.28697228697228694\n",
      "0.2870466321243523\n",
      "0.28712101580720395\n",
      "0.28719543805080355\n",
      "0.28726989888514387\n",
      "0.287344398340249\n",
      "0.2874189364461738\n",
      "0.28749351323300465\n",
      "0.2875681287308591\n",
      "0.2876427829698857\n",
      "0.28771747598026487\n",
      "0.2877922077922078\n",
      "0.2878669784359573\n",
      "0.287941787941788\n",
      "0.28801663634000524\n",
      "0.2880915236609464\n",
      "0.28816644993498053\n",
      "0.28824141519250784\n",
      "0.2883164194639604\n",
      "0.2883914627798022\n",
      "0.2884665451705285\n",
      "0.28854166666666664\n",
      "0.28861682729877575\n",
      "0.28869202709744657\n",
      "0.28876726609330206\n",
      "0.28884254431699685\n",
      "0.2889178617992177\n",
      "0.2889932185706834\n",
      "0.2890686146621445\n",
      "0.2891440501043841\n",
      "0.2892195249282172\n",
      "0.28929503916449084\n",
      "0.28937059284408456\n",
      "0.28944618599791017\n",
      "0.2895218186569114\n",
      "0.2895974908520648\n",
      "0.28967320261437907\n",
      "0.2897489539748954\n",
      "0.2898247449646874\n",
      "0.2899005756148614\n",
      "0.28997644595655586\n",
      "0.2900523560209424\n",
      "0.29012830583922494\n",
      "0.29020429544264015\n",
      "0.2902803248624574\n",
      "0.29035639412997905\n",
      "0.29043250327653997\n",
      "0.29050865233350814\n",
      "0.2905848413322843\n",
      "0.2906610703043022\n",
      "0.29073733928102863\n",
      "0.2908136482939633\n",
      "0.29088999737463905\n",
      "0.2909663865546218\n",
      "0.2910428158655109\n",
      "0.2911192853389385\n",
      "0.2911957950065703\n",
      "0.2912723449001052\n",
      "0.29134893505127535\n",
      "0.2914255654918464\n",
      "0.2915022362536175\n",
      "0.29157894736842105\n",
      "0.29165569886812326\n",
      "0.29173249078462354\n",
      "0.2918093231498552\n",
      "0.291886195995785\n",
      "0.2919631093544137\n",
      "0.2920400632577754\n",
      "0.2921170577379383\n",
      "0.2921940928270042\n",
      "0.29227116855710894\n",
      "0.29234828496042214\n",
      "0.2924254420691475\n",
      "0.29250263991552267\n",
      "0.2925798785318194\n",
      "0.29265715795034336\n",
      "0.29273447820343457\n",
      "0.29281183932346727\n",
      "0.2928892413428496\n",
      "0.29296668429402434\n",
      "0.2930441682094684\n",
      "0.29312169312169317\n",
      "0.29319925906324423\n",
      "0.29327686606670195\n",
      "0.293354514164681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2934322033898305\n",
      "0.2935099337748345\n",
      "0.2935877053524112\n",
      "0.2936655181553141\n",
      "0.29374337221633084\n",
      "0.2938212675682843\n",
      "0.2938992042440318\n",
      "0.2939771822764659\n",
      "0.29405520169851385\n",
      "0.29413326254313776\n",
      "0.2942113648433351\n",
      "0.29428950863213815\n",
      "0.2943676939426142\n",
      "0.2944459208078661\n",
      "0.2945241892610313\n",
      "0.29460249933528315\n",
      "0.2946808510638298\n",
      "0.29475924447991486\n",
      "0.2948376796168175\n",
      "0.29491615650785197\n",
      "0.29499467518636846\n",
      "0.2950732356857523\n",
      "0.29515183803942463\n",
      "0.295230482280842\n",
      "0.2953091684434968\n",
      "0.2953878965609171\n",
      "0.29546666666666666\n",
      "0.2955454787943451\n",
      "0.29562433297758806\n",
      "0.29570322925006676\n",
      "0.29578216764548854\n",
      "0.2958611481975968\n",
      "0.29594017094017094\n",
      "0.29601923590702645\n",
      "0.296098343132015\n",
      "0.2961774926490243\n",
      "0.29625668449197856\n",
      "0.2963359186948382\n",
      "0.29641519529159976\n",
      "0.2964945143162965\n",
      "0.2965738758029978\n",
      "0.2966532797858099\n",
      "0.2967327262988752\n",
      "0.2968122153763729\n",
      "0.2968917470525188\n",
      "0.2969713213615653\n",
      "0.29705093833780166\n",
      "0.29713059801555375\n",
      "0.2972103004291845\n",
      "0.29729004561309363\n",
      "0.29736983360171765\n",
      "0.29744966442953025\n",
      "0.2975295381310419\n",
      "0.2976094547408004\n",
      "0.29768941429339063\n",
      "0.29776941682343455\n",
      "0.29784946236559134\n",
      "0.29792955095455764\n",
      "0.2980096826250672\n",
      "0.29808985741189137\n",
      "0.2981700753498385\n",
      "0.29825033647375504\n",
      "0.2983306408185245\n",
      "0.29841098841906816\n",
      "0.29849137931034486\n",
      "0.29857181352735107\n",
      "0.29865229110512126\n",
      "0.2987328120787274\n",
      "0.2988133764832794\n",
      "0.298893984353925\n",
      "0.29897463572585\n",
      "0.29905533063427797\n",
      "0.2991360691144708\n",
      "0.29921685120172836\n",
      "0.2992976769313884\n",
      "0.2993785463388273\n",
      "0.29945945945945945\n",
      "0.29954041632873757\n",
      "0.2996214169821525\n",
      "0.29970246145523394\n",
      "0.29978354978354976\n",
      "0.2998646820027064\n",
      "0.2999458581483487\n",
      "0.3000270782561603\n",
      "0.3001083423618635\n",
      "0.3001896505012192\n",
      "0.30027100271002705\n",
      "0.3003523990241258\n",
      "0.3004338394793927\n",
      "0.300515324111744\n",
      "0.3005968529571351\n",
      "0.30067842605156037\n",
      "0.3007600434310532\n",
      "0.30084170513168607\n",
      "0.3009234111895709\n",
      "0.30100516164085844\n",
      "0.3010869565217391\n",
      "0.3011687958684425\n",
      "0.3012506797172376\n",
      "0.3013326081044329\n",
      "0.3014145810663765\n",
      "0.3014965986394558\n",
      "0.30157866086009805\n",
      "0.30166076776476997\n",
      "0.3017429193899782\n",
      "0.30182511577226917\n",
      "0.3019073569482289\n",
      "0.3019896429544835\n",
      "0.302071973827699\n",
      "0.3021543496045814\n",
      "0.30223677032187674\n",
      "0.3023192360163711\n",
      "0.30240174672489084\n",
      "0.3024843024843025\n",
      "0.30256690333151287\n",
      "0.302649549303469\n",
      "0.30273224043715846\n",
      "0.30281497676960917\n",
      "0.3028977583378895\n",
      "0.3029805851791086\n",
      "0.3030634573304158\n",
      "0.30314637482900136\n",
      "0.3032293377120963\n",
      "0.3033123460169723\n",
      "0.30339539978094193\n",
      "0.30347849904135854\n",
      "0.30356164383561646\n",
      "0.303644834201151\n",
      "0.3037280701754386\n",
      "0.3038113517959967\n",
      "0.303894679100384\n",
      "0.3039780521262002\n",
      "0.3040614709110867\n",
      "0.30414493549272575\n",
      "0.30422844590884135\n",
      "0.30431200219719856\n",
      "0.30439560439560437\n",
      "0.30447925254190705\n",
      "0.3045629466739967\n",
      "0.3046466868298048\n",
      "0.3047304730473047\n",
      "0.3048143053645117\n",
      "0.30489818381948264\n",
      "0.30498210845031654\n",
      "0.30506607929515417\n",
      "0.30515009639217844\n",
      "0.3052341597796144\n",
      "0.3053182694957288\n",
      "0.30540242557883135\n",
      "0.3054866280672732\n",
      "0.3055708769994484\n",
      "0.3056551724137931\n",
      "0.3057395143487858\n",
      "0.30582390284294786\n",
      "0.3059083379348426\n",
      "0.30599281966307645\n",
      "0.3060773480662983\n",
      "0.30616192318319985\n",
      "0.3062465450525152\n",
      "0.30633121371302185\n",
      "0.3064159292035398\n",
      "0.3065006915629322\n",
      "0.30658550083010516\n",
      "0.30667035704400775\n",
      "0.30675526024363237\n",
      "0.3068402104680144\n",
      "0.3069252077562327\n",
      "0.3070102521474093\n",
      "0.3070953436807095\n",
      "0.30718048239534235\n",
      "0.3072656683305602\n",
      "0.3073509015256588\n",
      "0.30743618201997785\n",
      "0.30752150985290033\n",
      "0.3076068850638534\n",
      "0.3076923076923077\n",
      "0.3077777777777778\n",
      "0.30786329535982215\n",
      "0.3079488604780434\n",
      "0.3080344731720879\n",
      "0.3081201334816463\n",
      "0.30820584144645335\n",
      "0.3082915971062883\n",
      "0.30837740050097406\n",
      "0.30846325167037864\n",
      "0.3085491506544138\n",
      "0.3086350974930362\n",
      "0.30872109222624683\n",
      "0.3088071348940914\n",
      "0.3088932255366602\n",
      "0.3089793641940881\n",
      "0.30906555090655513\n",
      "0.30915178571428575\n",
      "0.3092380686575495\n",
      "0.3093243997766611\n",
      "0.3094107791119799\n",
      "0.3094972067039106\n",
      "0.30958368259290303\n",
      "0.30967020681945223\n",
      "0.3097567794240984\n",
      "0.3098434004474273\n",
      "0.3099300699300699\n",
      "0.3100167879127028\n",
      "0.31010355443604815\n",
      "0.3101903695408734\n",
      "0.3102772332679921\n",
      "0.3103641456582633\n",
      "0.31045110675259174\n",
      "0.31053811659192826\n",
      "0.31062517521726946\n",
      "0.3107122826696579\n",
      "0.31079943899018236\n",
      "0.3108866442199776\n",
      "0.31097389840022455\n",
      "0.31106120157215045\n",
      "0.31114855377702894\n",
      "0.3112359550561798\n",
      "0.3113234054509694\n",
      "0.31141090500281055\n",
      "0.31149845375316276\n",
      "0.31158605174353204\n",
      "0.31167369901547115\n",
      "0.31176139561057964\n",
      "0.31184914157050386\n",
      "0.31193693693693697\n",
      "0.31202478175161924\n",
      "0.312112676056338\n",
      "0.31220061989292763\n",
      "0.31228861330326946\n",
      "0.31237665632929235\n",
      "0.3124647490129724\n",
      "0.3125528913963328\n",
      "0.31264108352144476\n",
      "0.31272932543042614\n",
      "0.31281761716544326\n",
      "0.31290595876870936\n",
      "0.3129943502824859\n",
      "0.3130827917490817\n",
      "0.3131712832108536\n",
      "0.3132598247102064\n",
      "0.3133484162895928\n",
      "0.31343705799151345\n",
      "0.3135257498585172\n",
      "0.31361449193320123\n",
      "0.31370328425821065\n",
      "0.31379212687623903\n",
      "0.3138810198300283\n",
      "0.31396996316236897\n",
      "0.31405895691609975\n",
      "0.3141480011341083\n",
      "0.3142370958593307\n",
      "0.31432624113475177\n",
      "0.3144154370034052\n",
      "0.3145046835083736\n",
      "0.3145939806927882\n",
      "0.3146833285998296\n",
      "0.31477272727272726\n",
      "0.31486217675475986\n",
      "0.3149516770892553\n",
      "0.3150412283195906\n",
      "0.31513083048919227\n",
      "0.31522048364153626\n",
      "0.3153101878201479\n",
      "0.3153999430686023\n",
      "0.31548974943052394\n",
      "0.315579606949587\n",
      "0.31566951566951573\n",
      "0.3157594756340838\n",
      "0.3158494868871151\n",
      "0.31593954947248354\n",
      "0.316029663434113\n",
      "0.3161198288159772\n",
      "0.3162100456621004\n",
      "0.3163003140165572\n",
      "0.31639063392347233\n",
      "0.31648100542702085\n",
      "0.31657142857142856\n",
      "0.3166619034009718\n",
      "0.3167524299599771\n",
      "0.3168430082928224\n",
      "0.31693363844393596\n",
      "0.3170243204577968\n",
      "0.31711505437893533\n",
      "0.3172058402519325\n",
      "0.3172966781214204\n",
      "0.31738756803208246\n",
      "0.3174785100286533\n",
      "0.3175695041559186\n",
      "0.31766055045871555\n",
      "0.31775164898193287\n",
      "0.31784279977051055\n",
      "0.31793400286944046\n",
      "0.3180252583237657\n",
      "0.3181165661785817\n",
      "0.318207926479035\n",
      "0.3182993392703246\n",
      "0.31839080459770114\n",
      "0.3184823225064674\n",
      "0.3185738930419781\n",
      "0.3186655162496405\n",
      "0.3187571921749137\n",
      "0.3188489208633094\n",
      "0.31894070236039146\n",
      "0.3190325367117766\n",
      "0.3191244239631336\n",
      "0.3192163641601844\n",
      "0.31930835734870316\n",
      "0.31940040357451716\n",
      "0.31949250288350634\n",
      "0.3195846553216037\n",
      "0.31967686093479514\n",
      "0.3197691197691198\n",
      "0.3198614318706698\n",
      "0.31995379728559054\n",
      "0.3200462160600809\n",
      "0.32013868824039293\n",
      "0.3202312138728324\n",
      "0.3203237930037584\n",
      "0.3204164256795836\n",
      "0.32050911194677467\n",
      "0.3206018518518518\n",
      "0.32069464544138926\n",
      "0.320787492762015\n",
      "0.32088039386041123\n",
      "0.32097334878331407\n",
      "0.3210663575775138\n",
      "0.321159420289855\n",
      "0.3212525369672369\n",
      "0.3213457076566126\n",
      "0.3214389324049899\n",
      "0.32153221125943127\n",
      "0.3216255442670537\n",
      "0.32171893147502906\n",
      "0.3218123729305838\n",
      "0.32190586868099946\n",
      "0.3219994187736123\n",
      "0.32209302325581396\n",
      "0.32218668217505086\n",
      "0.32228039557882493\n",
      "0.32237416351469306\n",
      "0.3224679860302678\n",
      "0.32256186317321683\n",
      "0.3226557949912638\n",
      "0.3227497815321876\n",
      "0.32284382284382285\n",
      "0.32293791897406005\n",
      "0.3230320699708455\n",
      "0.3231262758821814\n",
      "0.323220536756126\n",
      "0.3233148526407937\n",
      "0.323409223584355\n",
      "0.32350364963503647\n",
      "0.3235981308411215\n",
      "0.3236926672509495\n",
      "0.32378725891291643\n",
      "0.323881905875475\n",
      "0.32397660818713453\n",
      "0.32407136589646096\n",
      "0.32416617905207723\n",
      "0.3242610477026631\n",
      "0.3243559718969555\n",
      "0.32445095168374816\n",
      "0.3245459871118922\n",
      "0.3246410782302959\n",
      "0.324736225087925\n",
      "0.32483142773380236\n",
      "0.32492668621700876\n",
      "0.32502200058668235\n",
      "0.32511737089201875\n",
      "0.3252127971822718\n",
      "0.32540381791483114\n",
      "0.32549941245593417\n",
      "0.32559506317954745\n",
      "0.3256907701352146\n",
      "0.3257865333725375\n",
      "0.32588235294117646\n",
      "0.32597822889085026\n",
      "0.3260741612713361\n",
      "0.32617015013246986\n",
      "0.32626619552414604\n",
      "0.3263622974963181\n",
      "0.32645845609899826\n",
      "0.3265546713822576\n",
      "0.3266509433962264\n",
      "0.32674727219109406\n",
      "0.32684365781710917\n",
      "0.32694010032457954\n",
      "0.3270365997638725\n",
      "0.32713315618541483\n",
      "0.32722976963969286\n",
      "0.3273264401772526\n",
      "0.32742316784869974\n",
      "0.32751995270469997\n",
      "0.3276167947959787\n",
      "0.3277136941733215\n",
      "0.32781065088757394\n",
      "0.32790766498964186\n",
      "0.3280047365304914\n",
      "0.3281018655611489\n",
      "0.32819905213270145\n",
      "0.3282962962962963\n",
      "0.32839359810314167\n",
      "0.32849095760450636\n",
      "0.3285883748517201\n",
      "0.3286858498961732\n",
      "0.3287833827893175\n",
      "0.32888097358266555\n",
      "0.32897862232779096\n",
      "0.3290763290763291\n",
      "0.3291740938799762\n",
      "0.32927191679049034\n",
      "0.32936979785969084\n",
      "0.32946773713945887\n",
      "0.32956573468173705\n",
      "0.32966379053853023\n",
      "0.32976190476190476\n",
      "0.32986007740398926\n",
      "0.3299583085169744\n",
      "0.33005659815311283\n",
      "0.3301549463647199\n",
      "0.3302533532041729\n",
      "0.3303518187239118\n",
      "0.330450342976439\n",
      "0.3305489260143198\n",
      "0.33064756789018207\n",
      "0.3307462686567164\n",
      "0.3308450283666766\n",
      "0.33094384707287927\n",
      "0.33104272482820435\n",
      "0.3311416616855947\n",
      "0.3312406576980568\n",
      "0.33133971291866027\n",
      "0.3314388274005384\n",
      "0.33153800119688814\n",
      "0.33163723436096976\n",
      "0.3317365269461078\n",
      "0.3318358790056903\n",
      "0.33193529059316956\n",
      "0.3320347617620617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3321342925659473\n",
      "0.3322338830584708\n",
      "0.3323335332933413\n",
      "0.33243324332433244\n",
      "0.33253301320528206\n",
      "0.33263284299009305\n",
      "0.33273273273273274\n",
      "0.33283268248723336\n",
      "0.3329326923076923\n",
      "0.3330327622482717\n",
      "0.3331328923631991\n",
      "0.3332330827067669\n",
      "0.33333333333333337\n",
      "0.3334336442973217\n",
      "0.333534015653221\n",
      "0.3336344474555856\n",
      "0.3337349397590362\n",
      "0.33383549261825857\n",
      "0.33393610608800484\n",
      "0.3340367802230932\n",
      "0.3341375150784077\n",
      "0.334238310708899\n",
      "0.3343391671695836\n",
      "0.33444008451554486\n",
      "0.33454106280193235\n",
      "0.3346421020839625\n",
      "0.33474320241691835\n",
      "0.33484436385614985\n",
      "0.3349455864570738\n",
      "0.3350468702751738\n",
      "0.33514821536600115\n",
      "0.3352496217851739\n",
      "0.3353510895883777\n",
      "0.3354526188313654\n",
      "0.33555420956995763\n",
      "0.3356558618600424\n",
      "0.33575757575757575\n",
      "0.3358593513185814\n",
      "0.335961188599151\n",
      "0.3360630876554444\n",
      "0.33616504854368934\n",
      "0.3362670713201821\n",
      "0.33636915604128725\n",
      "0.3364713027634376\n",
      "0.33657351154313486\n",
      "0.3366757824369493\n",
      "0.33677811550151976\n",
      "0.3368805107935543\n",
      "0.33698296836982966\n",
      "0.337085488287192\n",
      "0.33718807060255634\n",
      "0.33729071537290717\n",
      "0.3373934226552984\n",
      "0.33749619250685353\n",
      "0.33759902498476535\n",
      "0.3377019201462968\n",
      "0.3378048780487805\n",
      "0.3379078987496188\n",
      "0.3380109823062843\n",
      "0.3381141287763198\n",
      "0.33821733821733824\n",
      "0.3383206106870229\n",
      "0.33832976445396146\n",
      "0.3384332925336597\n",
      "0.33853688399142945\n",
      "0.33864053888548684\n",
      "0.3387442572741195\n",
      "0.3388480392156863\n",
      "0.3389518847686178\n",
      "0.33905579399141633\n",
      "0.33915976694265565\n",
      "0.33926380368098163\n",
      "0.339367904265112\n",
      "0.3394720687538367\n",
      "0.33957629720601784\n",
      "0.3396805896805897\n",
      "0.3396923076923077\n",
      "0.33979686057248387\n",
      "0.3399014778325123\n",
      "0.3400061595318756\n",
      "0.3401109057301294\n",
      "0.34021571648690296\n",
      "0.3403205918618989\n",
      "0.34042553191489366\n",
      "0.3405305367057372\n",
      "0.34063560629435363\n",
      "0.34074074074074073\n",
      "0.34084594010497066\n",
      "0.3409512044471896\n",
      "0.3410565338276182\n",
      "0.34116192830655134\n",
      "0.34126738794435857\n",
      "0.34137291280148424\n",
      "0.3414785029384473\n",
      "0.34158415841584155\n",
      "0.34168987929433614\n",
      "0.3417956656346749\n",
      "0.34190151749767733\n",
      "0.3420074349442379\n",
      "0.3421134180353269\n",
      "0.34221946683199006\n",
      "0.34232558139534885\n",
      "0.34243176178660045\n",
      "0.34253800806701834\n",
      "0.3426443202979516\n",
      "0.34275069854082585\n",
      "0.34285714285714286\n",
      "0.3429636533084809\n",
      "0.3430702299564947\n",
      "0.3431768728629158\n",
      "0.34328358208955223\n",
      "0.3433903576982893\n",
      "0.343497199751089\n",
      "0.3436041083099906\n",
      "0.3437110834371109\n",
      "0.3438181251946434\n",
      "0.3439252336448598\n",
      "0.3440324088501091\n",
      "0.344139650872818\n",
      "0.3442469597754911\n",
      "0.34435433562071116\n",
      "0.34446177847113885\n",
      "0.3445692883895131\n",
      "0.3446768654386512\n",
      "0.3447845096814491\n",
      "0.34489222118088103\n",
      "0.345\n",
      "0.34510784620193813\n",
      "0.34521575984990616\n",
      "0.34532374100719426\n",
      "0.3454317897371714\n",
      "0.3455399061032864\n",
      "0.345648090169067\n",
      "0.3457563419981209\n",
      "0.3458646616541354\n",
      "0.34597304920087746\n",
      "0.34608150470219434\n",
      "0.3461900282220132\n",
      "0.34629861982434124\n",
      "0.34640727957326645\n",
      "0.34651600753295664\n",
      "0.34662480376766086\n",
      "0.3467336683417085\n",
      "0.34678436317780575\n",
      "0.3468937243771681\n",
      "0.3470031545741325\n",
      "0.34711265383401707\n",
      "0.3472222222222222\n",
      "0.34733185980423115\n",
      "0.3474415666456096\n",
      "0.34755134281200634\n",
      "0.347661188369153\n",
      "0.3477711033828644\n",
      "0.34788108791903855\n",
      "0.3479911420436571\n",
      "0.3481012658227848\n",
      "0.34821145932257047\n",
      "0.3483217226092463\n",
      "0.34843205574912894\n",
      "0.3485424588086185\n",
      "0.34865293185419965\n",
      "0.3487634749524413\n",
      "0.3488740881699968\n",
      "0.3489847715736041\n",
      "0.3490955252300857\n",
      "0.3492063492063492\n",
      "0.3493172435693871\n",
      "0.349428208386277\n",
      "0.3495392437241817\n",
      "0.34965034965034963\n",
      "0.3497615262321145\n",
      "0.34987277353689566\n",
      "0.3499840916321985\n",
      "0.35009548058561424\n",
      "0.35020694046482015\n",
      "0.35031847133757954\n",
      "0.3504300732717425\n",
      "0.3505417463352454\n",
      "0.3506534905961109\n",
      "0.350765306122449\n",
      "0.3508771929824561\n",
      "0.35098915124441604\n",
      "0.3511011809766997\n",
      "0.351213282247765\n",
      "0.35132545512615776\n",
      "0.35143769968051114\n",
      "0.35155001597954616\n",
      "0.35166240409207156\n",
      "0.35177486408698433\n",
      "0.35188739603326935\n",
      "0.352\n",
      "0.35211267605633806\n",
      "0.35222542427153375\n",
      "0.35233824471492636\n",
      "0.35245113745594364\n",
      "0.35256410256410253\n",
      "0.35267714010900936\n",
      "0.3527902501603592\n",
      "0.3529034327879371\n",
      "0.35301668806161746\n",
      "0.35313001605136435\n",
      "0.3532434168272318\n",
      "0.35335689045936397\n",
      "0.35347043701799485\n",
      "0.35358405657344905\n",
      "0.3536977491961415\n",
      "0.3538115149565777\n",
      "0.35392535392535396\n",
      "0.3540392661731574\n",
      "0.35415325177076623\n",
      "0.35426731078904994\n",
      "0.3543814432989691\n",
      "0.35449564937157585\n",
      "0.35460992907801414\n",
      "0.3547242824895195\n",
      "0.3548387096774194\n",
      "0.3549532107131333\n",
      "0.35506778566817304\n",
      "0.3551824346141427\n",
      "0.355297157622739\n",
      "0.3554119547657512\n",
      "0.3555268261150614\n",
      "0.3556417717426447\n",
      "0.35575679172056923\n",
      "0.35587188612099646\n",
      "0.3559870550161812\n",
      "0.35610229847847197\n",
      "0.35621761658031087\n",
      "0.3563330093942339\n",
      "0.356448476992871\n",
      "0.35656401944894645\n",
      "0.35667963683527887\n",
      "0.35672514619883045\n",
      "0.35684107897302564\n",
      "0.35695708712613783\n",
      "0.3570731707317073\n",
      "0.3571893298633702\n",
      "0.35730556459485846\n",
      "0.357421875\n",
      "0.357538261152719\n",
      "0.35765472312703583\n",
      "0.35777126099706746\n",
      "0.3578878748370274\n",
      "0.35800456472122594\n",
      "0.35812133072407043\n",
      "0.35823817292006527\n",
      "0.358355091383812\n",
      "0.3584720861900098\n",
      "0.3585891574134552\n",
      "0.3587063051290428\n",
      "0.3588235294117647\n",
      "0.35894083033671137\n",
      "0.35905820797907134\n",
      "0.35917566241413146\n",
      "0.35929319371727747\n",
      "0.35941080196399344\n",
      "0.35952848722986247\n",
      "0.3596462495905667\n",
      "0.3597640891218873\n",
      "0.359882005899705\n",
      "0.36000000000000004\n",
      "0.36011807149885205\n",
      "0.3602362204724409\n",
      "0.3603544469970462\n",
      "0.3604727511490479\n",
      "0.36059113300492607\n",
      "0.3607095926412615\n",
      "0.3608281301347355\n",
      "0.36094674556213013\n",
      "0.36106543900032884\n",
      "0.36118421052631583\n",
      "0.36130306021717673\n",
      "0.36142198815009874\n",
      "0.3615409944023708\n",
      "0.3616600790513834\n",
      "0.36177924217462937\n",
      "0.3618984838497033\n",
      "0.36201780415430274\n",
      "0.3621372031662269\n",
      "0.36225668096337843\n",
      "0.36237623762376237\n",
      "0.362495873225487\n",
      "0.3626155878467635\n",
      "0.3628552544613351\n",
      "0.3629752066115703\n",
      "0.3630952380952381\n",
      "0.36321534899106844\n",
      "0.3633355393778954\n",
      "0.3634558093346574\n",
      "0.3635761589403973\n",
      "0.363696588274263\n",
      "0.363817097415507\n",
      "0.36393768644348695\n",
      "0.3640583554376658\n",
      "0.3641791044776119\n",
      "0.36429993364299934\n",
      "0.3644208430136077\n",
      "0.36454183266932266\n",
      "0.3646629026901361\n",
      "0.36478405315614615\n",
      "0.36490528414755735\n",
      "0.3650265957446809\n",
      "0.3651479880279348\n",
      "0.3652694610778443\n",
      "0.3653910149750416\n",
      "0.3655126498002663\n",
      "0.3656343656343656\n",
      "0.3657561625582944\n",
      "0.3658780406531156\n",
      "0.36600000000000005\n",
      "0.3661220406802268\n",
      "0.3662441627751834\n",
      "0.3663663663663663\n",
      "0.36648865153538046\n",
      "0.3666110183639399\n",
      "0.366677818668451\n",
      "0.3668005354752343\n",
      "0.36692333444928016\n",
      "0.36704621567314133\n",
      "0.3671691792294807\n",
      "0.36729222520107235\n",
      "0.3674153536708012\n",
      "0.36753856472166324\n",
      "0.3676618584367662\n",
      "0.36778523489932885\n",
      "0.36790869419268213\n",
      "0.3680322364002686\n",
      "0.36815586160564323\n",
      "0.3684033613445378\n",
      "0.3685272360457297\n",
      "0.36859838274932616\n",
      "0.36872261543646784\n",
      "0.3688469318948079\n",
      "0.3689713322091063\n",
      "0.36909581646423756\n",
      "0.3692203847451907\n",
      "0.3693450371370695\n",
      "0.3694697737250929\n",
      "0.3695945945945946\n",
      "0.369719499831024\n",
      "0.36984448951994586\n",
      "0.3699695637470409\n",
      "0.37009472259810555\n",
      "0.37021996615905245\n",
      "0.3703452945159106\n",
      "0.3704707077548256\n",
      "0.3705962059620596\n",
      "0.37072178922399185\n",
      "0.3708474576271186\n",
      "0.3709732112580536\n",
      "0.3710990502035278\n",
      "0.3712249745503902\n",
      "0.3713509843856076\n",
      "0.3714770797962649\n",
      "0.3716032608695652\n",
      "0.37172952769283046\n",
      "0.37185588035350103\n",
      "0.37198231893913636\n",
      "0.37210884353741497\n",
      "0.37223545423613474\n",
      "0.372362151123213\n",
      "0.37248893428668706\n",
      "0.3726158038147139\n",
      "0.37274275979557075\n",
      "0.37286980231765504\n",
      "0.37299693146948515\n",
      "0.37312414733969984\n",
      "0.373251450017059\n",
      "0.3733788395904437\n",
      "0.37350631614885627\n",
      "0.37363387978142076\n",
      "0.373761530577383\n",
      "0.3738892686261107\n",
      "0.374017094017094\n",
      "0.3741450068399453\n",
      "0.3742730071843996\n",
      "0.374529270797672\n",
      "0.37465753424657533\n",
      "0.3747858855772525\n",
      "0.37491432488005483\n",
      "0.3750428522454577\n",
      "0.37517146776406035\n",
      "0.3753001715265866\n",
      "0.37542896362388467\n",
      "0.3755578441469276\n",
      "0.3756868131868132\n",
      "0.3758158708347647\n",
      "0.3759450171821306\n",
      "0.376074252320385\n",
      "0.3762035763411279\n",
      "0.37633298933608533\n",
      "0.3764624913971094\n",
      "0.376592082616179\n",
      "0.3767217630853994\n",
      "0.3768515328970031\n",
      "0.3769813921433494\n",
      "0.3771113409169252\n",
      "0.37724137931034485\n",
      "0.37737150741635045\n",
      "0.3775017253278123\n",
      "0.37763203313772864\n",
      "0.37776243093922657\n",
      "0.3778929188255613\n",
      "0.3780234968901175\n",
      "0.37815416522640855\n",
      "0.37828492392807744\n",
      "0.3784157730888966\n",
      "0.3785467128027682\n",
      "0.37867774316372443\n",
      "0.37880886426592797\n",
      "0.37894007620367165\n",
      "0.37907137907137906\n",
      "0.37920277296360483\n",
      "0.37933425797503467\n",
      "0.37946583420048563\n",
      "0.3795975017349063\n",
      "0.3797292606733773\n",
      "0.37986111111111115\n",
      "0.3799930531434526\n",
      "0.3801250868658791\n",
      "0.38025721237400073\n",
      "0.3803894297635605\n",
      "0.3805217391304348\n",
      "0.38065414057063324\n",
      "0.38078663418029934\n",
      "0.38091922005571033\n",
      "0.3810518982932776\n",
      "0.381184668989547\n",
      "0.381317532241199\n",
      "0.38145048814504884\n",
      "0.38158353679804674\n",
      "0.38171667829727846\n",
      "0.3818499127399651\n",
      "0.38198324022346364\n",
      "0.38211666084526724\n",
      "0.3822501747030049\n",
      "0.38238378189444244\n",
      "0.3825174825174825\n",
      "0.3826512766701644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3827851644506648\n",
      "0.38291914595729787\n",
      "0.38305322128851543\n",
      "0.3831873905429072\n",
      "0.38332165381920114\n",
      "0.3834560112162636\n",
      "0.38359046283309955\n",
      "0.38372500876885296\n",
      "0.383859649122807\n",
      "0.383994383994384\n",
      "0.38412921348314605\n",
      "0.3842641376887952\n",
      "0.38439915671117353\n",
      "0.38453427065026363\n",
      "0.38466947960618847\n",
      "0.3847780126849894\n",
      "0.3849136411702503\n",
      "0.38504936530324396\n",
      "0.3851851851851852\n",
      "0.38532110091743116\n",
      "0.3854571126014825\n",
      "0.38559322033898313\n",
      "0.3857294242317202\n",
      "0.3858657243816254\n",
      "0.38600212089077407\n",
      "0.3861386138613861\n",
      "0.386275203395826\n",
      "0.38641188959660294\n",
      "0.38654867256637165\n",
      "0.386685552407932\n",
      "0.38682252922422955\n",
      "0.3869132290184922\n",
      "0.3870508715759516\n",
      "0.38718861209964417\n",
      "0.38732645069419724\n",
      "0.3874643874643875\n",
      "0.3876024225151407\n",
      "0.38774055595153245\n",
      "0.3878787878787879\n",
      "0.38801711840228253\n",
      "0.38815554762754195\n",
      "0.3882940756602427\n",
      "0.38843270260621204\n",
      "0.3885714285714286\n",
      "0.38871025366202216\n",
      "0.3888491779842745\n",
      "0.3889882016446193\n",
      "0.3891273247496424\n",
      "0.3892665474060823\n",
      "0.38940586972083036\n",
      "0.3895452918009309\n",
      "0.38968481375358166\n",
      "0.389824435686134\n",
      "0.38996415770609316\n",
      "0.3901039799211187\n",
      "0.3902439024390244\n",
      "0.39038392536777894\n",
      "0.3905240488155062\n",
      "0.3906642728904847\n",
      "0.39080459770114945\n",
      "0.39094502335609055\n",
      "0.39108554996405465\n",
      "0.3912261776339446\n",
      "0.39136690647482014\n",
      "0.3915077365958978\n",
      "0.3916486681065515\n",
      "0.3917897011163126\n",
      "0.3919308357348703\n",
      "0.39207207207207206\n",
      "0.39221341023792355\n",
      "0.39235485034258927\n",
      "0.39249639249639245\n",
      "0.39263803680981596\n",
      "0.3927797833935018\n",
      "0.3929216323582521\n",
      "0.39306358381502887\n",
      "0.39320563787495477\n",
      "0.3933477946493131\n",
      "0.3934900542495479\n",
      "0.39363241678726485\n",
      "0.39377488237423086\n",
      "0.3939174511223751\n",
      "0.39406012314378847\n",
      "0.39420289855072466\n",
      "0.3943457774555999\n",
      "0.3944887599709935\n",
      "0.39463184620964814\n",
      "0.39477503628447025\n",
      "0.39491833030852996\n",
      "0.39506172839506176\n",
      "0.39520523065746455\n",
      "0.3953488372093023\n",
      "0.3954843408594319\n",
      "0.39562841530054643\n",
      "0.3957725947521866\n",
      "0.39591687932920155\n",
      "0.39606126914660833\n",
      "0.3962057643195914\n",
      "0.3963503649635037\n",
      "0.3964950711938664\n",
      "0.3966398831263696\n",
      "0.3967848008768725\n",
      "0.39692982456140347\n",
      "0.39707495429616085\n",
      "0.3972201901975128\n",
      "0.39736553238199784\n",
      "0.3975109809663251\n",
      "0.39765653606737456\n",
      "0.3978021978021978\n",
      "0.3979479662880176\n",
      "0.3980938416422288\n",
      "0.39823982398239827\n",
      "0.39838591342626556\n",
      "0.39853211009174316\n",
      "0.3986784140969163\n",
      "0.39882482556004406\n",
      "0.39897134459955913\n",
      "0.39911797133406834\n",
      "0.3992647058823529\n",
      "0.39941154836336884\n",
      "0.3995584988962472\n",
      "0.3997055576002944\n",
      "0.39985272459499255\n",
      "0.4\n",
      "0.4001473839351511\n",
      "0.4002948765204571\n",
      "0.40044247787610615\n",
      "0.400590188122464\n",
      "0.4007380073800738\n",
      "0.4008859357696567\n",
      "0.40103397341211233\n",
      "0.40118212042851864\n",
      "0.401330376940133\n",
      "0.4014787430683919\n",
      "0.40162721893491127\n",
      "0.4017758046614872\n",
      "0.40192450037009625\n",
      "0.4020733061828952\n",
      "0.4022222222222222\n",
      "0.40237124861059653\n",
      "0.402520385470719\n",
      "0.40266963292547275\n",
      "0.4028189910979228\n",
      "0.40296846011131726\n",
      "0.40311804008908686\n",
      "0.40326773115484593\n",
      "0.40341753343239223\n",
      "0.40356744704570785\n",
      "0.4037174721189591\n",
      "0.4038676087764968\n",
      "0.40401785714285715\n",
      "0.4041682173427615\n",
      "0.40431868950111693\n",
      "0.4044692737430168\n",
      "0.4046199701937407\n",
      "0.40477077897875513\n",
      "0.40492170022371365\n",
      "0.4050727340544574\n",
      "0.4052238805970149\n",
      "0.40537513997760355\n",
      "0.40552651232262876\n",
      "0.4056779977586851\n",
      "0.40582959641255606\n",
      "0.40584050917259445\n",
      "0.4059925093632959\n",
      "0.40614462345447727\n",
      "0.40644919385076866\n",
      "0.4066016504126032\n",
      "0.4067542213883677\n",
      "0.40690690690690695\n",
      "0.40705970709725875\n",
      "0.40721262208865516\n",
      "0.40736565201052233\n",
      "0.4075187969924812\n",
      "0.40767205716434757\n",
      "0.40782543265613247\n",
      "0.4079789235980429\n",
      "0.4081325301204819\n",
      "0.4082862523540489\n",
      "0.4084400904295404\n",
      "0.4085940444779495\n",
      "0.40874811463046756\n",
      "0.4089023010184836\n",
      "0.4090566037735849\n",
      "0.4092110230275575\n",
      "0.4093655589123868\n",
      "0.4095202115602569\n",
      "0.40967498110355255\n",
      "0.40982986767485824\n",
      "0.40984848484848485\n",
      "0.4100037893141341\n",
      "0.41015921152388174\n",
      "0.41031475161167996\n",
      "0.4104704097116844\n",
      "0.41062618595825423\n",
      "0.4107820804859529\n",
      "0.410938093429548\n",
      "0.41109422492401215\n",
      "0.41125047510452306\n",
      "0.4114068441064639\n",
      "0.4115633320654241\n",
      "0.41171993911719934\n",
      "0.41187666539779216\n",
      "0.41203351104341207\n",
      "0.4121904761904763\n",
      "0.4123475609756098\n",
      "0.4125047655356462\n",
      "0.41266209000762777\n",
      "0.41281953452880576\n",
      "0.4129770992366412\n",
      "0.41313478426880484\n",
      "0.413292589763178\n",
      "0.4134505158578524\n",
      "0.4136085626911315\n",
      "0.41376673040152956\n",
      "0.4139250191277736\n",
      "0.41408342900880213\n",
      "0.4142419601837672\n",
      "0.4144006127920337\n",
      "0.41455938697318007\n",
      "0.4147182828669988\n",
      "0.41487730061349704\n",
      "0.4150364403528961\n",
      "0.4151957022256331\n",
      "0.4153550863723609\n",
      "0.4155145929339479\n",
      "0.41567422205147897\n",
      "0.4158339738662567\n",
      "0.4159938485198001\n",
      "0.41615384615384615\n",
      "0.41631396691035016\n",
      "0.4164742109314858\n",
      "0.41679506933744226\n",
      "0.4169556840077072\n",
      "0.4171164225134926\n",
      "0.4172772849980717\n",
      "0.4174382716049383\n",
      "0.4175993824778078\n",
      "0.41776061776061774\n",
      "0.41792197759752797\n",
      "0.4180834621329212\n",
      "0.41824507151140317\n",
      "0.41840680587780354\n",
      "0.418568665377176\n",
      "0.4187306501547988\n",
      "0.41889276035617506\n",
      "0.41905499612703334\n",
      "0.4192173576133282\n",
      "0.41937984496124037\n",
      "0.4195424583171771\n",
      "0.4197051978277735\n",
      "0.4197434900893898\n",
      "0.41990668740279935\n",
      "0.42007001166861146\n",
      "0.4202334630350194\n",
      "0.42039704165044767\n",
      "0.42056074766355145\n",
      "0.42072458122321776\n",
      "0.4208885424785659\n",
      "0.4210526315789474\n",
      "0.42121684867394704\n",
      "0.4213811939133827\n",
      "0.42154566744730676\n",
      "0.4217102694260054\n",
      "0.421875\n",
      "0.42203985932004684\n",
      "0.42220484753713844\n",
      "0.42236996480250294\n",
      "0.42253521126760557\n",
      "0.42270058708414876\n",
      "0.422866092404072\n",
      "0.4230317273795535\n",
      "0.42319749216300945\n",
      "0.42336338690709524\n",
      "0.42340926944226237\n",
      "0.4235756385068762\n",
      "0.42374213836477986\n",
      "0.42390876917027137\n",
      "0.42407553107789137\n",
      "0.42424242424242425\n",
      "0.4244094488188976\n",
      "0.4245766049625837\n",
      "0.4247438928289992\n",
      "0.42491131257390613\n",
      "0.4250788643533123\n",
      "0.42524654832347136\n",
      "0.425414364640884\n",
      "0.4255823134622977\n",
      "0.4257503949447078\n",
      "0.42591860924535757\n",
      "0.42608695652173906\n",
      "0.42625543693159346\n",
      "0.4264240506329114\n",
      "0.4265927977839335\n",
      "0.4267616785431513\n",
      "0.4269306930693069\n",
      "0.4270998415213946\n",
      "0.42726912405866035\n",
      "0.4274385408406028\n",
      "0.4276080920269734\n",
      "0.4277777777777778\n",
      "0.42794759825327516\n",
      "0.4281175536139794\n",
      "0.42828764402065955\n",
      "0.4284578696343402\n",
      "0.4286282306163021\n",
      "0.4287987271280827\n",
      "0.4289693593314764\n",
      "0.429140127388535\n",
      "0.429311031461569\n",
      "0.42948207171314745\n",
      "0.4296532483060981\n",
      "0.4298245614035087\n",
      "0.42999601116872754\n",
      "0.4301675977653632\n",
      "0.4303393213572854\n",
      "0.43051118210862616\n",
      "0.43068318018377955\n",
      "0.43085531574740205\n",
      "0.43102758896441423\n",
      "0.43119999999999997\n",
      "0.43137254901960786\n",
      "0.43154523618895113\n",
      "0.4317180616740088\n",
      "0.4318910256410256\n",
      "0.4320641282565131\n",
      "0.43223736968724935\n",
      "0.4324107501002808\n",
      "0.43258426966292135\n",
      "0.4327579285427539\n",
      "0.43293172690763054\n",
      "0.433105664925673\n",
      "0.4332797427652733\n",
      "0.4334539605950945\n",
      "0.4336283185840708\n",
      "0.43380281690140843\n",
      "0.4339774557165861\n",
      "0.4341522351993557\n",
      "0.43432715551974216\n",
      "0.43450221684804513\n",
      "0.4346774193548387\n",
      "0.4348527632109722\n",
      "0.4350282485875706\n",
      "0.4352038756560356\n",
      "0.43537964458804523\n",
      "0.43555555555555553\n",
      "0.43573160873080025\n",
      "0.43590780428629194\n",
      "0.43608414239482196\n",
      "0.43626062322946174\n",
      "0.43643724696356273\n",
      "0.4366140137707574\n",
      "0.4367909238249595\n",
      "0.43696797730036485\n",
      "0.4371451743714518\n",
      "0.43732251521298177\n",
      "0.4375\n",
      "0.437677628907836\n",
      "0.43785540211210405\n",
      "0.4380333197887038\n",
      "0.43821138211382116\n",
      "0.4383895892639284\n",
      "0.4385679414157851\n",
      "0.4387464387464387\n",
      "0.43882544861337686\n",
      "0.4390044879640963\n",
      "0.43918367346938775\n",
      "0.4393630053082891\n",
      "0.4395424836601307\n",
      "0.4397221087045361\n",
      "0.43990188062142277\n",
      "0.4400817995910021\n",
      "0.4404420794105608\n",
      "0.44062244062244066\n",
      "0.4408029496108153\n",
      "0.44098360655737706\n",
      "0.4411644116441165\n",
      "0.44134536505332245\n",
      "0.44152646696758313\n",
      "0.4417077175697865\n",
      "0.44188911704312117\n",
      "0.4420706655710764\n",
      "0.4422523633374435\n",
      "0.4424342105263157\n",
      "0.44261620732208967\n",
      "0.44279835390946504\n",
      "0.44298065047344587\n",
      "0.443163097199341\n",
      "0.4433456942727647\n",
      "0.44352844187963725\n",
      "0.4437113402061855\n",
      "0.4438943894389439\n",
      "0.4440775897647544\n",
      "0.44426094137076794\n",
      "0.4444444444444444\n",
      "0.4446280991735538\n",
      "0.4448119057461762\n",
      "0.4449958643507031\n",
      "0.4451799751758378\n",
      "0.445364238410596\n",
      "0.44554865424430645\n",
      "0.4457332228666115\n",
      "0.4459179444674679\n",
      "0.4461028192371475\n",
      "0.4462878473662381\n",
      "0.4464730290456432\n",
      "0.44665836446658375\n",
      "0.44684385382059805\n",
      "0.447029497299543\n",
      "0.4472152950955943\n",
      "0.4474012474012474\n",
      "0.4475873544093178\n",
      "0.44777361631294216\n",
      "0.4479600333055787\n",
      "0.44814660558100794\n",
      "0.44833333333333336\n",
      "0.44852021675698206\n",
      "0.44870725604670564\n",
      "0.4488944513975802\n",
      "0.44908180300500833\n",
      "0.4492693110647181\n",
      "0.4494569757727653\n",
      "0.44964479732553275\n",
      "0.4498327759197325\n",
      "0.45002091175240483\n",
      "0.45020920502092054\n",
      "0.45039765592298037\n",
      "0.4505862646566164\n",
      "0.4507750314201927\n",
      "0.4509639564124056\n",
      "0.45115303983228505\n",
      "0.4513422818791947\n",
      "0.45153168275283256\n",
      "0.45172124265323255\n",
      "0.4519109617807644\n",
      "0.45210084033613446\n",
      "0.45229087852038674\n",
      "0.4524810765349033\n",
      "0.4526714345814051\n",
      "0.4528619528619529\n",
      "0.4530526315789474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45324347093513057\n",
      "0.4534344711335862\n",
      "0.45362563237774034\n",
      "0.4538169548713623\n",
      "0.4540084388185654\n",
      "0.45420008442380755\n",
      "0.4543918918918919\n",
      "0.45458386142796786\n",
      "0.4547759932375317\n",
      "0.454968287526427\n",
      "0.45516074450084604\n",
      "0.45535336436732965\n",
      "0.4555461473327688\n",
      "0.4557390936044049\n",
      "0.4558573853989814\n",
      "0.45605095541401275\n",
      "0.4562446898895497\n",
      "0.4564385890352741\n",
      "0.45663265306122447\n",
      "0.45682688217779666\n",
      "0.4570212765957447\n",
      "0.45721583652618136\n",
      "0.45741056218057924\n",
      "0.45760545377077116\n",
      "0.45780051150895146\n",
      "0.4579957356076759\n",
      "0.4581911262798635\n",
      "0.45838668373879643\n",
      "0.4585824081981213\n",
      "0.45877829987184965\n",
      "0.45897435897435895\n",
      "0.4591705857203933\n",
      "0.4593669803250642\n",
      "0.45956354300385116\n",
      "0.4597602739726028\n",
      "0.45995717344753745\n",
      "0.4601542416452442\n",
      "0.46035147878268323\n",
      "0.460548885077187\n",
      "0.4607464607464608\n",
      "0.4609442060085837\n",
      "0.4611421210820094\n",
      "0.46134020618556704\n",
      "0.46147223417993977\n",
      "0.46167097329888024\n",
      "0.4620689655172414\n",
      "0.4622682190599397\n",
      "0.4624676445211389\n",
      "0.46266724212343546\n",
      "0.46286701208980996\n",
      "0.4630669546436285\n",
      "0.46326707000864303\n",
      "0.4634673584089926\n",
      "0.46366782006920415\n",
      "0.4638684552141929\n",
      "0.4640692640692641\n",
      "0.46427024686011265\n",
      "0.4644714038128249\n",
      "0.4646727351538795\n",
      "0.46487424111014747\n",
      "0.4650759219088937\n",
      "0.46527777777777773\n",
      "0.4654798089448546\n",
      "0.4656820156385752\n",
      "0.46588439808778787\n",
      "0.4660869565217391\n",
      "0.46617197730248805\n",
      "0.46637554585152835\n",
      "0.46657929226736566\n",
      "0.46678321678321677\n",
      "0.46698731963270657\n",
      "0.46719160104986873\n",
      "0.46739606126914657\n",
      "0.46760070052539404\n",
      "0.4678055190538765\n",
      "0.4680105170902717\n",
      "0.46815985946420724\n",
      "0.46836555360281196\n",
      "0.4685714285714285\n",
      "0.4687774846086192\n",
      "0.46898372195336563\n",
      "0.46919014084507044\n",
      "0.46939674152355787\n",
      "0.4696035242290749\n",
      "0.46981048920229174\n",
      "0.47001763668430335\n",
      "0.47022496691662996\n",
      "0.470432480141218\n",
      "0.4706401766004415\n",
      "0.47079646017699117\n",
      "0.47100486941124386\n",
      "0.47121346324180685\n",
      "0.47142224191404514\n",
      "0.4716312056737589\n",
      "0.471840354767184\n",
      "0.47204968944099374\n",
      "0.4722592099422992\n",
      "0.47246891651865014\n",
      "0.4726788094180364\n",
      "0.4728888888888889\n",
      "0.47309915518008\n",
      "0.47330960854092524\n",
      "0.4735202492211838\n",
      "0.47373107747105964\n",
      "0.47394209354120265\n",
      "0.4741532976827095\n",
      "0.4743646901471244\n",
      "0.47457627118644063\n",
      "0.4747880410531012\n",
      "0.4749999999999999\n",
      "0.4752121482804823\n",
      "0.47542448614834676\n",
      "0.47563701385784535\n",
      "0.47580645161290325\n",
      "0.476019722097714\n",
      "0.47623318385650226\n",
      "0.4764468371467026\n",
      "0.4766606822262119\n",
      "0.4768747193533903\n",
      "0.47708894878706204\n",
      "0.47730337078651686\n",
      "0.4775179856115108\n",
      "0.47773279352226716\n",
      "0.4779477947794779\n",
      "0.47816298964430437\n",
      "0.4783783783783784\n",
      "0.47859396124380355\n",
      "0.478809738503156\n",
      "0.4790257104194858\n",
      "0.4792418772563176\n",
      "0.47945823927765235\n",
      "0.4796747967479674\n",
      "0.4798915499322187\n",
      "0.4801084990958409\n",
      "0.480325644504749\n",
      "0.48054298642533944\n",
      "0.4807605251244908\n",
      "0.4809782608695652\n",
      "0.4811961939284096\n",
      "0.4814143245693563\n",
      "0.48163265306122455\n",
      "0.4818511796733212\n",
      "0.4820699046754426\n",
      "0.4822888283378746\n",
      "0.48250795093139476\n",
      "0.4827272727272727\n",
      "0.4829467939972715\n",
      "0.4831665150136488\n",
      "0.4833864360491579\n",
      "0.4836065573770492\n",
      "0.4838268792710706\n",
      "0.4840474020054694\n",
      "0.4842681258549932\n",
      "0.48448905109489054\n",
      "0.4847101780009128\n",
      "0.484931506849315\n",
      "0.48515303791685704\n",
      "0.48537477148080443\n",
      "0.48559670781893005\n",
      "0.4858188472095152\n",
      "0.4860411899313501\n",
      "0.4862637362637363\n",
      "0.4864864864864865\n",
      "0.4867094408799267\n",
      "0.4869085898024805\n",
      "0.48713235294117646\n",
      "0.4873563218390805\n",
      "0.48758049678012877\n",
      "0.4878048780487805\n",
      "0.4880294659300184\n",
      "0.48825426070935046\n",
      "0.488479262672811\n",
      "0.4887044721069617\n",
      "0.48892988929889303\n",
      "0.48915551453622513\n",
      "0.48938134810710987\n",
      "0.4896073903002309\n",
      "0.4898336414048059\n",
      "0.4900601017105871\n",
      "0.4902867715078631\n",
      "0.4905136510874594\n",
      "0.4907407407407407\n",
      "0.49095127610208816\n",
      "0.4911792014856082\n",
      "0.49140733859730606\n",
      "0.491635687732342\n",
      "0.4918642491864249\n",
      "0.49209302325581394\n",
      "0.4923220102373197\n",
      "0.49255121042830535\n",
      "0.49278062412668844\n",
      "0.4930102516309413\n",
      "0.4932400932400933\n",
      "0.49347014925373134\n",
      "0.49370041997200187\n",
      "0.4939309056956116\n",
      "0.4941616067258291\n",
      "0.49439252336448597\n",
      "0.49462365591397844\n",
      "0.49485500467726845\n",
      "0.49508656995788486\n",
      "0.49531835205992514\n",
      "0.4955503512880562\n",
      "0.4957825679475164\n",
      "0.4960150023441163\n",
      "0.4962476547842401\n",
      "0.49648052557484745\n",
      "0.49671361502347416\n",
      "0.4969469234382338\n",
      "0.49718045112781956\n",
      "0.4974141984015045\n",
      "0.4976481655691439\n",
      "0.4978823529411765\n",
      "0.4981167608286252\n",
      "0.4983513895430994\n",
      "0.4985862393967955\n",
      "0.49881908360888044\n",
      "0.49905482041587906\n",
      "0.4992894362861203\n",
      "0.49952606635071095\n",
      "0.4997629208155524\n",
      "0.5\n",
      "0.5002373042240151\n",
      "0.500474833808167\n",
      "0.5007125890736341\n",
      "0.5009505703422054\n",
      "0.5011887779362815\n",
      "0.5014272121788772\n",
      "0.5016658733936221\n",
      "0.5019047619047619\n",
      "0.5021438780371605\n",
      "0.5023832221163013\n",
      "0.5023877745940784\n",
      "0.502627806975633\n",
      "0.502868068833652\n",
      "0.5031085604973696\n",
      "0.5033492822966508\n",
      "0.5035902345619914\n",
      "0.503831417624521\n",
      "0.5040728318160038\n",
      "0.5043144774688398\n",
      "0.5045563549160672\n",
      "0.5047984644913628\n",
      "0.5050408065290446\n",
      "0.505283381364073\n",
      "0.5055261893320521\n",
      "0.5057692307692307\n",
      "0.506012506012506\n",
      "0.5062560153994226\n",
      "0.5064997592681753\n",
      "0.5067437379576107\n",
      "0.506987951807229\n",
      "0.5072324011571842\n",
      "0.5074770863482875\n",
      "0.5077220077220077\n",
      "0.5077369439071566\n",
      "0.5079825834542815\n",
      "0.5079980610761027\n",
      "0.5082444228903977\n",
      "0.5084910237748665\n",
      "0.5085407515861395\n",
      "0.5087890625\n",
      "0.509037616023449\n",
      "0.509286412512219\n",
      "0.5095354523227383\n",
      "0.509784735812133\n",
      "0.5100342633382281\n",
      "0.5100539480137322\n",
      "0.5103042198233563\n",
      "0.510554737358861\n",
      "0.5108055009823183\n",
      "0.5110565110565111\n",
      "0.511307767944936\n",
      "0.5115592720118052\n",
      "0.5116049382716049\n",
      "0.5118577075098814\n",
      "0.5121107266435987\n",
      "0.5123639960435212\n",
      "0.512617516081148\n",
      "0.5128712871287128\n",
      "0.5131253095591877\n",
      "0.5132037867463877\n",
      "0.5134596211365903\n",
      "0.513715710723192\n",
      "0.5139720558882236\n",
      "0.5142286570144783\n",
      "0.5144855144855145\n",
      "0.5147426286856572\n",
      "0.515\n",
      "0.5152576288144073\n",
      "0.5155155155155156\n",
      "0.515773660490736\n",
      "0.5160320641282565\n",
      "0.5162907268170426\n",
      "0.5165496489468405\n",
      "0.5168088309081786\n",
      "0.5170682730923695\n",
      "0.5173279758915119\n",
      "0.5175879396984925\n",
      "0.5178481649069885\n",
      "0.5181086519114688\n",
      "0.5183694011071968\n",
      "0.5186304128902315\n",
      "0.5188916876574308\n",
      "0.5191532258064516\n",
      "0.5194150277357539\n",
      "0.5196770938446015\n",
      "0.5199394245330642\n",
      "0.5202020202020202\n",
      "0.520242914979757\n",
      "0.520506329113924\n",
      "0.5207700101317123\n",
      "0.5210339584389255\n",
      "0.5212981744421907\n",
      "0.5215626585489599\n",
      "0.5216065073716319\n",
      "0.5218718209562564\n",
      "0.5221374045801527\n",
      "0.5224032586558045\n",
      "0.5226693835965359\n",
      "0.5229357798165137\n",
      "0.5232024477307496\n",
      "0.523469387755102\n",
      "0.5237366003062787\n",
      "0.5240040858018385\n",
      "0.5242718446601942\n",
      "0.5245398773006135\n",
      "0.5248081841432225\n",
      "0.5250767656090071\n",
      "0.5253456221198156\n",
      "0.5256147540983607\n",
      "0.5258841619682214\n",
      "0.5261538461538462\n",
      "0.5264238070805541\n",
      "0.5266940451745379\n",
      "0.5269645608628659\n",
      "0.527235354573484\n",
      "0.5275064267352184\n",
      "0.5277777777777778\n",
      "0.528049408131755\n",
      "0.5283213182286303\n",
      "0.5285935085007728\n",
      "0.5288659793814433\n",
      "0.5291387313047964\n",
      "0.5294117647058822\n",
      "0.5296850800206505\n",
      "0.529746508018624\n",
      "0.529932326913066\n",
      "0.5302083333333334\n",
      "0.5304846274101095\n",
      "0.5307612095933263\n",
      "0.531038080333855\n",
      "0.5313152400835073\n",
      "0.5315926892950392\n",
      "0.5318704284221526\n",
      "0.5321484579194982\n",
      "0.5324267782426778\n",
      "0.532705389848247\n",
      "0.5329842931937173\n",
      "0.5332634887375589\n",
      "0.5335429769392034\n",
      "0.5338227582590457\n",
      "0.534102833158447\n",
      "0.5343832020997374\n",
      "0.5346638655462185\n",
      "0.5349448239621649\n",
      "0.5350184307530279\n",
      "0.5353003161222339\n",
      "0.5355824986821297\n",
      "0.5358649789029536\n",
      "0.5361477572559367\n",
      "0.5364308342133053\n",
      "0.536714210248283\n",
      "0.5369978858350951\n",
      "0.5372818614489688\n",
      "0.5375661375661376\n",
      "0.5378507146638433\n",
      "0.538135593220339\n",
      "0.5384207737148913\n",
      "0.5387062566277837\n",
      "0.5389920424403183\n",
      "0.5392781316348195\n",
      "0.5395645246946362\n",
      "0.5398512221041444\n",
      "0.5401382243487507\n",
      "0.5404255319148936\n",
      "0.5407131452900479\n",
      "0.5410010649627264\n",
      "0.5410885805763074\n",
      "0.5413043478260868\n",
      "0.5415986949429037\n",
      "0.5418933623503808\n",
      "0.5421883505715841\n",
      "0.5424836601307189\n",
      "0.5426695842450766\n",
      "0.5429666119321291\n",
      "0.5432639649507119\n",
      "0.5435616438356166\n",
      "0.5436573311367381\n",
      "0.5439560439560439\n",
      "0.5442550852116548\n",
      "0.5445544554455445\n",
      "0.5448541552008805\n",
      "0.5449531163816879\n",
      "0.5452538631346578\n",
      "0.545554942020983\n",
      "0.5458563535911602\n",
      "0.5461580983969043\n",
      "0.5464601769911505\n",
      "0.5467625899280576\n",
      "0.5470653377630121\n",
      "0.5473684210526316\n",
      "0.5476718403547672\n",
      "0.5477777777777777\n",
      "0.548082267926626\n",
      "0.5483870967741935\n",
      "0.5486922648859209\n",
      "0.5489977728285078\n",
      "0.5493036211699164\n",
      "0.5496098104793756\n",
      "0.5499163413273843\n",
      "0.5502232142857142\n",
      "0.5505304299274149\n",
      "0.5505617977528091\n",
      "0.5508712759977517\n",
      "0.5511010728402033\n",
      "0.5514072372199885\n",
      "0.5517241379310346\n",
      "0.5520414031052329\n",
      "0.5523590333716917\n",
      "0.5526770293609672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5528455284552846\n",
      "0.5529685681024447\n",
      "0.5532906231799651\n",
      "0.5536130536130537\n",
      "0.553935860058309\n",
      "0.5542590431738623\n",
      "0.5545826036193812\n",
      "0.5549065420560748\n",
      "0.5552308591466978\n",
      "0.5555555555555556\n",
      "0.5558806319485079\n",
      "0.5562060889929742\n",
      "0.556338028169014\n",
      "0.5566647093364652\n",
      "0.5569917743830788\n",
      "0.5573192239858906\n",
      "0.5576470588235294\n",
      "0.5579752795762213\n",
      "0.5583038869257951\n",
      "0.5586328815556865\n",
      "0.5589622641509434\n",
      "0.5592920353982301\n",
      "0.5596221959858323\n",
      "0.5599527466036621\n",
      "0.5602836879432624\n",
      "0.560615020697812\n",
      "0.5609467455621302\n",
      "0.5612788632326821\n",
      "0.561611374407583\n",
      "0.5618649133293485\n",
      "0.562200956937799\n",
      "0.5625374027528426\n",
      "0.562874251497006\n",
      "0.5632115038945477\n",
      "0.5635491606714629\n",
      "0.5638872225554888\n",
      "0.5642256902761105\n",
      "0.5645645645645646\n",
      "0.5649038461538463\n",
      "0.5652435357787132\n",
      "0.565583634175692\n",
      "0.5659241420830825\n",
      "0.5660832830416416\n",
      "0.5664251207729469\n",
      "0.5667673716012085\n",
      "0.5671100362756953\n",
      "0.5674531155474895\n",
      "0.5677966101694916\n",
      "0.5681405208964264\n",
      "0.5683060109289617\n",
      "0.56865127582017\n",
      "0.5689969604863221\n",
      "0.5693430656934306\n",
      "0.5695039804041642\n",
      "0.5698529411764706\n",
      "0.57002457002457\n",
      "0.5703749231714812\n",
      "0.5707257072570726\n",
      "0.571076923076923\n",
      "0.5714285714285714\n",
      "0.5717806531115219\n",
      "0.5717837165941578\n",
      "0.572139303482587\n",
      "0.5724953329184816\n",
      "0.5728518057285181\n",
      "0.573208722741433\n",
      "0.57356608478803\n",
      "0.5739238927011853\n",
      "0.5742821473158551\n",
      "0.5746408494690818\n",
      "0.575\n",
      "0.575187969924812\n",
      "0.5755485893416927\n",
      "0.5759096612296111\n",
      "0.5762711864406779\n",
      "0.5766331658291457\n",
      "0.5769956002514142\n",
      "0.5771896660365469\n",
      "0.5775535939470365\n",
      "0.5779179810725553\n",
      "0.5779467680608364\n",
      "0.5783132530120483\n",
      "0.5786802030456852\n",
      "0.579047619047619\n",
      "0.5792488860598344\n",
      "0.5796178343949046\n",
      "0.5798212005108557\n",
      "0.5801916932907348\n",
      "0.5805626598465472\n",
      "0.580934101087652\n",
      "0.5813060179257362\n",
      "0.5816784112748239\n",
      "0.5820512820512821\n",
      "0.5820991629104958\n",
      "0.5824742268041238\n",
      "0.5828497743391361\n",
      "0.5832258064516129\n",
      "0.5836023240800516\n",
      "0.5839793281653747\n",
      "0.5843568196509373\n",
      "0.5847347994825356\n",
      "0.5851132686084143\n",
      "0.5854922279792747\n",
      "0.5858716785482826\n",
      "0.5860948667966213\n",
      "0.5864759427828349\n",
      "0.5867014341590613\n",
      "0.5870841487279843\n",
      "0.5874673629242819\n",
      "0.587696335078534\n",
      "0.5881578947368422\n",
      "0.5885450954575379\n",
      "0.5889328063241107\n",
      "0.5890138980807412\n",
      "0.5894039735099338\n",
      "0.5897945659377071\n",
      "0.5901856763925729\n",
      "0.5905773059057731\n",
      "0.5909694555112881\n",
      "0.5912117177097204\n",
      "0.5916055962691539\n",
      "0.592\n",
      "0.5923949299533022\n",
      "0.5927903871829104\n",
      "0.593186372745491\n",
      "0.5935828877005347\n",
      "0.5939799331103679\n"
     ]
    }
   ],
   "source": [
    "final_result = sum_results\n",
    "sorted_sum = np.sort(final_result)\n",
    "final_f1 = 0\n",
    "for i in range(10, np.shape(result_list)[1]):\n",
    "    threshold = sorted_sum[i]\n",
    "    predictions = np.array([int(i) for i in final_result > threshold])\n",
    "    cur_f1 = metrics.f1_score(y, predictions)\n",
    "    if cur_f1 > final_f1:\n",
    "        final_f1 = cur_f1\n",
    "        print(final_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.05897855 0.13639677 0.1047345  0.1047345  0.03393291 0.03461023\n",
      "  0.28589728 0.19985044 0.068563   0.22703522]\n",
      " [0.04920814 0.34734176 0.08591236 0.08591236 0.03983111 0.0190931\n",
      "  0.84340958 0.16253437 0.11602947 0.68815897]\n",
      " [0.06474265 0.80856865 0.20794167 0.20794167 0.15123069 0.04110886\n",
      "  0.88969001 0.38799463 0.27582538 0.96504887]\n",
      " [0.13696665 0.99034665 0.67490854 0.67490854 0.64481484 0.27631153\n",
      "  0.94400612 0.93191529 0.33388876 0.99999122]\n",
      " [0.55682176 0.99999986 0.97344206 0.97344206 0.97820084 0.98207504\n",
      "  0.98279235 0.99545659 0.5485493  0.99999998]\n",
      " [0.43674965 0.99999999 0.99999037 0.99999037 0.99232843 0.97568343\n",
      "  0.97067126 0.99512725 0.53959333 0.99999999]]\n",
      "[0.21544916 0.70787664 0.50362471 0.50362471 0.46947749 0.38493921\n",
      " 0.81263911 0.60708737 0.31114863 0.80665029]\n",
      "[0 1 1 1 1 0 1 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(result_list[:, disagree_indexes[0:10]])\n",
    "print(np.array(kf_final_results)[disagree_indexes[0:10]])\n",
    "print(y[disagree_indexes[0:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_update_list(R, value_list):\n",
    "    P = 2\n",
    "    prediction = 0\n",
    "    for new_value in value_list:\n",
    "        K = P / (P + R)\n",
    "        prediction = prediction + K * (new_value - prediction)\n",
    "        P = (1 - K) * P\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5238805970149255, 0.5277161862527716, 0.5358711566617862, 0.5447924253459577, 0.5496402877697842]\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "result_list = result_list[0:6,:]\n",
    "for j in range(1, 6):\n",
    "    kf_final_results = []\n",
    "    for i in range(np.shape(result_list)[1]):\n",
    "        kf_final_results.append(filter_update_list(0.1, list(result_list[:j,i])))\n",
    "    kf_final_results = np.array(kf_final_results)\n",
    "    predictions = np.array([int(i) for i in kf_final_results>0.5])\n",
    "    result.append(metrics.f1_score(y, predictions))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 962,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of definite inliers:  4611 4611\n",
      "Number of definite outliers:  718 718\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5814648729446935"
      ]
     },
     "execution_count": 962,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "definite_inliers = np.where(np.max(prediction_result_list, axis = 0) < 0.5)[0]\n",
    "definite_outliers = np.where(np.min(prediction_result_list, axis = 0) > 0.5)[0]\n",
    "\n",
    "kf_final_results = []\n",
    "for i in range(np.shape(prediction_result_list)[1]):\n",
    "    kf_final_results.append(filter_update_list(0.1, list(prediction_result_list[:,i])))\n",
    "kf_final_results = np.array(kf_final_results)\n",
    "\n",
    "print('Number of definite inliers: ', len(definite_inliers), sum(kf_final_results[definite_inliers] < 0.5))\n",
    "print('Number of definite outliers: ', len(definite_outliers), sum(kf_final_results[definite_outliers] > 0.5))\n",
    "\n",
    "# compute upper bound of KF (if all unsure points are correctly classified)\n",
    "ideal_predictions = np.zeros((5473))\n",
    "other_index = np.setdiff1d(range(5473), np.union1d(definite_inliers, definite_outliers))\n",
    "ideal_predictions[other_index]  = y[other_index]\n",
    "ideal_predictions[definite_outliers] = 1\n",
    "metrics.f1_score(y, ideal_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 933,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5471167369901547"
      ]
     },
     "execution_count": 933,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y, np.array([int(i) for i in np.max(prediction_result_list, axis=0) > 0.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 928,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = np.max(prediction_result_list[0:6,:], axis= 0)-np.min(prediction_result_list[0:6,:], axis= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 929,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.73635127, 0.22041315, 0.26245019, 0.08860593, 0.15643282,\n",
       "        0.08860593, 0.11559841, 0.08860593, 0.18865282, 0.29647056,\n",
       "        0.24121649, 0.31324514, 0.26283802, 0.26245019, 0.26245019,\n",
       "        0.3638856 , 0.29647056, 0.11559841, 0.29647056, 0.15643282,\n",
       "        0.26245019, 0.24121649, 0.26283802, 0.26283802, 0.13807442,\n",
       "        0.13807442, 0.26245019, 0.26245019, 0.08860593, 0.42588565,\n",
       "        0.26245019, 0.26283802, 0.26283802, 0.11135822, 0.26245019,\n",
       "        0.29647056, 0.24121649, 0.26283802, 0.29647056, 0.29647056,\n",
       "        0.26245019, 0.26283802],\n",
       "       [0.46094281, 0.66071351, 0.46210253, 0.53753793, 0.46436948,\n",
       "        0.53753793, 0.63110333, 0.53753793, 0.74422459, 0.49020966,\n",
       "        0.50603041, 0.64177248, 0.47560128, 0.46210253, 0.46210253,\n",
       "        0.8309298 , 0.49020966, 0.63110333, 0.49020966, 0.46436948,\n",
       "        0.46210253, 0.50603041, 0.47560128, 0.47560128, 0.5320883 ,\n",
       "        0.5320883 , 0.46210253, 0.46210253, 0.53753793, 0.83389458,\n",
       "        0.46210253, 0.47560128, 0.47560128, 0.55131156, 0.46210253,\n",
       "        0.49020966, 0.50603041, 0.47560128, 0.49020966, 0.49020966,\n",
       "        0.46210253, 0.47560128],\n",
       "       [0.43049238, 0.78114318, 0.76103592, 0.80430738, 0.75892957,\n",
       "        0.80430738, 0.88666716, 0.80430738, 0.93682416, 0.78292121,\n",
       "        0.77209847, 0.74133477, 0.77215463, 0.76103592, 0.76103592,\n",
       "        0.90409205, 0.78292121, 0.88666716, 0.78292121, 0.75892957,\n",
       "        0.76103592, 0.77209847, 0.77215463, 0.77215463, 0.78445518,\n",
       "        0.78445518, 0.76103592, 0.76103592, 0.80430738, 0.96364281,\n",
       "        0.76103592, 0.77215463, 0.77215463, 0.6407986 , 0.76103592,\n",
       "        0.78292121, 0.77209847, 0.77215463, 0.78292121, 0.78292121,\n",
       "        0.76103592, 0.77215463],\n",
       "       [0.18206611, 0.77012159, 0.81610193, 0.84369892, 0.81015432,\n",
       "        0.84369892, 0.91024749, 0.84369892, 0.94754863, 0.82682377,\n",
       "        0.81610578, 0.84942361, 0.81842869, 0.81610193, 0.81610193,\n",
       "        0.89503635, 0.82682377, 0.91024749, 0.82682377, 0.81015432,\n",
       "        0.81610193, 0.81610578, 0.81842869, 0.81842869, 0.82279077,\n",
       "        0.82279077, 0.81610193, 0.81610193, 0.84369892, 0.96825552,\n",
       "        0.81610193, 0.81842869, 0.81842869, 0.72703534, 0.81610193,\n",
       "        0.82682377, 0.81610578, 0.81842869, 0.82682377, 0.82682377,\n",
       "        0.81610193, 0.81842869],\n",
       "       [0.20853938, 0.69352328, 0.81970204, 0.82815637, 0.81483874,\n",
       "        0.82815637, 0.8664873 , 0.82815637, 0.89503661, 0.82013912,\n",
       "        0.82161984, 0.77203385, 0.81850584, 0.81970204, 0.81970204,\n",
       "        0.72372433, 0.82013912, 0.8664873 , 0.82013912, 0.81483874,\n",
       "        0.81970204, 0.82161984, 0.81850584, 0.81850584, 0.82409753,\n",
       "        0.82409753, 0.81970204, 0.81970204, 0.82815637, 0.91761181,\n",
       "        0.81970204, 0.81850584, 0.81850584, 0.47805587, 0.81970204,\n",
       "        0.82013912, 0.82161984, 0.81850584, 0.82013912, 0.82013912,\n",
       "        0.81970204, 0.81850584],\n",
       "       [0.23438396, 0.68373608, 0.82124672, 0.82913328, 0.81583339,\n",
       "        0.82913328, 0.86864677, 0.82913328, 0.89778711, 0.82186094,\n",
       "        0.82301301, 0.76825508, 0.82012606, 0.82124672, 0.82124672,\n",
       "        0.68616454, 0.82186094, 0.86864677, 0.82186094, 0.81583339,\n",
       "        0.82124672, 0.82301301, 0.82012606, 0.82012606, 0.82510506,\n",
       "        0.82510506, 0.82124672, 0.82124672, 0.82913328, 0.92066971,\n",
       "        0.82124672, 0.82012606, 0.82012606, 0.45631616, 0.82124672,\n",
       "        0.82186094, 0.82301301, 0.82012606, 0.82186094, 0.82186094,\n",
       "        0.82124672, 0.82012606]])"
      ]
     },
     "execution_count": 929,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_result_list[0:6,(diff > 0.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 934,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6487191  0.52181983 0.60300223 0.37235965 0.31958812 0.62969434\n",
      " 0.65167592 0.64982476 0.63149724 0.71360303 0.64982476 0.70723148\n",
      " 0.64982476 0.76199569 0.7507356  0.53693104 0.66750831 0.83985609\n",
      " 0.3553015  0.66574747 0.65786512 0.51837613 0.77553389 0.76789859\n",
      " 0.67538263 0.86425427 0.65581066 0.65167592 0.65167592 0.72790623\n",
      " 0.65812582 0.46840968 0.76308054 0.82281385 0.66750831 0.70723148\n",
      " 0.66750831 0.63149724 0.5274681  0.65167592 0.65786512 0.65581066\n",
      " 0.65581066 0.81636217 0.64902665 0.66665546 0.64902665 0.65167592\n",
      " 0.65167592 0.62452867 0.64982476 0.39047746 0.33852316 0.83139836\n",
      " 0.63551554 0.43451361 0.38812871 0.50519529 0.61586713 0.64619127\n",
      " 0.52809992 0.48133154 0.65167592 0.65581066 0.36023844 0.65581066\n",
      " 0.85448117 0.39790922 0.74818212 0.49006211 0.65167592 0.66750831\n",
      " 0.65786512 0.65581066 0.66750831 0.66750831 0.65167592 0.86449789\n",
      " 0.65581066]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1\n",
      " 1 0 1 1 1 1 1 1 0 1 1 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 1 1 1 1\n",
      " 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "kf_final_results = []\n",
    "for i in range(np.shape(prediction_result_list)[1]):\n",
    "    kf_final_results.append(filter_update_list(0.1, list(prediction_result_list[:6,i])))\n",
    "\n",
    "print(np.array(kf_final_results)[(diff > 0.4)])\n",
    "print(y[(diff > 0.4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5206536541080345,\n",
       " 0.5209090909090909,\n",
       " 0.520828590940132,\n",
       " 0.5209281164695178,\n",
       " 0.5209281164695178,\n",
       " 0.521203830369357,\n",
       " 0.5213226909920182,\n",
       " 0.5209662716499543,\n",
       " 0.5218585488670177,\n",
       " 0.5213009619789281,\n",
       " 0.5228005527406726,\n",
       " 0.5222016651248844,\n",
       " 0.5216184971098267,\n",
       " 0.5217190388170055]"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1032,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5718954248366013, 0.5718954248366013, 0.5742251223491027, 0.5742251223491027, 0.5742251223491027, 0.5742251223491027, 0.5742251223491027, 0.5742251223491027, 0.5742251223491027, 0.5742251223491027, 0.5742251223491027, 0.5742251223491027, 0.5742251223491027, 0.5742251223491027]\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "for j in range(1, 23-8):\n",
    "    kf_final_results = []\n",
    "    for i in range(np.shape(prediction_result_list)[1]):\n",
    "        cur_prediction_result_list = prediction_result_list[:23-8,i][::-1]\n",
    "        kf_final_results.append(filter_update_list(0.1, list(cur_prediction_result_list[:j])))\n",
    "    kf_final_results = np.array(kf_final_results)\n",
    "    predictions = np.array([int(i) for i in kf_final_results>0.5])\n",
    "    result.append(metrics.f1_score(y, predictions))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x12eb24910>"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(1, 8), cur_f1_scores[1:8], label='best-f1')\n",
    "plt.plot(range(1, 8), result, label='KF')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8381742738589212\n",
      "0.831275720164609\n"
     ]
    }
   ],
   "source": [
    "print(max(cur_f1_scores))\n",
    "print(max(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LR(L, scores):\n",
    "    print('L shape', np.shape(L))\n",
    "    num_methods = np.shape(L)[1]\n",
    "\n",
    "    agree_outlier_indexes = np.sum(L,axis=1)==np.shape(L)[1]\n",
    "#     print('All agree, Number of outliers = {}'.format(sum(agree_outlier_indexes)))\n",
    "    agree_inlier_indexes = np.sum(L,axis=1)==0\n",
    "#     print('All agree, Number of inliers = {}'.format(sum(agree_inlier_indexes)))\n",
    "\n",
    "    disagree_indexes = np.where(np.logical_or(np.sum(L,axis = 1)==0, np.sum(L,axis = 1)==num_methods)==0)[0]\n",
    "    self_agree_index_list = []\n",
    "    for i in range(0, len(index_range)):\n",
    "        if(index_range[i,1]-index_range[i,0] < 1):\n",
    "            continue\n",
    "        temp_index = disagree_indexes[np.where(np.sum(L[disagree_indexes][:,index_range[i,0]: index_range[i,1]], axis = 1)==(index_range[i,1]-index_range[i,0]))[0]]\n",
    "        self_agree_index_list = np.union1d(self_agree_index_list, temp_index)\n",
    "    self_agree_index_list = [int(i) for i in self_agree_index_list]\n",
    "    \n",
    "    all_inlier_indexes = np.where(agree_inlier_indexes)[0]\n",
    "#     print('num of inliers = {}'.format(np.shape(all_inlier_indexes)[0]))\n",
    "    all_outlier_indexes = np.union1d(np.where(agree_outlier_indexes)[0], self_agree_index_list)\n",
    "#     print('num of outliers = {}'.format(np.shape(all_outlier_indexes)[0]))\n",
    "    \n",
    "    from sklearn.preprocessing import RobustScaler\n",
    "    data_indexes = np.concatenate((all_inlier_indexes, all_outlier_indexes), axis = 0)\n",
    "    data_indexes = np.array([int(i) for i in data_indexes])\n",
    "    labels = np.concatenate((np.zeros(len(all_inlier_indexes)), np.ones(len(all_outlier_indexes))), axis = 0)\n",
    "    transformer = RobustScaler().fit(scores)\n",
    "    scores_transformed = transformer.transform(scores)\n",
    "    training_data = scores_transformed[data_indexes]\n",
    "\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    clf = LogisticRegression(random_state=0, penalty='l2').fit(training_data, labels)\n",
    "    clf_predictions = clf.predict(scores_transformed)\n",
    "    print(\"F-1 score:\",metrics.f1_score(y, clf_predictions))\n",
    "    \n",
    "    agreed_outlier_indexes = np.where(np.sum(L,axis=1)==np.shape(L)[1])[0]\n",
    "    agreed_inlier_indexes = np.where(np.sum(L,axis=1)==0)[0]\n",
    "    return metrics.f1_score(y, clf_predictions), clf.predict_proba(scores_transformed)[:,1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5339547270306259,\n",
       " array([2.85098071e-03, 1.24246216e-04, 4.15808259e-05, ...,\n",
       "        2.74592296e-04, 1.33453116e-04, 9.03708973e-01]))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_LR(L, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3]\n",
      "5\n",
      "9\n",
      "2\n",
      "1\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[2 0 3 1]\n",
      "4\n",
      "7\n",
      "1\n",
      "9\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[2]\n",
      "2\n",
      "5\n",
      "L shape (5473, 5)\n",
      "F-1 score: 0.422282120395328\n",
      "[0 1 3 2]\n",
      "3\n",
      "1\n",
      "1\n",
      "4\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[3]\n",
      "1\n",
      "1\n",
      "L shape (5473, 1)\n",
      "F-1 score: 0.527336860670194\n",
      "[2 1 0]\n",
      "4\n",
      "1\n",
      "4\n",
      "25\n",
      "L shape (5473, 25)\n",
      "F-1 score: 0.47679324894514763\n",
      "[2 1 0]\n",
      "4\n",
      "4\n",
      "2\n",
      "25\n",
      "L shape (5473, 25)\n",
      "F-1 score: 0.47679324894514763\n",
      "[1 2 0]\n",
      "2\n",
      "5\n",
      "1\n",
      "25\n",
      "L shape (5473, 25)\n",
      "F-1 score: 0.47679324894514763\n",
      "[0]\n",
      "6\n",
      "10\n",
      "L shape (5473, 10)\n",
      "F-1 score: 0.4952919020715631\n",
      "[2 3 0 1]\n",
      "5\n",
      "1\n",
      "1\n",
      "10\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[1]\n",
      "9\n",
      "10\n",
      "L shape (5473, 10)\n",
      "F-1 score: 0.4148430066603235\n",
      "[3 1 2]\n",
      "1\n",
      "2\n",
      "3\n",
      "16\n",
      "L shape (5473, 16)\n",
      "F-1 score: 0.4134705332086062\n",
      "[0 1]\n",
      "7\n",
      "8\n",
      "20\n",
      "L shape (5473, 20)\n",
      "F-1 score: 0.4672454617205999\n",
      "[2]\n",
      "5\n",
      "5\n",
      "L shape (5473, 5)\n",
      "F-1 score: 0.422282120395328\n",
      "[1 0 2]\n",
      "5\n",
      "3\n",
      "2\n",
      "25\n",
      "L shape (5473, 25)\n",
      "F-1 score: 0.47679324894514763\n",
      "[0 1 2 3]\n",
      "3\n",
      "10\n",
      "5\n",
      "1\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[2 3]\n",
      "2\n",
      "1\n",
      "6\n",
      "L shape (5473, 6)\n",
      "F-1 score: 0.4392439243924392\n",
      "[0 3 2]\n",
      "2\n",
      "1\n",
      "5\n",
      "16\n",
      "L shape (5473, 16)\n",
      "F-1 score: 0.4949402023919043\n",
      "[2 0 1 3]\n",
      "5\n",
      "1\n",
      "6\n",
      "1\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[0]\n",
      "2\n",
      "10\n",
      "L shape (5473, 10)\n",
      "F-1 score: 0.4952919020715631\n",
      "[1 0 2]\n",
      "6\n",
      "10\n",
      "4\n",
      "25\n",
      "L shape (5473, 25)\n",
      "F-1 score: 0.47679324894514763\n",
      "[2 1 3]\n",
      "5\n",
      "1\n",
      "1\n",
      "16\n",
      "L shape (5473, 16)\n",
      "F-1 score: 0.4134705332086062\n",
      "[1 2 3]\n",
      "2\n",
      "1\n",
      "1\n",
      "16\n",
      "L shape (5473, 16)\n",
      "F-1 score: 0.4134705332086062\n",
      "[3 2 0]\n",
      "1\n",
      "1\n",
      "3\n",
      "16\n",
      "L shape (5473, 16)\n",
      "F-1 score: 0.4949402023919043\n",
      "[1 0 2]\n",
      "10\n",
      "8\n",
      "5\n",
      "25\n",
      "L shape (5473, 25)\n",
      "F-1 score: 0.47679324894514763\n",
      "[2]\n",
      "3\n",
      "5\n",
      "L shape (5473, 5)\n",
      "F-1 score: 0.422282120395328\n",
      "[2 3 0 1]\n",
      "3\n",
      "1\n",
      "6\n",
      "6\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[2 1]\n",
      "4\n",
      "5\n",
      "15\n",
      "L shape (5473, 15)\n",
      "F-1 score: 0.40866290018832396\n",
      "[3]\n",
      "1\n",
      "1\n",
      "L shape (5473, 1)\n",
      "F-1 score: 0.527336860670194\n",
      "[0]\n",
      "10\n",
      "10\n",
      "L shape (5473, 10)\n",
      "F-1 score: 0.4952919020715631\n",
      "[0 2 1 3]\n",
      "5\n",
      "5\n",
      "2\n",
      "1\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[0 3 1 2]\n",
      "7\n",
      "1\n",
      "3\n",
      "2\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[1]\n",
      "1\n",
      "10\n",
      "L shape (5473, 10)\n",
      "F-1 score: 0.4148430066603235\n",
      "[0]\n",
      "6\n",
      "10\n",
      "L shape (5473, 10)\n",
      "F-1 score: 0.4952919020715631\n",
      "[0 2 3]\n",
      "5\n",
      "2\n",
      "1\n",
      "16\n",
      "L shape (5473, 16)\n",
      "F-1 score: 0.4949402023919043\n",
      "[0 1 2 3]\n",
      "6\n",
      "3\n",
      "3\n",
      "1\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[3 1 2]\n",
      "1\n",
      "10\n",
      "4\n",
      "16\n",
      "L shape (5473, 16)\n",
      "F-1 score: 0.4134705332086062\n",
      "[1 3 2 0]\n",
      "10\n",
      "1\n",
      "4\n",
      "8\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[3 2 1]\n",
      "1\n",
      "4\n",
      "5\n",
      "16\n",
      "L shape (5473, 16)\n",
      "F-1 score: 0.4134705332086062\n",
      "[3 1 2 0]\n",
      "1\n",
      "2\n",
      "1\n",
      "8\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[2 1]\n",
      "3\n",
      "4\n",
      "15\n",
      "L shape (5473, 15)\n",
      "F-1 score: 0.40866290018832396\n",
      "[2 3]\n",
      "2\n",
      "1\n",
      "6\n",
      "L shape (5473, 6)\n",
      "F-1 score: 0.4392439243924392\n",
      "[3]\n",
      "1\n",
      "1\n",
      "L shape (5473, 1)\n",
      "F-1 score: 0.527336860670194\n",
      "[2 3 0 1]\n",
      "4\n",
      "1\n",
      "9\n",
      "5\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[1 2]\n",
      "9\n",
      "1\n",
      "15\n",
      "L shape (5473, 15)\n",
      "F-1 score: 0.40866290018832396\n",
      "[3 2 0]\n",
      "1\n",
      "1\n",
      "10\n",
      "16\n",
      "L shape (5473, 16)\n",
      "F-1 score: 0.4949402023919043\n",
      "[2 3]\n",
      "5\n",
      "1\n",
      "6\n",
      "L shape (5473, 6)\n",
      "F-1 score: 0.4392439243924392\n",
      "[0 3]\n",
      "3\n",
      "1\n",
      "11\n",
      "L shape (5473, 11)\n",
      "F-1 score: 0.5\n",
      "[2 0 3 1]\n",
      "3\n",
      "2\n",
      "1\n",
      "10\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[1]\n",
      "2\n",
      "10\n",
      "L shape (5473, 10)\n",
      "F-1 score: 0.4148430066603235\n",
      "[2 3 1 0]\n",
      "4\n",
      "1\n",
      "3\n",
      "6\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[1 2]\n",
      "8\n",
      "3\n",
      "15\n",
      "L shape (5473, 15)\n",
      "F-1 score: 0.40866290018832396\n",
      "[1]\n",
      "2\n",
      "10\n",
      "L shape (5473, 10)\n",
      "F-1 score: 0.4148430066603235\n",
      "[3 2 0]\n",
      "1\n",
      "4\n",
      "7\n",
      "16\n",
      "L shape (5473, 16)\n",
      "F-1 score: 0.4949402023919043\n",
      "[3 1 0 2]\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[2 1 3]\n",
      "3\n",
      "5\n",
      "1\n",
      "16\n",
      "L shape (5473, 16)\n",
      "F-1 score: 0.4134705332086062\n",
      "[0 1 2 3]\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[2]\n",
      "5\n",
      "5\n",
      "L shape (5473, 5)\n",
      "F-1 score: 0.422282120395328\n",
      "[0 3 1 2]\n",
      "10\n",
      "1\n",
      "5\n",
      "2\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[3 0]\n",
      "1\n",
      "5\n",
      "11\n",
      "L shape (5473, 11)\n",
      "F-1 score: 0.5\n",
      "[3 0]\n",
      "1\n",
      "4\n",
      "11\n",
      "L shape (5473, 11)\n",
      "F-1 score: 0.5\n",
      "[1 0]\n",
      "4\n",
      "4\n",
      "20\n",
      "L shape (5473, 20)\n",
      "F-1 score: 0.4672454617205999\n",
      "[2 3]\n",
      "4\n",
      "1\n",
      "6\n",
      "L shape (5473, 6)\n",
      "F-1 score: 0.4392439243924392\n",
      "[2]\n",
      "3\n",
      "5\n",
      "L shape (5473, 5)\n",
      "F-1 score: 0.422282120395328\n",
      "[1 2 0 3]\n",
      "2\n",
      "5\n",
      "6\n",
      "1\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[0 3]\n",
      "7\n",
      "1\n",
      "11\n",
      "L shape (5473, 11)\n",
      "F-1 score: 0.5\n",
      "[1 2 0 3]\n",
      "1\n",
      "4\n",
      "1\n",
      "1\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[0 1 2]\n",
      "7\n",
      "2\n",
      "4\n",
      "25\n",
      "L shape (5473, 25)\n",
      "F-1 score: 0.47679324894514763\n",
      "[3 0 1]\n",
      "1\n",
      "10\n",
      "3\n",
      "21\n",
      "L shape (5473, 21)\n",
      "F-1 score: 0.4807987711213517\n",
      "[3 0 1]\n",
      "1\n",
      "8\n",
      "9\n",
      "21\n",
      "L shape (5473, 21)\n",
      "F-1 score: 0.4807987711213517\n",
      "[2 1]\n",
      "4\n",
      "7\n",
      "15\n",
      "L shape (5473, 15)\n",
      "F-1 score: 0.40866290018832396\n",
      "[0 1 3]\n",
      "5\n",
      "10\n",
      "1\n",
      "21\n",
      "L shape (5473, 21)\n",
      "F-1 score: 0.4807987711213517\n",
      "[3 0 1 2]\n",
      "1\n",
      "8\n",
      "6\n",
      "2\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[0 2 3]\n",
      "1\n",
      "3\n",
      "1\n",
      "16\n",
      "L shape (5473, 16)\n",
      "F-1 score: 0.4949402023919043\n",
      "[2 1]\n",
      "4\n",
      "6\n",
      "15\n",
      "L shape (5473, 15)\n",
      "F-1 score: 0.40866290018832396\n",
      "[0 1 3]\n",
      "7\n",
      "7\n",
      "1\n",
      "21\n",
      "L shape (5473, 21)\n",
      "F-1 score: 0.4807987711213517\n",
      "[2]\n",
      "5\n",
      "5\n",
      "L shape (5473, 5)\n",
      "F-1 score: 0.422282120395328\n",
      "[3 0 1]\n",
      "1\n",
      "6\n",
      "10\n",
      "21\n",
      "L shape (5473, 21)\n",
      "F-1 score: 0.4807987711213517\n",
      "[2]\n",
      "4\n",
      "5\n",
      "L shape (5473, 5)\n",
      "F-1 score: 0.422282120395328\n",
      "[1 3 0]\n",
      "8\n",
      "1\n",
      "10\n",
      "21\n",
      "L shape (5473, 21)\n",
      "F-1 score: 0.4807987711213517\n",
      "[0 2 1]\n",
      "4\n",
      "4\n",
      "9\n",
      "25\n",
      "L shape (5473, 25)\n",
      "F-1 score: 0.47679324894514763\n",
      "[0 2 3]\n",
      "10\n",
      "3\n",
      "1\n",
      "16\n",
      "L shape (5473, 16)\n",
      "F-1 score: 0.4949402023919043\n",
      "[0 3 2]\n",
      "2\n",
      "1\n",
      "1\n",
      "16\n",
      "L shape (5473, 16)\n",
      "F-1 score: 0.4949402023919043\n",
      "[1 2 0]\n",
      "2\n",
      "4\n",
      "8\n",
      "25\n",
      "L shape (5473, 25)\n",
      "F-1 score: 0.47679324894514763\n",
      "[2 0 3 1]\n",
      "2\n",
      "6\n",
      "1\n",
      "7\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[0 2]\n",
      "1\n",
      "5\n",
      "15\n",
      "L shape (5473, 15)\n",
      "F-1 score: 0.49250936329588013\n",
      "[0 2]\n",
      "9\n",
      "2\n",
      "15\n",
      "L shape (5473, 15)\n",
      "F-1 score: 0.49250936329588013\n",
      "[1 2]\n",
      "10\n",
      "5\n",
      "15\n",
      "L shape (5473, 15)\n",
      "F-1 score: 0.40866290018832396\n",
      "[3]\n",
      "1\n",
      "1\n",
      "L shape (5473, 1)\n",
      "F-1 score: 0.527336860670194\n",
      "[1 2 0]\n",
      "7\n",
      "3\n",
      "9\n",
      "25\n",
      "L shape (5473, 25)\n",
      "F-1 score: 0.47679324894514763\n",
      "[0 2 1 3]\n",
      "5\n",
      "5\n",
      "2\n",
      "1\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[2 1]\n",
      "5\n",
      "7\n",
      "15\n",
      "L shape (5473, 15)\n",
      "F-1 score: 0.40866290018832396\n",
      "[2 0 3]\n",
      "4\n",
      "2\n",
      "1\n",
      "16\n",
      "L shape (5473, 16)\n",
      "F-1 score: 0.4949402023919043\n",
      "[0 2 1]\n",
      "1\n",
      "1\n",
      "2\n",
      "25\n",
      "L shape (5473, 25)\n",
      "F-1 score: 0.47679324894514763\n",
      "[0 1 2 3]\n",
      "3\n",
      "9\n",
      "4\n",
      "1\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[3 1 2]\n",
      "1\n",
      "5\n",
      "2\n",
      "16\n",
      "L shape (5473, 16)\n",
      "F-1 score: 0.4134705332086062\n",
      "[0 3 2]\n",
      "3\n",
      "1\n",
      "5\n",
      "16\n",
      "L shape (5473, 16)\n",
      "F-1 score: 0.4949402023919043\n",
      "[0]\n",
      "10\n",
      "10\n",
      "L shape (5473, 10)\n",
      "F-1 score: 0.4952919020715631\n",
      "[0 3 2 1]\n",
      "8\n",
      "1\n",
      "4\n",
      "6\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[3]\n",
      "1\n",
      "1\n",
      "L shape (5473, 1)\n",
      "F-1 score: 0.527336860670194\n",
      "[0]\n",
      "1\n",
      "10\n",
      "L shape (5473, 10)\n",
      "F-1 score: 0.4952919020715631\n",
      "[1]\n",
      "5\n",
      "10\n",
      "L shape (5473, 10)\n",
      "F-1 score: 0.4148430066603235\n",
      "[3 1 2 0]\n",
      "1\n",
      "2\n",
      "1\n",
      "7\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[3 2 1 0]\n",
      "1\n",
      "4\n",
      "8\n",
      "10\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[2 3]\n",
      "4\n",
      "1\n",
      "6\n",
      "L shape (5473, 6)\n",
      "F-1 score: 0.4392439243924392\n",
      "[3 0]\n",
      "1\n",
      "10\n",
      "11\n",
      "L shape (5473, 11)\n",
      "F-1 score: 0.5\n",
      "[1 3 2]\n",
      "2\n",
      "1\n",
      "5\n",
      "16\n",
      "L shape (5473, 16)\n",
      "F-1 score: 0.4134705332086062\n",
      "[3 0]\n",
      "1\n",
      "7\n",
      "11\n",
      "L shape (5473, 11)\n",
      "F-1 score: 0.5\n",
      "[2 3 1]\n",
      "5\n",
      "1\n",
      "6\n",
      "16\n",
      "L shape (5473, 16)\n",
      "F-1 score: 0.4134705332086062\n",
      "[3 2]\n",
      "1\n",
      "3\n",
      "6\n",
      "L shape (5473, 6)\n",
      "F-1 score: 0.4392439243924392\n",
      "[3 0 1]\n",
      "1\n",
      "8\n",
      "3\n",
      "21\n",
      "L shape (5473, 21)\n",
      "F-1 score: 0.4807987711213517\n",
      "[2 0]\n",
      "4\n",
      "10\n",
      "15\n",
      "L shape (5473, 15)\n",
      "F-1 score: 0.49250936329588013\n",
      "[0 2 3]\n",
      "9\n",
      "4\n",
      "1\n",
      "16\n",
      "L shape (5473, 16)\n",
      "F-1 score: 0.4949402023919043\n",
      "[2 1 3 0]\n",
      "1\n",
      "6\n",
      "1\n",
      "1\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[0 2]\n",
      "5\n",
      "4\n",
      "15\n",
      "L shape (5473, 15)\n",
      "F-1 score: 0.49250936329588013\n",
      "[0]\n",
      "5\n",
      "10\n",
      "L shape (5473, 10)\n",
      "F-1 score: 0.4952919020715631\n",
      "[3 0]\n",
      "1\n",
      "10\n",
      "11\n",
      "L shape (5473, 11)\n",
      "F-1 score: 0.5\n",
      "[2 1 0]\n",
      "4\n",
      "10\n",
      "5\n",
      "25\n",
      "L shape (5473, 25)\n",
      "F-1 score: 0.47679324894514763\n",
      "[2 1 0 3]\n",
      "3\n",
      "7\n",
      "1\n",
      "1\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[1 2]\n",
      "7\n",
      "4\n",
      "15\n",
      "L shape (5473, 15)\n",
      "F-1 score: 0.40866290018832396\n",
      "[1]\n",
      "9\n",
      "10\n",
      "L shape (5473, 10)\n",
      "F-1 score: 0.4148430066603235\n",
      "[0 2]\n",
      "1\n",
      "1\n",
      "15\n",
      "L shape (5473, 15)\n",
      "F-1 score: 0.49250936329588013\n",
      "[1]\n",
      "6\n",
      "10\n",
      "L shape (5473, 10)\n",
      "F-1 score: 0.4148430066603235\n",
      "[2 0 3 1]\n",
      "4\n",
      "1\n",
      "1\n",
      "9\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[1 0]\n",
      "1\n",
      "4\n",
      "20\n",
      "L shape (5473, 20)\n",
      "F-1 score: 0.4672454617205999\n",
      "[0 1 3 2]\n",
      "6\n",
      "1\n",
      "1\n",
      "1\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[2 3 1 0]\n",
      "4\n",
      "1\n",
      "2\n",
      "10\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[1]\n",
      "3\n",
      "10\n",
      "L shape (5473, 10)\n",
      "F-1 score: 0.4148430066603235\n",
      "[0 1 2]\n",
      "8\n",
      "4\n",
      "4\n",
      "25\n",
      "L shape (5473, 25)\n",
      "F-1 score: 0.47679324894514763\n",
      "[3 1 0 2]\n",
      "1\n",
      "3\n",
      "5\n",
      "3\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[2 1]\n",
      "3\n",
      "4\n",
      "15\n",
      "L shape (5473, 15)\n",
      "F-1 score: 0.40866290018832396\n",
      "[3 0 2 1]\n",
      "1\n",
      "2\n",
      "3\n",
      "5\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[0 2 3]\n",
      "3\n",
      "4\n",
      "1\n",
      "16\n",
      "L shape (5473, 16)\n",
      "F-1 score: 0.4949402023919043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 0 2 1]\n",
      "1\n",
      "5\n",
      "4\n",
      "6\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[3]\n",
      "1\n",
      "1\n",
      "L shape (5473, 1)\n",
      "F-1 score: 0.527336860670194\n",
      "[2 0]\n",
      "2\n",
      "2\n",
      "15\n",
      "L shape (5473, 15)\n",
      "F-1 score: 0.49250936329588013\n",
      "[0 1 3]\n",
      "8\n",
      "4\n",
      "1\n",
      "21\n",
      "L shape (5473, 21)\n",
      "F-1 score: 0.4807987711213517\n",
      "[0]\n",
      "8\n",
      "10\n",
      "L shape (5473, 10)\n",
      "F-1 score: 0.4952919020715631\n",
      "[3]\n",
      "1\n",
      "1\n",
      "L shape (5473, 1)\n",
      "F-1 score: 0.527336860670194\n",
      "[0 3]\n",
      "9\n",
      "1\n",
      "11\n",
      "L shape (5473, 11)\n",
      "F-1 score: 0.5\n",
      "[2 0 1]\n",
      "2\n",
      "1\n",
      "5\n",
      "25\n",
      "L shape (5473, 25)\n",
      "F-1 score: 0.47679324894514763\n",
      "[2 3 1 0]\n",
      "3\n",
      "1\n",
      "8\n",
      "3\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[3 2 1]\n",
      "1\n",
      "1\n",
      "6\n",
      "16\n",
      "L shape (5473, 16)\n",
      "F-1 score: 0.4134705332086062\n",
      "[2 3]\n",
      "5\n",
      "1\n",
      "6\n",
      "L shape (5473, 6)\n",
      "F-1 score: 0.4392439243924392\n",
      "[1 0 3]\n",
      "7\n",
      "3\n",
      "1\n",
      "21\n",
      "L shape (5473, 21)\n",
      "F-1 score: 0.4807987711213517\n",
      "[3 2 1]\n",
      "1\n",
      "4\n",
      "9\n",
      "16\n",
      "L shape (5473, 16)\n",
      "F-1 score: 0.4134705332086062\n",
      "[0 1 2 3]\n",
      "10\n",
      "6\n",
      "5\n",
      "1\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[0 1 3 2]\n",
      "6\n",
      "10\n",
      "1\n",
      "3\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[2 0 3]\n",
      "5\n",
      "8\n",
      "1\n",
      "16\n",
      "L shape (5473, 16)\n",
      "F-1 score: 0.4949402023919043\n",
      "[2 3 0 1]\n",
      "3\n",
      "1\n",
      "4\n",
      "2\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[0 3 2 1]\n",
      "10\n",
      "1\n",
      "1\n",
      "9\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[2]\n",
      "2\n",
      "5\n",
      "L shape (5473, 5)\n",
      "F-1 score: 0.422282120395328\n",
      "[2]\n",
      "5\n",
      "5\n",
      "L shape (5473, 5)\n",
      "F-1 score: 0.422282120395328\n",
      "[0 3]\n",
      "4\n",
      "1\n",
      "11\n",
      "L shape (5473, 11)\n",
      "F-1 score: 0.5\n",
      "[2 3 1]\n",
      "4\n",
      "1\n",
      "1\n",
      "16\n",
      "L shape (5473, 16)\n",
      "F-1 score: 0.4134705332086062\n",
      "[1 3 0]\n",
      "4\n",
      "1\n",
      "4\n",
      "21\n",
      "L shape (5473, 21)\n",
      "F-1 score: 0.4807987711213517\n",
      "[1]\n",
      "1\n",
      "10\n",
      "L shape (5473, 10)\n",
      "F-1 score: 0.4148430066603235\n",
      "[2 1 0 3]\n",
      "5\n",
      "10\n",
      "1\n",
      "1\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[3]\n",
      "1\n",
      "1\n",
      "L shape (5473, 1)\n",
      "F-1 score: 0.527336860670194\n",
      "[2 0 1]\n",
      "3\n",
      "3\n",
      "3\n",
      "25\n",
      "L shape (5473, 25)\n",
      "F-1 score: 0.47679324894514763\n",
      "[0 2 3 1]\n",
      "10\n",
      "4\n",
      "1\n",
      "8\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[3 2]\n",
      "1\n",
      "2\n",
      "6\n",
      "L shape (5473, 6)\n",
      "F-1 score: 0.4392439243924392\n",
      "[3]\n",
      "1\n",
      "1\n",
      "L shape (5473, 1)\n",
      "F-1 score: 0.527336860670194\n",
      "[1 2 3 0]\n",
      "5\n",
      "3\n",
      "1\n",
      "6\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[1 3 0 2]\n",
      "7\n",
      "1\n",
      "2\n",
      "3\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[2 0 1]\n",
      "2\n",
      "5\n",
      "2\n",
      "25\n",
      "L shape (5473, 25)\n",
      "F-1 score: 0.47679324894514763\n",
      "[3 1]\n",
      "1\n",
      "4\n",
      "11\n",
      "L shape (5473, 11)\n",
      "F-1 score: 0.4199623352165725\n",
      "[3 1 2 0]\n",
      "1\n",
      "1\n",
      "1\n",
      "3\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[3 0 1]\n",
      "1\n",
      "10\n",
      "6\n",
      "21\n",
      "L shape (5473, 21)\n",
      "F-1 score: 0.4807987711213517\n",
      "[1 0 2 3]\n",
      "7\n",
      "9\n",
      "5\n",
      "1\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[1 0 3]\n",
      "1\n",
      "7\n",
      "1\n",
      "21\n",
      "L shape (5473, 21)\n",
      "F-1 score: 0.4807987711213517\n",
      "[1 2 0]\n",
      "10\n",
      "3\n",
      "5\n",
      "25\n",
      "L shape (5473, 25)\n",
      "F-1 score: 0.47679324894514763\n",
      "[1 2 3 0]\n",
      "9\n",
      "4\n",
      "1\n",
      "5\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[0 3 1 2]\n",
      "3\n",
      "1\n",
      "9\n",
      "5\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[1]\n",
      "7\n",
      "10\n",
      "L shape (5473, 10)\n",
      "F-1 score: 0.4148430066603235\n",
      "[0]\n",
      "10\n",
      "10\n",
      "L shape (5473, 10)\n",
      "F-1 score: 0.4952919020715631\n",
      "[0 2 1 3]\n",
      "9\n",
      "5\n",
      "3\n",
      "1\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[3 2]\n",
      "1\n",
      "1\n",
      "6\n",
      "L shape (5473, 6)\n",
      "F-1 score: 0.4392439243924392\n",
      "[2 1 0]\n",
      "5\n",
      "10\n",
      "10\n",
      "25\n",
      "L shape (5473, 25)\n",
      "F-1 score: 0.47679324894514763\n",
      "[0 1 2]\n",
      "3\n",
      "4\n",
      "3\n",
      "25\n",
      "L shape (5473, 25)\n",
      "F-1 score: 0.47679324894514763\n",
      "[0]\n",
      "9\n",
      "10\n",
      "L shape (5473, 10)\n",
      "F-1 score: 0.4952919020715631\n",
      "[0 2]\n",
      "10\n",
      "2\n",
      "15\n",
      "L shape (5473, 15)\n",
      "F-1 score: 0.49250936329588013\n",
      "[1 0 2 3]\n",
      "7\n",
      "4\n",
      "4\n",
      "1\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[2 3 1]\n",
      "3\n",
      "1\n",
      "9\n",
      "16\n",
      "L shape (5473, 16)\n",
      "F-1 score: 0.4134705332086062\n",
      "[2 1]\n",
      "2\n",
      "1\n",
      "15\n",
      "L shape (5473, 15)\n",
      "F-1 score: 0.40866290018832396\n",
      "[0 1 2]\n",
      "8\n",
      "10\n",
      "3\n",
      "25\n",
      "L shape (5473, 25)\n",
      "F-1 score: 0.47679324894514763\n",
      "[2 3 0]\n",
      "2\n",
      "1\n",
      "6\n",
      "16\n",
      "L shape (5473, 16)\n",
      "F-1 score: 0.4949402023919043\n",
      "[1 0]\n",
      "1\n",
      "10\n",
      "20\n",
      "L shape (5473, 20)\n",
      "F-1 score: 0.4672454617205999\n",
      "[0 2]\n",
      "9\n",
      "3\n",
      "15\n",
      "L shape (5473, 15)\n",
      "F-1 score: 0.49250936329588013\n",
      "[3 1]\n",
      "1\n",
      "7\n",
      "11\n",
      "L shape (5473, 11)\n",
      "F-1 score: 0.4199623352165725\n",
      "[0 2 3 1]\n",
      "7\n",
      "4\n",
      "1\n",
      "8\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[2]\n",
      "5\n",
      "5\n",
      "L shape (5473, 5)\n",
      "F-1 score: 0.422282120395328\n",
      "[2 0 3]\n",
      "2\n",
      "8\n",
      "1\n",
      "16\n",
      "L shape (5473, 16)\n",
      "F-1 score: 0.4949402023919043\n",
      "[0]\n",
      "9\n",
      "10\n",
      "L shape (5473, 10)\n",
      "F-1 score: 0.4952919020715631\n",
      "[2 3]\n",
      "1\n",
      "1\n",
      "6\n",
      "L shape (5473, 6)\n",
      "F-1 score: 0.4392439243924392\n",
      "[1 3 2 0]\n",
      "2\n",
      "1\n",
      "5\n",
      "6\n",
      "26\n",
      "L shape (5473, 26)\n",
      "F-1 score: 0.5339547270306259\n",
      "[3]\n",
      "1\n",
      "1\n",
      "L shape (5473, 1)\n",
      "F-1 score: 0.527336860670194\n",
      "[0 3]\n",
      "9\n",
      "1\n",
      "11\n",
      "L shape (5473, 11)\n",
      "F-1 score: 0.5\n",
      "[3 0 1]\n",
      "1\n",
      "10\n",
      "10\n",
      "21\n",
      "L shape (5473, 21)\n",
      "F-1 score: 0.4807987711213517\n",
      "[3 0 1]\n",
      "1\n",
      "2\n",
      "9\n",
      "21\n",
      "L shape (5473, 21)\n",
      "F-1 score: 0.4807987711213517\n"
     ]
    }
   ],
   "source": [
    "num_method = 4\n",
    "random = np.random.RandomState(1234)\n",
    "cur_f1_scores = []\n",
    "prediction_result_list = []\n",
    "\n",
    "for i in range(0, 200):\n",
    "    cur_num_method = random.randint(1, num_method+1)\n",
    "    cur_method_list = random.permutation(range(4))[:cur_num_method]\n",
    "    print(cur_method_list)\n",
    "    all_indexes = []\n",
    "    for method in cur_method_list:\n",
    "        s, e = index_range[method]\n",
    "        print(random.randint(1,e-s+1))\n",
    "        all_indexes.extend(range(s,e))\n",
    "    print(len(all_indexes))\n",
    "#         all_indexes.extend(random.permutation(range(s,e))[:random.randint(1,e-s+1)])\n",
    "    all_indexes = np.sort(all_indexes)\n",
    "    f1, prediction = train_LR(L[:,all_indexes], scores[:,all_indexes])\n",
    "    cur_f1_scores.append(f1)\n",
    "    prediction_result_list.append(prediction)\n",
    "    \n",
    "# print(np.shape(prediction_result_list))\n",
    "# print(cur_f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5339547270306259"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(cur_f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-f3405eb837fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mkf_final_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_result_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mkf_final_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter_update_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_result_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mkf_final_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkf_final_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkf_final_results\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result = []\n",
    "for j in range(1, 2000):\n",
    "    kf_final_results = []\n",
    "    for i in range(np.shape(prediction_result_list)[1]):\n",
    "        kf_final_results.append(filter_update_list(0.1, list(np.array(prediction_result_list)[:j,i])))\n",
    "    kf_final_results = np.array(kf_final_results)\n",
    "    predictions = np.array([int(i) for i in kf_final_results>0.5])\n",
    "    result.append(metrics.f1_score(y, predictions))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 5473)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(prediction_result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48520710059171596"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_result_list = np.array(prediction_result_list)\n",
    "kf_final_results = []\n",
    "for i in range(np.shape(prediction_result_list)[1]):\n",
    "    kf_final_results.append(filter_update_list(0.1, list(prediction_result_list[:,i])))\n",
    "kf_final_results = np.array(kf_final_results)\n",
    "predictions = np.array([int(i) for i in kf_final_results>0.5])\n",
    "metrics.f1_score(y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(1, 2000), result, label='KF')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(309, 16)\n",
      "(309,)\n"
     ]
    }
   ],
   "source": [
    "# get obvious errors and remove\n",
    "remove_inliers = agreed_inlier_indexes[clf_predictions[agreed_inlier_indexes]==1]\n",
    "all_inlier_indexes = np.setdiff1d(all_inlier_indexes,remove_inliers)\n",
    "data_indexes = np.concatenate((all_inlier_indexes, all_outlier_indexes), axis = 0)\n",
    "labels = np.concatenate((np.zeros(len(all_inlier_indexes)), np.ones(len(all_outlier_indexes))), axis = 0)\n",
    "transformer = RobustScaler().fit(scores)\n",
    "scores_transformed = transformer.transform(scores)\n",
    "training_data = scores_transformed[data_indexes]\n",
    "print(np.shape(training_data))\n",
    "print(np.shape(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n",
      "52.0\n",
      "[[55 30]\n",
      " [ 5 22]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.75862069, 0.55696203])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unused_indexes = np.setdiff1d(range(0, 5473), np.union1d(all_inlier_indexes, all_outlier_indexes))\n",
    "# # unused_indexes = disagree_indexes\n",
    "# unused_predict_proba = clf.predict_proba(scores_transformed)[unused_indexes]\n",
    "# threshold = 0.99\n",
    "# certain_predictions = np.where(np.max(unused_predict_proba, axis = 1) > threshold)[0]\n",
    "# print(len(certain_predictions))\n",
    "# print(sum(clf_predictions[unused_indexes][certain_predictions]))\n",
    "# print(metrics.confusion_matrix(y[unused_indexes][certain_predictions], \n",
    "#                                clf_predictions[unused_indexes][certain_predictions]))\n",
    "# metrics.f1_score(y[unused_indexes][certain_predictions], clf_predictions[unused_indexes][certain_predictions], average=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = clf.predict_proba(scores_transformed)[:,1]\n",
    "# max_f1 = 0\n",
    "# best_threshold = 0\n",
    "# for threshold in range(100, 400, 10):\n",
    "#     x = np.sort(predictions)[::-1][threshold]\n",
    "#     clf_predictions = [int(i) for i in (predictions>x)]\n",
    "#     cur_f1 = metrics.f1_score(y, clf_predictions)\n",
    "#     if(cur_f1 > max_f1):\n",
    "#         max_f1 = cur_f1\n",
    "# print(max_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 for MV: 0.8278688524590164\n",
      "F1 for MV: 0.6666666666666666\n",
      "43\n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling import MajorityLabelVoter\n",
    "majority_model = MajorityLabelVoter()\n",
    "preds_train = majority_model.predict_proba(L=L)\n",
    "threshold = 0.5\n",
    "predictions = np.full((len(y)), 0)\n",
    "predictions[preds_train[:,1] > threshold] = 1\n",
    "print('F1 for MV:', metrics.f1_score(y, predictions))\n",
    "print('F1 for MV:', metrics.f1_score(y[disagree_indexes], predictions[disagree_indexes]))\n",
    "print(sum(predictions[disagree_indexes]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5445205479452054"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agreement_indexes = np.logical_or(np.sum(L,axis = 1)==0, np.sum(L,axis = 1)==4)==1\n",
    "predictions = np.array([int(i) for i in np.sum(L[agreement_indexes], axis = 1)==4])\n",
    "metrics.f1_score(y[agreement_indexes], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.09237809  0.06831342  0.37657298  2.53878933]\n",
      " [ 1.05785957  0.04875727  0.37202809  0.75776378]\n",
      " [ 1.00117835  0.04275626  0.3724401   1.06894808]\n",
      " ...\n",
      " [ 1.02377944  0.06086581  0.37517007  1.555354  ]\n",
      " [ 1.04662907  0.05618816  0.36757721  1.55595838]\n",
      " [ 0.96330172  0.02607994  0.45039024 16.10601786]]\n",
      "[[4816 4839 4593 4565]\n",
      " [4756 4458 4361 4246]\n",
      " [4794 4575 4669 4330]\n",
      " [4815 4858 4758 4573]\n",
      " [4795 4890 4704 4649]\n",
      " [3159 4338 4088 4382]\n",
      " [4899 4313 4546 3765]\n",
      " [2073 2312 3918 3170]\n",
      " [4270 4404 4863 4780]\n",
      " [4802 4773 3932 4497]\n",
      " [3607 4670 4563 4432]\n",
      " [3111 4423 4482 4201]\n",
      " [3829 4812 4787 4556]\n",
      " [4399 4192 4095 4040]\n",
      " [3380 4443 4691 4242]\n",
      " [2839 4452 4236 4458]\n",
      " [3062 4456 4262 4454]\n",
      " [3158 3548 4648 4613]\n",
      " [4841 4635 4748 4788]\n",
      " [4695 4647 4388 4738]\n",
      " [3633 4717 4806 4502]\n",
      " [4734 4874 4672 4452]\n",
      " [3955 4587 4843 4734]\n",
      " [4376 3445 3970 3490]\n",
      " [4680 1053 3565 3240]\n",
      " [3641 4615 4834 4713]\n",
      " [2963 4551 4766 4585]\n",
      " [2351 2530 3872 3174]\n",
      " [3050 4483 4684 4570]\n",
      " [4426 4153 4214 3973]\n",
      " [3568 3968 4191 4299]\n",
      " [3360 4485 4171 4240]\n",
      " [4398 4191 4033 4039]\n",
      " [3246 4539 4334 4576]\n",
      " [3008 4570 4386 4698]\n",
      " [4147 2804 3664 2836]\n",
      " [3084 4629 4588 4536]\n",
      " [4259 4558 4401 4725]\n",
      " [4282 4505 4809 4755]\n",
      " [4849 3807 4428 3680]\n",
      " [3160 4339 4223 4380]\n",
      " [1926 1276 3909 2078]\n",
      " [3889 4685 4552 4582]\n",
      " [4866 4769 4362 4494]\n",
      " [3486 4537 4789 4656]\n",
      " [3112 4526 4541 4671]\n",
      " [4497 3993 4027 3977]\n",
      " [4878 4340 3855 4291]\n",
      " [3354 2476 3219 3067]\n",
      " [4496 3992 4028 3976]\n",
      " [3238 4542 4580 4625]\n",
      " [2608 3029 3460 3801]\n",
      " [2219 3918 3979 4484]\n",
      " [4714 3925 3843 4334]\n",
      " [2933 4225 4212 4513]\n",
      " [3978 4080 3920 4183]\n",
      " [3877 4309 4840 4393]\n",
      " [3685 4778 4446 4787]\n",
      " [4672 4437 3896 4567]\n",
      " [3121 1402 3111 1077]\n",
      " [2251 3917 3852 4429]\n",
      " [3660 1521 3194 3994]\n",
      " [3122 1403 3110 1076]\n",
      " [4475 4246 3590 4281]\n",
      " [3039 3470 3894 4258]\n",
      " [2467 3944 4057 4383]\n",
      " [4528 3867 3583 3913]\n",
      " [3699 2775 3414 3494]\n",
      " [3256 3979 4079 4354]\n",
      " [3553 4395 4222 4644]\n",
      " [3761 4885 4653 4912]\n",
      " [3455 4830 4527 4854]\n",
      " [4790 4645 4234 4752]\n",
      " [3781 4809 4577 4857]\n",
      " [4122 4841 4884 4688]\n",
      " [4702 4207 4287 3962]\n",
      " [3433 4674 4600 4470]\n",
      " [4570 2303 3815 2997]\n",
      " [4492 4396 3736 4453]\n",
      " [3070 3079 3068 3802]\n",
      " [3394 4476 4554 4701]\n",
      " [3651 4585 4258 4675]\n",
      " [3362 4324 4404 4460]\n",
      " [3677 3910 4374 4253]\n",
      " [4789 4646 4235 4751]\n",
      " [4338 4495 4156 4336]\n",
      " [4392 4258 4389 4289]\n",
      " [3687 4351 4024 4443]\n",
      " [2779 4245 4828 4627]\n",
      " [4393 2948 3947 3579]\n",
      " [3474 2718 3828 3322]\n",
      " [3135 4784 4613 4825]\n",
      " [4737 4886 4270 4694]\n",
      " [4378 4738 4767 4548]\n",
      " [3108 4548 4350 4639]\n",
      " [4455 3593 3639 4051]\n",
      " [2937 4150 4498 4575]\n",
      " [4478 4122 3498 4152]\n",
      " [2546 3688 3944 4314]\n",
      " [4379 3702 3365 3831]\n",
      " [3742 2223 3646 2220]\n",
      " [3471 1457 3285 4017]\n",
      " [1915  559 2674 3443]\n",
      " [3673 4494 4423 4583]\n",
      " [4193 2124 3966 3055]\n",
      " [4707 4763 4106 4759]\n",
      " [3208 4432 4221 4257]\n",
      " [3335 2241  936 2160]\n",
      " [2612 4236 4454 4560]\n",
      " [4074 4512 4413 4552]\n",
      " [3951 3465 4476 4456]\n",
      " [2666 3226 4358 4305]\n",
      " [2309 3741 3957 4278]\n",
      " [4703 4316 3945 4244]\n",
      " [3346 4292 4721 4495]\n",
      " [3960 3420 4253 3396]\n",
      " [4131 3396 4282 3320]\n",
      " [4119 3265 4240 3400]\n",
      " [4120 3429 4276 3352]\n",
      " [4190 3305 4192 3311]\n",
      " [3575 3549 4294 4239]\n",
      " [3707 4073 4544 4290]\n",
      " [3817 3372 4274 3404]\n",
      " [3534 4260 4687 4428]\n",
      " [4176 3457 4290 3526]\n",
      " [3690 4513 4736 4522]\n",
      " [4165 3303 4144 3480]\n",
      " [4058 4758 4750 4636]\n",
      " [3007 4571 4550 4696]\n",
      " [2983 4549 4363 4543]\n",
      " [3076 4499 4256 4518]\n",
      " [4305 3645 3992 3743]\n",
      " [3170 3289 4040 4286]\n",
      " [3064 2961 4056 3262]\n",
      " [3027 4369 4359 4326]\n",
      " [4306 3646 4048 3744]\n",
      " [3234 4388 4092 4249]\n",
      " [4820 4899 4514 4638]\n",
      " [4851 3811 4494 3678]\n",
      " [4307 2048 3880 2892]\n",
      " [4632 3109 3977 3259]\n",
      " [4219 3561 4127 3658]\n",
      " [3879 4574 4596 4328]\n",
      " [2619 2153 3755 2937]\n",
      " [4008 1879 3754 2813]\n",
      " [4397 4820 4424 4648]\n",
      " [1868 4057 3823 4521]\n",
      " [3082 4392 4052 4311]\n",
      " [4724 4562 3983 4438]\n",
      " [4112 4796 4861 4718]\n",
      " [4432 3956 4112 3885]\n",
      " [4057 3770 4099 4284]\n",
      " [3961 4552 4055 4596]\n",
      " [3146 2708 3887 3073]\n",
      " [3753 1096 3554 3588]\n",
      " [3464 1033 3401 3619]\n",
      " [2998  414 3750 2908]\n",
      " [3968 1232 4313 2919]\n",
      " [4806 4637 4497 4340]\n",
      " [3183 4516 4475 4629]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4839, 4756, 4794, 4858, 4890, 4382, 4899, 3918, 4863, 4802, 4670,\n",
       "       4482, 4812, 4399, 4691, 4458, 4456, 4648, 4841, 4738, 4806, 4874,\n",
       "       4843, 4376, 4680, 4834, 4766, 3872, 4684, 4426, 4299, 4485, 4398,\n",
       "       4576, 4698, 4147, 4629, 4725, 4809, 4849, 4380, 3909, 4685, 4866,\n",
       "       4789, 4671, 4497, 4878, 3354, 4496, 4625, 3801, 4484, 4714, 4513,\n",
       "       4183, 4840, 4787, 4672, 3121, 4429, 3994, 3122, 4475, 4258, 4383,\n",
       "       4528, 3699, 4354, 4644, 4912, 4854, 4790, 4857, 4884, 4702, 4674,\n",
       "       4570, 4492, 3802, 4701, 4675, 4460, 4374, 4789, 4495, 4392, 4443,\n",
       "       4828, 4393, 3828, 4825, 4886, 4767, 4639, 4455, 4575, 4478, 4314,\n",
       "       4379, 3742, 4017, 3443, 4583, 4193, 4763, 4432, 3335, 4560, 4552,\n",
       "       4476, 4358, 4278, 4703, 4721, 4253, 4282, 4240, 4276, 4192, 4294,\n",
       "       4544, 4274, 4687, 4290, 4736, 4165, 4758, 4696, 4549, 4518, 4305,\n",
       "       4286, 4056, 4369, 4306, 4388, 4899, 4851, 4307, 4632, 4219, 4596,\n",
       "       3755, 4008, 4820, 4521, 4392, 4724, 4861, 4432, 4284, 4596, 3887,\n",
       "       3753, 3619, 3750, 4313, 4806, 4629])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array(scores)\n",
    "print(x)\n",
    "print(np.array(x).argsort(axis=0).argsort(axis=0)[all_predict_inlier][y[all_predict_inlier]==1])\n",
    "np.max(np.array(x).argsort(axis=0).argsort(axis=0)[all_predict_inlier][y[all_predict_inlier]==1], axis=1)\n",
    "# print(np.argsort(scores[:,0], axis = -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, ..., False, False,  True])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ldisagree_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1498  440  339 ... 4691 1394 4017]\n",
      "[1.09237809 1.05785957 1.00117835 ... 1.02377944 1.04662907 0.96330172]\n"
     ]
    }
   ],
   "source": [
    "print(np.argsort(-scores[:,0]))\n",
    "print(scores[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 1]\n",
      " [0 1 0 0]\n",
      " [1 1 0 0]\n",
      " [0 0 1 0]\n",
      " [0 1 0 0]\n",
      " [1 1 0 0]\n",
      " [0 1 0 0]\n",
      " [1 1 0 0]\n",
      " [0 1 0 0]\n",
      " [1 1 1 0]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [1 1 1 0]\n",
      " [0 1 1 1]\n",
      " [1 0 0 0]\n",
      " [1 1 1 0]\n",
      " [1 1 0 0]]\n",
      "[0 0 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1]\n",
      "[[1440 2683  221 4543]\n",
      " [4756  784 2816 2368]\n",
      " [5391  876 3265 2638]\n",
      " [5428 1028  428 1130]\n",
      " [4879 5391  917  894]\n",
      " [4892 5428 2934  882]\n",
      " [3312 4410 2384  509]\n",
      " [3854 4390 2766  596]\n",
      " [3704  885  481 4924]\n",
      " [5363 2330 1978 3367]\n",
      " [1701   52  696  696]\n",
      " [ 804  510  326  592]\n",
      " [2744 1403   50 2582]\n",
      " [3204 2088 3242  615]\n",
      " [2683 3327 1743  951]\n",
      " [2686  591 3353 1175]\n",
      " [2830 5167 3180 1296]\n",
      " [2998 2422 1700 1017]\n",
      " [1947 3280  951 1211]\n",
      " [3376 2552 3423 2684]]\n"
     ]
    }
   ],
   "source": [
    "print(L[disagree_indexes,:][:20,:])\n",
    "print(y[disagree_indexes][:20])\n",
    "print(np.argsort(scores, axis=0)[disagree_indexes,:][:20,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method disagree, num_method_as_outliers=1, number of points = 41\n",
      "Method disagree, num_method_as_outliers=1, number of true inliers = 35\n",
      "Method disagree, num_method_as_outliers=1, number of true outiers = 6\n",
      "Method disagree, num_method_as_outliers=2, number of points = 31\n",
      "Method disagree, num_method_as_outliers=2, number of true inliers = 28\n",
      "Method disagree, num_method_as_outliers=2, number of true outiers = 3\n",
      "Method disagree, num_method_as_outliers=3, number of points = 26\n",
      "Method disagree, num_method_as_outliers=3, number of true inliers = 18\n",
      "Method disagree, num_method_as_outliers=3, number of true outiers = 8\n",
      "Method disagree, num_method_as_outliers=4, number of points = 51\n",
      "Method disagree, num_method_as_outliers=4, number of true inliers = 19\n",
      "Method disagree, num_method_as_outliers=4, number of true outiers = 32\n",
      "Method disagree, num_method_as_outliers=5, number of points = 54\n",
      "Method disagree, num_method_as_outliers=5, number of true inliers = 40\n",
      "Method disagree, num_method_as_outliers=5, number of true outiers = 14\n",
      "Method disagree, num_method_as_outliers=6, number of points = 31\n",
      "Method disagree, num_method_as_outliers=6, number of true inliers = 20\n",
      "Method disagree, num_method_as_outliers=6, number of true outiers = 11\n",
      "Method disagree, num_method_as_outliers=7, number of points = 20\n",
      "Method disagree, num_method_as_outliers=7, number of true inliers = 17\n",
      "Method disagree, num_method_as_outliers=7, number of true outiers = 3\n",
      "Method disagree, num_method_as_outliers=8, number of points = 14\n",
      "Method disagree, num_method_as_outliers=8, number of true inliers = 13\n",
      "Method disagree, num_method_as_outliers=8, number of true outiers = 1\n",
      "Method disagree, num_method_as_outliers=9, number of points = 15\n",
      "Method disagree, num_method_as_outliers=9, number of true inliers = 11\n",
      "Method disagree, num_method_as_outliers=9, number of true outiers = 4\n",
      "Method disagree, num_method_as_outliers=10, number of points = 26\n",
      "Method disagree, num_method_as_outliers=10, number of true inliers = 22\n",
      "Method disagree, num_method_as_outliers=10, number of true outiers = 4\n",
      "Method disagree, num_method_as_outliers=11, number of points = 19\n",
      "Method disagree, num_method_as_outliers=11, number of true inliers = 15\n",
      "Method disagree, num_method_as_outliers=11, number of true outiers = 4\n",
      "Method disagree, num_method_as_outliers=12, number of points = 18\n",
      "Method disagree, num_method_as_outliers=12, number of true inliers = 12\n",
      "Method disagree, num_method_as_outliers=12, number of true outiers = 6\n",
      "Method disagree, num_method_as_outliers=13, number of points = 19\n",
      "Method disagree, num_method_as_outliers=13, number of true inliers = 13\n",
      "Method disagree, num_method_as_outliers=13, number of true outiers = 6\n",
      "Method disagree, num_method_as_outliers=14, number of points = 13\n",
      "Method disagree, num_method_as_outliers=14, number of true inliers = 10\n",
      "Method disagree, num_method_as_outliers=14, number of true outiers = 3\n",
      "Method disagree, num_method_as_outliers=15, number of points = 33\n",
      "Method disagree, num_method_as_outliers=15, number of true inliers = 27\n",
      "Method disagree, num_method_as_outliers=15, number of true outiers = 6\n",
      "Method disagree, num_method_as_outliers=16, number of points = 13\n",
      "Method disagree, num_method_as_outliers=16, number of true inliers = 12\n",
      "Method disagree, num_method_as_outliers=16, number of true outiers = 1\n",
      "Method disagree, num_method_as_outliers=17, number of points = 20\n",
      "Method disagree, num_method_as_outliers=17, number of true inliers = 13\n",
      "Method disagree, num_method_as_outliers=17, number of true outiers = 7\n",
      "Method disagree, num_method_as_outliers=18, number of points = 15\n",
      "Method disagree, num_method_as_outliers=18, number of true inliers = 10\n",
      "Method disagree, num_method_as_outliers=18, number of true outiers = 5\n",
      "Method disagree, num_method_as_outliers=19, number of points = 24\n",
      "Method disagree, num_method_as_outliers=19, number of true inliers = 18\n",
      "Method disagree, num_method_as_outliers=19, number of true outiers = 6\n",
      "Method disagree, num_method_as_outliers=20, number of points = 69\n",
      "Method disagree, num_method_as_outliers=20, number of true inliers = 47\n",
      "Method disagree, num_method_as_outliers=20, number of true outiers = 22\n",
      "Method disagree, num_method_as_outliers=21, number of points = 22\n",
      "Method disagree, num_method_as_outliers=21, number of true inliers = 16\n",
      "Method disagree, num_method_as_outliers=21, number of true outiers = 6\n",
      "Method disagree, num_method_as_outliers=22, number of points = 24\n",
      "Method disagree, num_method_as_outliers=22, number of true inliers = 13\n",
      "Method disagree, num_method_as_outliers=22, number of true outiers = 11\n",
      "Method disagree, num_method_as_outliers=23, number of points = 16\n",
      "Method disagree, num_method_as_outliers=23, number of true inliers = 12\n",
      "Method disagree, num_method_as_outliers=23, number of true outiers = 4\n",
      "Method disagree, num_method_as_outliers=24, number of points = 16\n",
      "Method disagree, num_method_as_outliers=24, number of true inliers = 12\n",
      "Method disagree, num_method_as_outliers=24, number of true outiers = 4\n",
      "Method disagree, num_method_as_outliers=25, number of points = 20\n",
      "Method disagree, num_method_as_outliers=25, number of true inliers = 16\n",
      "Method disagree, num_method_as_outliers=25, number of true outiers = 4\n",
      "Method disagree, num_method_as_outliers=26, number of points = 17\n",
      "Method disagree, num_method_as_outliers=26, number of true inliers = 13\n",
      "Method disagree, num_method_as_outliers=26, number of true outiers = 4\n",
      "Method disagree, num_method_as_outliers=27, number of points = 15\n",
      "Method disagree, num_method_as_outliers=27, number of true inliers = 13\n",
      "Method disagree, num_method_as_outliers=27, number of true outiers = 2\n",
      "Method disagree, num_method_as_outliers=28, number of points = 12\n",
      "Method disagree, num_method_as_outliers=28, number of true inliers = 11\n",
      "Method disagree, num_method_as_outliers=28, number of true outiers = 1\n",
      "Method disagree, num_method_as_outliers=29, number of points = 14\n",
      "Method disagree, num_method_as_outliers=29, number of true inliers = 10\n",
      "Method disagree, num_method_as_outliers=29, number of true outiers = 4\n",
      "Method disagree, num_method_as_outliers=30, number of points = 13\n",
      "Method disagree, num_method_as_outliers=30, number of true inliers = 8\n",
      "Method disagree, num_method_as_outliers=30, number of true outiers = 5\n",
      "Method disagree, num_method_as_outliers=31, number of points = 6\n",
      "Method disagree, num_method_as_outliers=31, number of true inliers = 4\n",
      "Method disagree, num_method_as_outliers=31, number of true outiers = 2\n",
      "Method disagree, num_method_as_outliers=32, number of points = 19\n",
      "Method disagree, num_method_as_outliers=32, number of true inliers = 14\n",
      "Method disagree, num_method_as_outliers=32, number of true outiers = 5\n",
      "Method disagree, num_method_as_outliers=33, number of points = 7\n",
      "Method disagree, num_method_as_outliers=33, number of true inliers = 4\n",
      "Method disagree, num_method_as_outliers=33, number of true outiers = 3\n",
      "Method disagree, num_method_as_outliers=34, number of points = 13\n",
      "Method disagree, num_method_as_outliers=34, number of true inliers = 8\n",
      "Method disagree, num_method_as_outliers=34, number of true outiers = 5\n",
      "Method disagree, num_method_as_outliers=35, number of points = 19\n",
      "Method disagree, num_method_as_outliers=35, number of true inliers = 13\n",
      "Method disagree, num_method_as_outliers=35, number of true outiers = 6\n",
      "Method disagree, num_method_as_outliers=36, number of points = 13\n",
      "Method disagree, num_method_as_outliers=36, number of true inliers = 8\n",
      "Method disagree, num_method_as_outliers=36, number of true outiers = 5\n",
      "Method disagree, num_method_as_outliers=37, number of points = 4\n",
      "Method disagree, num_method_as_outliers=37, number of true inliers = 3\n",
      "Method disagree, num_method_as_outliers=37, number of true outiers = 1\n",
      "Method disagree, num_method_as_outliers=38, number of points = 10\n",
      "Method disagree, num_method_as_outliers=38, number of true inliers = 7\n",
      "Method disagree, num_method_as_outliers=38, number of true outiers = 3\n",
      "Method disagree, num_method_as_outliers=39, number of points = 16\n",
      "Method disagree, num_method_as_outliers=39, number of true inliers = 14\n",
      "Method disagree, num_method_as_outliers=39, number of true outiers = 2\n",
      "Method disagree, num_method_as_outliers=40, number of points = 18\n",
      "Method disagree, num_method_as_outliers=40, number of true inliers = 16\n",
      "Method disagree, num_method_as_outliers=40, number of true outiers = 2\n",
      "Method disagree, num_method_as_outliers=41, number of points = 14\n",
      "Method disagree, num_method_as_outliers=41, number of true inliers = 10\n",
      "Method disagree, num_method_as_outliers=41, number of true outiers = 4\n",
      "Method disagree, num_method_as_outliers=42, number of points = 12\n",
      "Method disagree, num_method_as_outliers=42, number of true inliers = 8\n",
      "Method disagree, num_method_as_outliers=42, number of true outiers = 4\n",
      "Method disagree, num_method_as_outliers=43, number of points = 21\n",
      "Method disagree, num_method_as_outliers=43, number of true inliers = 13\n",
      "Method disagree, num_method_as_outliers=43, number of true outiers = 8\n",
      "Method disagree, num_method_as_outliers=44, number of points = 55\n",
      "Method disagree, num_method_as_outliers=44, number of true inliers = 48\n",
      "Method disagree, num_method_as_outliers=44, number of true outiers = 7\n",
      "Method disagree, num_method_as_outliers=45, number of points = 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method disagree, num_method_as_outliers=45, number of true inliers = 6\n",
      "Method disagree, num_method_as_outliers=45, number of true outiers = 4\n",
      "Method disagree, num_method_as_outliers=46, number of points = 4\n",
      "Method disagree, num_method_as_outliers=46, number of true inliers = 1\n",
      "Method disagree, num_method_as_outliers=46, number of true outiers = 3\n",
      "Method disagree, num_method_as_outliers=47, number of points = 5\n",
      "Method disagree, num_method_as_outliers=47, number of true inliers = 4\n",
      "Method disagree, num_method_as_outliers=47, number of true outiers = 1\n",
      "Method disagree, num_method_as_outliers=48, number of points = 7\n",
      "Method disagree, num_method_as_outliers=48, number of true inliers = 4\n",
      "Method disagree, num_method_as_outliers=48, number of true outiers = 3\n",
      "Method disagree, num_method_as_outliers=49, number of points = 8\n",
      "Method disagree, num_method_as_outliers=49, number of true inliers = 4\n",
      "Method disagree, num_method_as_outliers=49, number of true outiers = 4\n",
      "Method disagree, num_method_as_outliers=50, number of points = 10\n",
      "Method disagree, num_method_as_outliers=50, number of true inliers = 7\n",
      "Method disagree, num_method_as_outliers=50, number of true outiers = 3\n",
      "Method disagree, num_method_as_outliers=51, number of points = 10\n",
      "Method disagree, num_method_as_outliers=51, number of true inliers = 7\n",
      "Method disagree, num_method_as_outliers=51, number of true outiers = 3\n",
      "Method disagree, num_method_as_outliers=52, number of points = 6\n",
      "Method disagree, num_method_as_outliers=52, number of true inliers = 3\n",
      "Method disagree, num_method_as_outliers=52, number of true outiers = 3\n",
      "Method disagree, num_method_as_outliers=53, number of points = 8\n",
      "Method disagree, num_method_as_outliers=53, number of true inliers = 6\n",
      "Method disagree, num_method_as_outliers=53, number of true outiers = 2\n",
      "Method disagree, num_method_as_outliers=54, number of points = 4\n",
      "Method disagree, num_method_as_outliers=54, number of true inliers = 1\n",
      "Method disagree, num_method_as_outliers=54, number of true outiers = 3\n",
      "Method disagree, num_method_as_outliers=55, number of points = 8\n",
      "Method disagree, num_method_as_outliers=55, number of true inliers = 6\n",
      "Method disagree, num_method_as_outliers=55, number of true outiers = 2\n",
      "Method disagree, num_method_as_outliers=56, number of points = 5\n",
      "Method disagree, num_method_as_outliers=56, number of true inliers = 4\n",
      "Method disagree, num_method_as_outliers=56, number of true outiers = 1\n",
      "Method disagree, num_method_as_outliers=57, number of points = 8\n",
      "Method disagree, num_method_as_outliers=57, number of true inliers = 5\n",
      "Method disagree, num_method_as_outliers=57, number of true outiers = 3\n",
      "Method disagree, num_method_as_outliers=58, number of points = 9\n",
      "Method disagree, num_method_as_outliers=58, number of true inliers = 0\n",
      "Method disagree, num_method_as_outliers=58, number of true outiers = 9\n",
      "Method disagree, num_method_as_outliers=59, number of points = 7\n",
      "Method disagree, num_method_as_outliers=59, number of true inliers = 3\n",
      "Method disagree, num_method_as_outliers=59, number of true outiers = 4\n",
      "Method disagree, num_method_as_outliers=60, number of points = 8\n",
      "Method disagree, num_method_as_outliers=60, number of true inliers = 3\n",
      "Method disagree, num_method_as_outliers=60, number of true outiers = 5\n",
      "Method disagree, num_method_as_outliers=61, number of points = 14\n",
      "Method disagree, num_method_as_outliers=61, number of true inliers = 7\n",
      "Method disagree, num_method_as_outliers=61, number of true outiers = 7\n",
      "Method disagree, num_method_as_outliers=62, number of points = 17\n",
      "Method disagree, num_method_as_outliers=62, number of true inliers = 8\n",
      "Method disagree, num_method_as_outliers=62, number of true outiers = 9\n",
      "Method disagree, num_method_as_outliers=63, number of points = 21\n",
      "Method disagree, num_method_as_outliers=63, number of true inliers = 15\n",
      "Method disagree, num_method_as_outliers=63, number of true outiers = 6\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "for i in range(1, num_methods):\n",
    "    num_all_disagree = sum(np.sum(L[disagree_indexes, :],axis=1)==i)\n",
    "    print('Method disagree, num_method_as_outliers={}, number of points = {}'\n",
    "          .format(i, num_all_disagree))\n",
    "    num_disagree_inliers = sum(y[disagree_indexes][np.sum(L[disagree_indexes, :],axis=1)==i]==0)\n",
    "    print('Method disagree, num_method_as_outliers={}, number of true inliers = {}'\n",
    "          .format(i, num_disagree_inliers))\n",
    "    num_disagree_outliers = sum(y[disagree_indexes][np.sum(L[disagree_indexes, :],axis=1)==i]==1)\n",
    "    print('Method disagree, num_method_as_outliers={}, number of true outiers = {}'\n",
    "          .format(i, num_disagree_outliers))\n",
    "    result.append([num_all_disagree, num_disagree_inliers, num_disagree_outliers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x133eae410>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(range(0,len(scores[y==1,3])), scores[y==1,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 911,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L shape (5473, 156)\n",
      "All agree, Number of outliers = 55\n",
      "All agree, Number of inliers = 3788\n",
      "num of inliers = 3788\n",
      "num of outliers = 362\n",
      "(4150, 26)\n",
      "(4150,)\n",
      "F-1 score: 0.518860662047729\n",
      "[ 0.2756969   0.06014979  0.29439604  0.29648246  0.20636889  0.0871536\n",
      "  0.01097172 -0.01679439  0.05334285  0.13574258  0.00687914 -0.05657723\n",
      " -0.07584194  0.02252633  0.00581494  0.03069127  0.03884938  0.18490425\n",
      "  0.30661874  0.45375213 -0.01199057 -0.36638537 -0.11584035  0.32368891\n",
      "  0.03577571  1.29149479]\n",
      "0\n",
      "[0 1 2 3 4 5 6 8 9]\n",
      "[0 3 4 5 6 7 8 9]\n",
      "[3 4]\n",
      "[0]\n",
      "[[ 0  9]\n",
      " [ 9 17]\n",
      " [17 19]\n",
      " [19 20]]\n",
      "[[  0  54]\n",
      " [ 54 102]\n",
      " [102 114]\n",
      " [114 120]]\n",
      "393\n",
      "3844\n",
      "L shape (5473, 120)\n",
      "All agree, Number of outliers = 57\n",
      "All agree, Number of inliers = 3793\n",
      "num of inliers = 3983\n",
      "num of outliers = 431\n",
      "(4414, 20)\n",
      "(4414,)\n",
      "F-1 score: 0.5208955223880598\n",
      "[ 0.15186768  0.06620574  0.27847413  0.31807438  0.21494111  0.12382872\n",
      "  0.05226428  0.07120398  0.19148699 -0.09839986 -0.0143421  -0.02946763\n",
      " -0.03840814 -0.03438845  0.02990353  0.21041925  0.40018027  0.47817535\n",
      "  0.13126373  1.31217651]\n",
      "0\n",
      "[0 1 2 3 4 5 6 7 8]\n",
      "[5 6 7]\n",
      "[0 1]\n",
      "[0]\n",
      "[[ 0  9]\n",
      " [ 9 12]\n",
      " [12 14]\n",
      " [14 15]]\n",
      "[[ 0 54]\n",
      " [54 72]\n",
      " [72 84]\n",
      " [84 90]]\n",
      "424\n",
      "3868\n",
      "L shape (5473, 90)\n",
      "All agree, Number of outliers = 57\n",
      "All agree, Number of inliers = 3852\n",
      "num of inliers = 4057\n",
      "num of outliers = 470\n",
      "(4527, 15)\n",
      "(4527,)\n",
      "F-1 score: 0.5269461077844312\n",
      "[ 0.1192614   0.03076471  0.29650831  0.25070876  0.18460797  0.12303834\n",
      "  0.08155141  0.09331996  0.26328567 -0.15531654  0.11631465  0.35156833\n",
      "  0.49966601  0.01700636  1.51613067]\n",
      "0\n",
      "[0 1 2 3 4 5 6 7 8]\n",
      "[1 2]\n",
      "[0 1]\n",
      "[0]\n",
      "[[ 0  9]\n",
      " [ 9 11]\n",
      " [11 13]\n",
      " [13 14]]\n",
      "[[ 0 54]\n",
      " [54 66]\n",
      " [66 78]\n",
      " [78 84]]\n",
      "425\n",
      "3920\n",
      "L shape (5473, 84)\n",
      "All agree, Number of outliers = 57\n",
      "All agree, Number of inliers = 3853\n",
      "num of inliers = 4080\n",
      "num of outliers = 480\n",
      "(4560, 14)\n",
      "(4560,)\n",
      "F-1 score: 0.53125\n",
      "[ 0.09008706  0.0384081   0.30403211  0.25243864  0.16588645  0.11806223\n",
      "  0.07553728  0.07973523  0.246283    0.07607139  0.3380389   0.48096547\n",
      " -0.02502064  1.58679724]\n",
      "0\n",
      "[0 1 2 3 4 5 6 7 8]\n",
      "[0 1]\n",
      "[0]\n",
      "[0]\n",
      "[[ 0  9]\n",
      " [ 9 11]\n",
      " [11 12]\n",
      " [12 13]]\n",
      "[[ 0 54]\n",
      " [54 66]\n",
      " [66 72]\n",
      " [72 78]]\n",
      "432\n",
      "3934\n",
      "L shape (5473, 78)\n",
      "All agree, Number of outliers = 57\n",
      "All agree, Number of inliers = 3853\n",
      "num of inliers = 4097\n",
      "num of outliers = 494\n",
      "(4591, 13)\n",
      "(4591,)\n",
      "F-1 score: 0.5565969718817592\n",
      "[ 0.02638199  0.04739549  0.33375272  0.25262573  0.16922049  0.05689232\n",
      "  0.0695676   0.08856946  0.26934301 -0.00234265  0.30747095  0.64334284\n",
      "  1.64667895]\n",
      "0\n",
      "[0 1 2 3 4 5 6 7 8]\n",
      "[1]\n",
      "[0]\n",
      "[0]\n",
      "[[ 0  9]\n",
      " [ 9 10]\n",
      " [10 11]\n",
      " [11 12]]\n",
      "[[ 0 54]\n",
      " [54 60]\n",
      " [60 66]\n",
      " [66 72]]\n",
      "455\n",
      "3935\n",
      "L shape (5473, 72)\n",
      "All agree, Number of outliers = 57\n",
      "All agree, Number of inliers = 3857\n",
      "num of inliers = 4112\n",
      "num of outliers = 507\n",
      "(4619, 12)\n",
      "(4619,)\n",
      "F-1 score: 0.5545977011494253\n",
      "[0.0144405  0.04763186 0.36029471 0.24818463 0.13372467 0.0613611\n",
      " 0.09557389 0.07398658 0.26982241 0.30332509 0.64142644 1.68396247]\n",
      "0\n",
      "[0 1 2 3 4 5 6 7 8]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[[ 0  9]\n",
      " [ 9 10]\n",
      " [10 11]\n",
      " [11 12]]\n",
      "[[ 0 54]\n",
      " [54 60]\n",
      " [60 66]\n",
      " [66 72]]\n",
      "458\n",
      "3948\n",
      "L shape (5473, 72)\n",
      "All agree, Number of outliers = 57\n",
      "All agree, Number of inliers = 3857\n",
      "num of inliers = 4122\n",
      "num of outliers = 507\n",
      "(4629, 12)\n",
      "(4629,)\n",
      "F-1 score: 0.5549964054636952\n",
      "[0.0066128  0.05586915 0.35591369 0.24814468 0.13219181 0.06491244\n",
      " 0.09776725 0.07228901 0.26570035 0.30668346 0.64250172 1.68605599]\n",
      "0\n",
      "[0 1 2 3 4 5 6 7 8]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "breakcond = 0\n",
    "\n",
    "for i in range(0, 26):\n",
    "    print('L shape', np.shape(L))\n",
    "    num_methods = np.shape(L)[1]\n",
    "    \n",
    "    ########################################################################\n",
    "\n",
    "    agree_outlier_indexes = np.sum(L,axis=1)==np.shape(L)[1]\n",
    "    print('All agree, Number of outliers = {}'.format(sum(agree_outlier_indexes)))\n",
    "    agree_inlier_indexes = np.sum(L,axis=1)==0\n",
    "    print('All agree, Number of inliers = {}'.format(sum(agree_inlier_indexes)))\n",
    "\n",
    "    disagree_indexes = np.where(np.logical_or(np.sum(L,axis = 1)==0, np.sum(L,axis = 1)==num_methods)==0)[0]\n",
    "    # print('Number of disagreed points = {}'.format(len(disagree_indexes)))\n",
    "    # print('Number of disagreed points (true outliers) = {}'.format(sum(y[disagree_indexes] == 1)))\n",
    "    # print('Number of disagreed points (true inliers) = {}'.format(sum(y[disagree_indexes] == 0)))\n",
    "\n",
    "    self_agree_index_list = []\n",
    "    for i in range(0, len(index_range)):\n",
    "        if(index_range[i,1]-index_range[i,0] < 1):\n",
    "            continue\n",
    "        temp_index = disagree_indexes[np.where(np.sum(L[disagree_indexes][:,index_range[i,0]: index_range[i,1]], axis = 1)==(index_range[i,1]-index_range[i,0]))[0]]\n",
    "        self_agree_index_list = np.union1d(self_agree_index_list, temp_index)\n",
    "    self_agree_index_list = [int(i) for i in self_agree_index_list]\n",
    "\n",
    "#     all_inlier_indexes = np.where(agree_inlier_indexes)[0]\n",
    "    all_inlier_indexes = np.union1d(np.setdiff1d(np.where(agree_inlier_indexes)[0], prediction_high_conf_outliers), prediction_high_conf_inliers)\n",
    "    print('num of inliers = {}'.format(np.shape(all_inlier_indexes)[0]))\n",
    "#     all_outlier_indexes = np.union1d(np.where(agree_outlier_indexes)[0], self_agree_index_list)\n",
    "    all_outlier_indexes = np.union1d(np.union1d(np.where(agree_outlier_indexes)[0], self_agree_index_list), prediction_high_conf_outliers)\n",
    "    print('num of outliers = {}'.format(np.shape(all_outlier_indexes)[0]))\n",
    "\n",
    "    ####################################################################\n",
    "    \n",
    "    from sklearn.preprocessing import RobustScaler\n",
    "    data_indexes = np.concatenate((all_inlier_indexes, all_outlier_indexes), axis = 0)\n",
    "    data_indexes = np.array([int(i) for i in data_indexes])\n",
    "    labels = np.concatenate((np.zeros(len(all_inlier_indexes)), np.ones(len(all_outlier_indexes))), axis = 0)\n",
    "    transformer = RobustScaler().fit(scores_for_training)\n",
    "    scores_transformed = transformer.transform(scores_for_training)\n",
    "    training_data = scores_transformed[data_indexes]\n",
    "    print(np.shape(training_data))\n",
    "    print(np.shape(labels))\n",
    "\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    clf = LogisticRegression(random_state=0, penalty='l2').fit(training_data, labels)\n",
    "    clf_predictions = clf.predict(scores_transformed)\n",
    "    clf_predict_proba = clf.predict_proba(scores_transformed)[:,1]\n",
    "    print(\"F-1 score:\",metrics.f1_score(y, clf_predictions))\n",
    "    cur_f1_scores.append(metrics.f1_score(y, clf_predictions))\n",
    "\n",
    "    agreed_outlier_indexes = np.where(np.sum(L,axis=1)==np.shape(L)[1])[0]\n",
    "    agreed_inlier_indexes = np.where(np.sum(L,axis=1)==0)[0]\n",
    "#     minmax_diff = max(clf.coef_[0])/min(clf.coef_[0])\n",
    "#     print('max/min = ', minmax_diff)\n",
    "#     if(minmax_diff > 0 and minmax_diff < 10):\n",
    "#         break\n",
    "        \n",
    "    print(clf.coef_[0])\n",
    "#     print(np.std(scores_transformed, 0)*clf.coef_[0])\n",
    "    prediction_result_list.append(clf_predict_proba)\n",
    "    prediction_list.append(np.array([int(i) for i in clf_predictions]))\n",
    "\n",
    "    cutoff = 0 #max(0, np.sort(clf.coef_[0])[0])\n",
    "    print(cutoff)\n",
    "    cur_clf_coef = clf.coef_[0] #* np.std(scores_transformed, 0)\n",
    "    remain_indexes_after_cond =  cur_clf_coef > cutoff #np.logical_and(cur_clf_coef > cutoff, abs(cur_clf_coef) > 0.01) # #\n",
    "    remain_indexes_after_cond_expanded = []\n",
    "    for i in range(0, len(coef_index_range)): #\n",
    "        s_e_range = coef_index_range[i,1]-coef_index_range[i,0]\n",
    "        s1, e1 = coef_index_range[i,0], coef_index_range[i,1]\n",
    "        s2, e2 = index_range[i,0], index_range[i,1]\n",
    "        saved_indexes = np.where(cur_clf_coef[s1:e1] > cutoff)[0]\n",
    "        print(saved_indexes)\n",
    "        for j in range(N_size):\n",
    "            remain_indexes_after_cond_expanded.extend(np.array(saved_indexes) + j * s_e_range + s2)\n",
    "    \n",
    "    if sum(remain_indexes_after_cond) == len(remain_indexes_after_cond):\n",
    "        if(breakcond == 1):\n",
    "            break\n",
    "        breakcond = 1\n",
    "    \n",
    "    new_coef_index_range_seq = []\n",
    "    for i in range(0, len(coef_index_range)): #\n",
    "        s, e = coef_index_range[i,0], coef_index_range[i,1]\n",
    "        new_coef_index_range_seq.append(sum((remain_indexes_after_cond)[s:e]))\n",
    "\n",
    "    coef_index_range = []\n",
    "    index_range = []\n",
    "    cur_sum = 0\n",
    "    for i in range(0, len(new_coef_index_range_seq)):\n",
    "        coef_index_range.append([cur_sum, cur_sum + new_coef_index_range_seq[i]])\n",
    "        index_range.append([cur_sum * 6, 6 * (cur_sum + new_coef_index_range_seq[i])])\n",
    "        cur_sum += new_coef_index_range_seq[i]\n",
    "\n",
    "    coef_index_range = np.array(coef_index_range)\n",
    "    index_range = np.array(index_range)\n",
    "    print(coef_index_range)\n",
    "    print(index_range)\n",
    "\n",
    "    L=L[:,remain_indexes_after_cond_expanded]\n",
    "    scores_for_training = scores_for_training[:, remain_indexes_after_cond]\n",
    "    \n",
    "    prediction_high_conf_outliers = np.where(prediction_result_list[-1] > 0.99)[0]\n",
    "    print(len(prediction_high_conf_outliers))\n",
    "    prediction_high_conf_inliers = np.where(prediction_result_list[-1] < 0.01)[0]\n",
    "    print(len(prediction_high_conf_inliers))\n",
    "    # coef_remain_index = np.array(coef_remain_index)[np.where(clf.coef_[0] > cutoff)[0]]\n",
    "    # print('remain_coef_range: ', coef_remain_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-class SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 for oneclass-SVM : 0.0010326578030205242\n",
      "F1 for oneclass-SVM : 0.0018611576400521124\n",
      "F1 for oneclass-SVM : 0.05679767788499255\n",
      "F1 for oneclass-SVM : 0.10714043534278156\n",
      "F1 for oneclass-SVM : 0.16106853020739403\n",
      "F1 for oneclass-SVM : 0.15696861591862316\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "oneclass_SVM_model = pickle.load(open(\"oneclass_SVM_model_cover.pickle\", \"rb\" ))\n",
    "prediction_scores = oneclass_SVM_model['scores']\n",
    "\n",
    "for i in range(len(mahalanobis_N_range)):\n",
    "    ocsvm_predictions,ocsvm_scores,f1 = get_predictions(prediction_scores, num_outliers=mahalanobis_N_range[i], method_name='oneclass-SVM')\n",
    "    all_results.append(ocsvm_predictions)\n",
    "    all_scores.append(ocsvm_scores)\n",
    "    f1s.append(f1)\n",
    "L = np.stack(all_results).T\n",
    "scores = np.stack(all_scores).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "clf = OneClassSVM(gamma='auto', verbose=True).fit(X_transformed)\n",
    "print('model fitted')\n",
    "prediction_scores = clf.score_samples(X_transformed)\n",
    "\n",
    "for i in range(len(mahalanobis_N_range)):\n",
    "    threshold = np.sort(prediction_scores)[::-1][mahalanobis_N_range[i]]\n",
    "    predictions = np.array(prediction_scores > threshold)\n",
    "    predictions = np.array([int(i) for i in predictions])\n",
    "    print('F1 for one-class SVM : {}'.format(metrics.f1_score(y, predictions)))\n",
    "    \n",
    "import pickle\n",
    "oneclass_SVM_model = {'model': clf, 'scores': prediction_scores}\n",
    "pickle.dump(oneclass_SVM_model, open(\"oneclass_SVM_model_cover_transformed.pickle\", \"wb\" ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
